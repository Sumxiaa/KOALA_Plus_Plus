{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U0PZfnC0kOt"
      },
      "source": [
        "# Optimizers Comparisons\n",
        "\n",
        "In this project, we are going to use and compare different optimizers on the classic CIFAR10 image classification task, image generation tasks, and also on machine translation tasks, not only including some classic optimizers like Adam, and SGD, but also including some new optimizers like KOALA, which was proposed in [paper](https://arxiv.org/pdf/2107.03331), and you can also add some other\n",
        "\n",
        "Also, we are going to do some visualizations of different optimizers to see the effects of the gradient descent procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2COtA8uM2HqG"
      },
      "source": [
        "## Google Colab Setup\n",
        "Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
        "\n",
        "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cbgT3tHm0xP6"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvks81Ca2Y5X"
      },
      "source": [
        "Then, we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
        "Run the following cell to mount your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ5q4M9N2c5H",
        "outputId": "37aa5dbd-6950-4ef0-c176-9dd92fade3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvoDCWo2fjh"
      },
      "source": [
        "Recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4WO62uv2wn3",
        "outputId": "f6fabacb-f139-44cd-f054-f7e3ee2ab1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Optimizers_in_DL.ipynb', 'pytorch101.py', '__pycache__', 'Image_Classification']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you uploaded the project\n",
        "# Example: If you create a 2020FA folder and put all the files under A1 folder, then '2020FA/A1'\n",
        "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2020FA/A1'\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Koala'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBnKblxk3UNO"
      },
      "source": [
        "In the very last step of the environment setup procedure, we need to go into the current directory by using the following command to go into the current folder, you can change the folder name and the path if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoaxPBqq3Tol",
        "outputId": "13b40e23-426c-402e-bb85-6b77584cd6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from hello_pytorch.py!\n",
            "pytorch101.py last edited on Mon Jan 20 08:21:21 2025\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "import time, os\n",
        "os.environ[\"TZ\"] = \"US/Eastern\"\n",
        "time.tzset()\n",
        "\n",
        "from pytorch101 import hello\n",
        "hello()\n",
        "\n",
        "pytorch101_path = os.path.join(GOOGLE_DRIVE_PATH, 'pytorch101.py')\n",
        "pytorch101_edit_time = time.ctime(os.path.getmtime(pytorch101_path))\n",
        "print('pytorch101.py last edited on %s' % pytorch101_edit_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBDqbBZUKzB"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gyzc197pxoUi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "def _adadelta(parameters, **kwargs):\n",
        "    return optim.Adadelta(parameters, **kwargs)\n",
        "\n",
        "\n",
        "def _adagrad(parameters, **kwargs):\n",
        "    return optim.Adagrad(parameters, **kwargs)\n",
        "\n",
        "\n",
        "def _adam(parameters, **kwargs):\n",
        "    return optim.Adam(parameters, **kwargs)\n",
        "\n",
        "\n",
        "def _rmsprop(parameters, **kwargs):\n",
        "    return optim.RMSprop(parameters, **kwargs)\n",
        "\n",
        "\n",
        "def _sgd(parameters, **kwargs):\n",
        "    return optim.SGD(parameters, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P-1ROYLSGn90"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implementation of AdaFisher/AdaFisherW\"\"\"\n",
        "\n",
        "from typing import Callable, Dict, List, Union, Tuple, Type\n",
        "from torch import (Tensor, kron, is_grad_enabled, no_grad, zeros_like,\n",
        "                   preserve_format, ones_like, cat, einsum, sum, inf)\n",
        "from torch.optim import Optimizer\n",
        "import torch.distributed as dist\n",
        "from torch.nn.functional import pad\n",
        "from math import prod\n",
        "from torch.nn import Module, Linear, Conv2d, BatchNorm2d, LayerNorm, Parameter\n",
        "\n",
        "\n",
        "def smart_detect_inf(tensor: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Replaces positive infinity in the tensor with 1. and negative infinity with 0..\n",
        "\n",
        "    Parameters:\n",
        "    tensor (torch.Tensor): Input tensor that can have any dimension.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: A tensor with the same shape, dtype, and device as the input, where\n",
        "                  positive infinities are replaced by 1. and negative infinities by 0..\n",
        "    \"\"\"\n",
        "    result_tensor = tensor.clone()\n",
        "    result_tensor[tensor == inf] = 1.\n",
        "    result_tensor[tensor == -inf] = 0.\n",
        "    return result_tensor\n",
        "\n",
        "\n",
        "def MinMaxNormalization(tensor: Tensor, epsilon: float = 1e-6) -> Tensor:\n",
        "    \"\"\"\n",
        "    Scales tensor values to range [0,1] using min-max normalization.\n",
        "\n",
        "    Args:\n",
        "        tensor: Input tensor\n",
        "        epsilon: Small value to prevent division by zero (default: 1e-6)\n",
        "\n",
        "    Returns:\n",
        "        Normalized tensor with values in [0,1]\n",
        "    \"\"\"\n",
        "    tensor = smart_detect_inf(tensor)\n",
        "    min_tensor = tensor.min()\n",
        "    max_tensor = tensor.max()\n",
        "    range_tensor = max_tensor - min_tensor\n",
        "    return tensor.add_(-min_tensor).div_(range_tensor + epsilon)\n",
        "\n",
        "\n",
        "def update_running_avg(new: Tensor, current: Tensor, gamma: float):\n",
        "    \"\"\"\n",
        "    Updates exponential moving average of parameters in-place.\n",
        "\n",
        "    Args:\n",
        "        new: New parameter values\n",
        "        current: Current parameter values to update\n",
        "        gamma: Smoothing coefficient (0-1)\n",
        "    \"\"\"\n",
        "    current *= gamma * 1e-1\n",
        "    current += (gamma * 1e-2) * new\n",
        "\n",
        "\n",
        "def _extract_patches(x: Tensor, kernel_size: Tuple[int],\n",
        "                     stride: Tuple[int],\n",
        "                     padding: Tuple[int],\n",
        "                     groups: int) -> Tensor:\n",
        "\n",
        "    \"\"\"\n",
        "    Extracts sliding window patches from input feature maps for convolution operations.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor (batch_size, in_channels, height, width)\n",
        "        kernel_size: Height and width of kernel (h, w)\n",
        "        stride: Step size for sliding window (h, w)\n",
        "        padding: Input padding (h, w)\n",
        "        groups: Number of groups for grouped convolution\n",
        "\n",
        "    Returns:\n",
        "        Reshaped patches tensor (batch_size, output_h, output_w, in_channels * kernel_h * kernel_w)\n",
        "    \"\"\"\n",
        "\n",
        "    if padding[0] + padding[1] > 0:\n",
        "        x = pad(x, (padding[1], padding[1], padding[0], padding[0]))\n",
        "    batch_size, in_channels, height, width = x.size()\n",
        "    x = x.view(batch_size, groups, in_channels // groups, height, width)\n",
        "    x = x.unfold(3, kernel_size[0], stride[0])\n",
        "    x = x.unfold(4, kernel_size[1], stride[1])\n",
        "    x = x.permute(0, 1, 3, 4, 2, 5, 6).contiguous()\n",
        "    x = x.view(batch_size, groups, -1, in_channels // groups * kernel_size[0] * kernel_size[1])\n",
        "    x = x.view(batch_size, -1, x.size(2), x.size(3))\n",
        "    return x\n",
        "\n",
        "\n",
        "class Compute_H_D:\n",
        "    \"\"\"\n",
        "    Computes diagonal elements of activation covariance matrices for different neural network layers.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def compute_H_D(cls, h, layer) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes diagonal of activation covariance matrix.\n",
        "\n",
        "        Args:\n",
        "            h: Input activations\n",
        "            layer: PyTorch layer (Linear, Conv2d, BatchNorm2d, or LayerNorm)\n",
        "\n",
        "        Returns:\n",
        "            Diagonal elements of covariance matrix\n",
        "        \"\"\"\n",
        "        return cls.__call__(h, layer)\n",
        "\n",
        "    @classmethod\n",
        "    def __call__(cls, h: Tensor, layer: Module) -> Tensor:\n",
        "        \"\"\"\n",
        "        Delegates computation to layer-specific methods.\n",
        "\n",
        "        Args:\n",
        "            h: Input activations\n",
        "            layer: PyTorch layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance matrix diagonal\n",
        "        \"\"\"\n",
        "        if isinstance(layer, Linear):\n",
        "            H_D = cls.linear(h, layer)\n",
        "        elif isinstance(layer, Conv2d):\n",
        "            H_D = cls.conv2d(h, layer)\n",
        "        elif isinstance(layer, BatchNorm2d):\n",
        "            H_D = cls.batchnorm2d(h, layer)\n",
        "        elif isinstance(layer, LayerNorm):\n",
        "            H_D = cls.layernorm(h, layer)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return H_D\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d(h: Tensor, layer: Conv2d) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes covariance diagonal for Conv2d layer.\n",
        "\n",
        "        Args:\n",
        "            h: Input (batch_size, in_channels, height, width)\n",
        "            layer: Conv2d layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        batch_size = h.size(0)\n",
        "        h = _extract_patches(h, layer.kernel_size, layer.stride, layer.padding, layer.groups)\n",
        "        spatial_size = h.size(2) * h.size(3)\n",
        "        h = h.reshape(-1, h.size(-1))\n",
        "        if layer.bias is not None:\n",
        "            h_bar = cat([h, h.new(h.size(0), 1).fill_(1)], 1)\n",
        "        return einsum('ij,ij->j', h_bar, h_bar) / (batch_size * spatial_size) if layer.bias is not None \\\n",
        "            else einsum('ij,ij->j', h, h) / (batch_size * spatial_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def linear(h: Tensor, layer: Linear) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes covariance diagonal for Linear layer.\n",
        "\n",
        "        Args:\n",
        "            h: Input activations\n",
        "            layer: Linear layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        if len(h.shape) > 2:\n",
        "            h = h.reshape(-1, h.shape[-1])\n",
        "        batch_size = h.size(0)\n",
        "        if layer.bias is not None:\n",
        "            h_bar = cat([h, h.new(h.size(0), 1).fill_(1)], 1)\n",
        "        return einsum('ij,ij->j', h_bar, h_bar) / batch_size if layer.bias is not None \\\n",
        "            else einsum('ij,ij->j', h, h) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def batchnorm2d(h: Tensor, layer: BatchNorm2d) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes covariance diagonal for BatchNorm2d layer.\n",
        "\n",
        "        Args:\n",
        "            h: Input activations\n",
        "            layer: BatchNorm2d layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        batch_size, spatial_size = h.size(0), h.size(2) * h.size(3)\n",
        "        sum_h = sum(h, dim=(0, 2, 3)).unsqueeze(1) / (spatial_size ** 2)\n",
        "        h_bar = cat([sum_h, sum_h.new(sum_h.size(0), 1).fill_(1)], 1) # 1st column for H of beta parameter\n",
        "        return einsum('ij,ij->j', h_bar, h_bar) / (batch_size ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def layernorm(h: Tensor, layer: LayerNorm) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes covariance diagonal for LayerNorm layer.\n",
        "\n",
        "        Args:\n",
        "            h: Input activations\n",
        "            layer: LayerNorm layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        dim_to_reduce = [d for d in range(h.ndim) if d != 1] # T in the paper\n",
        "        batch_size, dim_norm = h.shape[0], prod([h.shape[dim] for dim in dim_to_reduce if dim != 0])\n",
        "        sum_h = sum(h, dim=dim_to_reduce).unsqueeze(1) / (dim_norm ** 2)\n",
        "        h_bar = cat([sum_h, sum_h.new(sum_h.size(0), 1).fill_(1)], 1) # 1st column for H of beta parameter\n",
        "        return einsum('ij,ij->j', h_bar, h_bar) / (batch_size ** 2)\n",
        "\n",
        "\n",
        "class Compute_S_D:\n",
        "    \"\"\"\n",
        "    Computes diagonal elements of gradient covariance matrices for different neural network layers.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def compute_S_D(cls, s: Tensor, layer: Module) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes gradient covariance matrix diagonal.\n",
        "\n",
        "        Args:\n",
        "            s: Output gradients\n",
        "            layer: PyTorch layer (Conv2d, Linear, BatchNorm2d, or LayerNorm)\n",
        "\n",
        "        Returns:\n",
        "            Diagonal elements of gradient covariance matrix\n",
        "        \"\"\"\n",
        "        return cls.__call__(s, layer)\n",
        "\n",
        "    @classmethod\n",
        "    def __call__(cls, s: Tensor, layer: Module) -> Tensor:\n",
        "        \"\"\"\n",
        "        Delegates computation to layer-specific methods.\n",
        "\n",
        "        Args:\n",
        "            s: Output gradients\n",
        "            layer: PyTorch layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance matrix diagonal\n",
        "        \"\"\"\n",
        "        if isinstance(layer, Conv2d):\n",
        "            S_D = cls.conv2d(s, layer)\n",
        "        elif isinstance(layer, Linear):\n",
        "            S_D = cls.linear(s, layer)\n",
        "        elif isinstance(layer, BatchNorm2d):\n",
        "            S_D = cls.batchnorm2d(s, layer)\n",
        "        elif isinstance(layer, LayerNorm):\n",
        "            S_D = cls.layernorm(s, layer)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return S_D\n",
        "\n",
        "    @staticmethod\n",
        "    def conv2d(s: Tensor, layer: Conv2d) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes gradient covariance diagonal for Conv2d.\n",
        "\n",
        "        Args:\n",
        "            s: Gradients (batch_size, n_filters, out_h, out_w)\n",
        "            layer: Conv2d layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        batch_size = s.shape[0]\n",
        "        spatial_size = s.size(2) * s.size(3)\n",
        "        s = s.transpose(1, 2).transpose(2, 3)\n",
        "        s = s.reshape(-1, s.size(-1))\n",
        "        return einsum('ij,ij->j', s, s) / (batch_size * spatial_size)\n",
        "\n",
        "    @staticmethod\n",
        "    def linear(s: Tensor, layer: Linear) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes gradient covariance diagonal for Linear layer.\n",
        "\n",
        "        Args:\n",
        "            s: Output gradients\n",
        "            layer: Linear layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        if len(s.shape) > 2:\n",
        "            s = s.reshape(-1, s.shape[-1])\n",
        "        batch_size = s.size(0)\n",
        "        return einsum('ij,ij->j', s, s) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def batchnorm2d(s: Tensor, layer: BatchNorm2d) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes gradient covariance diagonal for BatchNorm2d.\n",
        "\n",
        "        Args:\n",
        "            s: Output gradients\n",
        "            layer: BatchNorm2d layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        batch_size = s.size(0)\n",
        "        sum_s = sum(s, dim=(0, 2, 3))\n",
        "        return einsum('i,i->i', sum_s, sum_s) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def layernorm(s: Tensor, layer: LayerNorm) -> Tensor:\n",
        "        \"\"\"\n",
        "        Computes gradient covariance diagonal for LayerNorm.\n",
        "\n",
        "        Args:\n",
        "            s: Output gradients\n",
        "            layer: LayerNorm layer\n",
        "\n",
        "        Returns:\n",
        "            Covariance diagonal\n",
        "        \"\"\"\n",
        "        batch_size = s.size(0)\n",
        "        sum_s = sum(s, dim=tuple(range(s.ndim - 1)))\n",
        "        return einsum('i,i->i', sum_s, sum_s) / batch_size\n",
        "\n",
        "class AdaFisherBackBone(Optimizer):\n",
        "    \"\"\"\n",
        "    The AdaFisherBackBone class serves as the base class for optimizers that adjust model parameters\n",
        "    based on the Fisher Information Matrix to more efficiently navigate the curvature of the loss landscape.\n",
        "    This class is designed to work with neural network models and supports specific module types\n",
        "    for which Fisher Information can be effectively computed.\n",
        "\n",
        "    The class initializes with model-related parameters and configurations for the optimizer,\n",
        "    including learning rate adjustments and regularization techniques based on the second-order\n",
        "    information. It integrates advanced techniques such as dynamic preconditioning with the Fisher\n",
        "    Information to stabilize and speed up the convergence of the training process.\n",
        "\n",
        "    Attributes:\n",
        "        SUPPORTED_MODULES (Tuple[Type[str], ...]): A tuple listing the types of model modules (layers)\n",
        "        supported by this optimizer. Typical supported types include \"Linear\", \"Conv2d\",\n",
        "        \"BatchNorm2d\", and \"LayerNorm\".\n",
        "\n",
        "    Args:\n",
        "        model (Module): The neural network model to optimize.\n",
        "        lr (float, optional): Learning rate for the optimizer. Defaults to 1e-3.\n",
        "        beta (float, optional): The beta parameter for the optimizer, used in calculations\n",
        "                                like those in momentum. Defaults to 0.9.\n",
        "        Lambda (float, optional): Regularization parameter. Defaults to 1e-3.\n",
        "        gammas (List[float], optional): A list containing gamma parameters used in the\n",
        "                                        running average computations. Defaults to [0.92, 0.008].\n",
        "        TCov (int, optional): Interval in steps for recalculating the covariance matrices. Defaults to 100.\n",
        "        weight_decay (float, optional): Weight decay coefficient to help prevent overfitting. Defaults to 0.\n",
        "        dist (bool, optional): True when using a Mutli-GPUs environment, otherwise False.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any of the parameters are out of their expected ranges.\n",
        "    \"\"\"\n",
        "\n",
        "    SUPPORTED_MODULES: Tuple[Type[str], ...] = (\"Linear\", \"Conv2d\", \"BatchNorm2d\", \"LayerNorm\")\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: Module,\n",
        "                 lr: float = 1e-3,\n",
        "                 beta: float = 0.9,\n",
        "                 Lambda: float = 1e-3,\n",
        "                 gamma: float = 0.8,\n",
        "                 TCov: int = 100,\n",
        "                 weight_decay: float = 0,\n",
        "                 dist: bool = False\n",
        "                 ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= beta < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter: {beta}\")\n",
        "        if not 0.0 <= gamma < 1.0:\n",
        "            raise ValueError(f\"Invalid gamma parameter: {gamma}\")\n",
        "        if not TCov > 0:\n",
        "            raise ValueError(f\"Invalid TCov parameter: {TCov}\")\n",
        "        defaults = dict(lr=lr, beta=beta,\n",
        "                        weight_decay=weight_decay)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.Lambda = Lambda\n",
        "        self.model = model\n",
        "        self.TCov = TCov\n",
        "        self.dist = dist\n",
        "        self.steps = 0\n",
        "\n",
        "        # store vectors for the pre-conditioner\n",
        "        self.H_D: Dict[Module, Tensor] = {}\n",
        "        self.S_D: Dict[Module, Tensor] = {}\n",
        "        # save the layers of the network where FIM can be computed\n",
        "        self.modules: List[Module] = []\n",
        "        self.Compute_H_D = Compute_H_D()\n",
        "        self.Compute_S_D = Compute_S_D()\n",
        "        self._prepare_model()\n",
        "        super(AdaFisherBackBone, self).__init__(model.parameters(), defaults)\n",
        "\n",
        "    def _save_input(self, module: Module, input: Tensor, output: Tensor):\n",
        "        \"\"\"\n",
        "        Updates diagonal elements of activation covariance matrix periodically.\n",
        "\n",
        "        Args:\n",
        "            module: Neural network layer\n",
        "            input: Layer input tensor (first element used)\n",
        "            output: Layer output tensor (unused)\n",
        "\n",
        "         Note:\n",
        "            Attaches as forward hook to track layer activations for AdaFisher optimization.\n",
        "        \"\"\"\n",
        "        if is_grad_enabled() and self.steps % self.TCov == 0:\n",
        "            H_D_i = self.Compute_H_D(input[0].data, module)\n",
        "            if self.steps == 0:\n",
        "                self.H_D[module] = H_D_i.new(H_D_i.size(0)).fill_(1)\n",
        "            update_running_avg(MinMaxNormalization(H_D_i), self.H_D[module], self.gamma)\n",
        "\n",
        "\n",
        "    def _save_grad_output(self, module: Module, grad_input: Tensor, grad_output: Tensor):\n",
        "        \"\"\"\n",
        "        Updates diagonal elements of gradient covariance matrix periodically.\n",
        "\n",
        "        Args:\n",
        "            module: Neural network layer\n",
        "            grad_input: Input gradients (unused)\n",
        "            grad_output: Output gradients (first element used)\n",
        "\n",
        "        Note:\n",
        "            Attaches as backward hook to track layer gradients for AdaFisher optimization.\n",
        "        \"\"\"\n",
        "        if self.steps % self.TCov == 0:\n",
        "            S_D_i = self.Compute_S_D(grad_output[0].data, module)\n",
        "            if self.steps == 0:\n",
        "                self.S_D[module] = S_D_i.new(S_D_i.size(0)).fill_(1)\n",
        "            update_running_avg(MinMaxNormalization(S_D_i), self.S_D[module], self.gamma)\n",
        "\n",
        "    def _prepare_model(self):\n",
        "        \"\"\"\n",
        "        Registers hooks on supported layers to track activations and gradients.\n",
        "\n",
        "        Warning:\n",
        "            Avoid in-place operations (e.g., relu(inplace=True) or '*=') in model layers\n",
        "            as they can interfere with hook functionality.\n",
        "        \"\"\"\n",
        "        for module in self.model.modules():\n",
        "            classname = module.__class__.__name__\n",
        "            if classname in self.SUPPORTED_MODULES:\n",
        "                self.modules.append(module)\n",
        "                module.register_forward_hook(self._save_input)\n",
        "                module.register_full_backward_hook(self._save_grad_output)\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Save the Kronecker factors in the dictionary state of the optimizer\n",
        "\n",
        "        Returns:\n",
        "            Dict[str:List[Tensor]]: Dictionnary containing the states of the optimizer\n",
        "        \"\"\"\n",
        "        state_dict = super().state_dict()\n",
        "        # Save H_D and S_D as lists ordered by self.modules\n",
        "        state_dict['H_D'] = [self.H_D[module] for module in self.modules]\n",
        "        state_dict['S_D'] = [self.S_D[module] for module in self.modules]\n",
        "        state_dict['steps'] = self.steps\n",
        "        return state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Load the Kronecker factors from the dictionary state of the optimizer\n",
        "\n",
        "        Args:\n",
        "            state_dict (Dict[str:List[Tensor]]): Dictionnary containing the states of the optimizer\n",
        "        \"\"\"\n",
        "        H_D_list = state_dict.pop('H_D', [])\n",
        "        S_D_list = state_dict.pop('S_D', [])\n",
        "        self.steps = state_dict.pop('steps', 0)\n",
        "        # Load standard parameter states\n",
        "        super().load_state_dict(state_dict)\n",
        "        # Rebuild H_D and S_D dictionaries\n",
        "        self.H_D = {}\n",
        "        self.S_D = {}\n",
        "        for module, h_bar, s in zip(self.modules, H_D_list, S_D_list):\n",
        "            self.H_D[module] = h_bar\n",
        "            self.S_D[module] = s\n",
        "\n",
        "    def aggregate_kronecker_factors(self, module: Module):\n",
        "        \"\"\"\n",
        "        Aggregates Kronecker factors across multiple GPUs in a distributed setting.\n",
        "\n",
        "        This function performs an all-reduce operation to sum the Kronecker factors\n",
        "        `H_D` and `S_D` across all GPUs involved in the training. It then averages\n",
        "        these factors by dividing the summed values by the total number of GPUs (`world_size`).\n",
        "\n",
        "        Args:\n",
        "            module (Module): The neural network module for which the Kronecker factors\n",
        "                            `H_D` and `S_D` are being aggregated.\n",
        "        \"\"\"\n",
        "        dist.all_reduce(self.H_D[module], op=dist.ReduceOp.SUM)\n",
        "        dist.all_reduce(self.S_D[module], op=dist.ReduceOp.SUM)\n",
        "\n",
        "        self.H_D[module] /= dist.get_world_size()\n",
        "        self.S_D[module] /= dist.get_world_size()\n",
        "\n",
        "    def _get_F_tilde(self, module: Module):\n",
        "        \"\"\"\n",
        "        Computes Fisher Information Matrix with regularization for parameter updates.\n",
        "\n",
        "        Args:\n",
        "            module: Neural network layer\n",
        "\n",
        "        Returns:\n",
        "            - For layers with bias: [weight_update, bias_update]\n",
        "            - For layers without bias: weight_update tensor\n",
        "\n",
        "        Note:\n",
        "            Uses Kronecker product of activation and gradient covariance diagonals.\n",
        "        \"\"\"\n",
        "        # Fisher Computation\n",
        "        if self.dist:\n",
        "            self.aggregate_kronecker_factors(module = module)\n",
        "        F_tilde = kron(self.H_D[module].unsqueeze(1), self.S_D[module].unsqueeze(0)).t() + self.Lambda\n",
        "        if module.bias is not None:\n",
        "            F_tilde = [F_tilde[:, :-1], F_tilde[:, -1:]]\n",
        "            F_tilde[0] = F_tilde[0].view(*module.weight.grad.data.size())\n",
        "            F_tilde[1] = F_tilde[1].view(*module.bias.grad.data.size())\n",
        "            return F_tilde\n",
        "        else:\n",
        "            return F_tilde.reshape(module.weight.grad.data.size())\n",
        "\n",
        "    def _check_dim(self, param: List[Parameter], idx_module: int, idx_param: int) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if the dimensions of a given parameter match the dimensions of its corresponding module's weights or bias.\n",
        "        This function is crucial for ensuring that the Fisher information is available for a specific module.\n",
        "\n",
        "        Parameters:\n",
        "            - param (List[Parameter]): A list of all parameters within the model.\n",
        "            - idx_module (int): The index of the module within the model's modules list.\n",
        "            - idx_param (int): The index of the parameter within the list of parameters being checked.\n",
        "\n",
        "        Returns:\n",
        "            - bool: True if the parameter's dimensions match those of the corresponding module's weights or bias\n",
        "            (Fisher information is available);\n",
        "            False otherwise.\n",
        "        \"\"\"\n",
        "        params = param[idx_param]\n",
        "        module = self.modules[idx_module]\n",
        "        param_size = params.data.size()\n",
        "        return param_size == module.weight.data.size() or (module.bias is not None and param_size == module.bias.data.size())\n",
        "\n",
        "\n",
        "class AdaFisher(AdaFisherBackBone):\n",
        "    \"\"\"AdaFisher Optimizer: An adaptive learning rate optimizer that leverages Fisher Information for parameter updates.\n",
        "\n",
        "       This class AdaFisher optimizer extends traditional optimization techniques by incorporating Fisher Information to\n",
        "       adaptively adjust the learning rates based on the curvature of the loss landscape. This approach aims to enhance\n",
        "       convergence stability and speed by scaling updates according to the inverse curvature of the parameter space,\n",
        "       making it particularly suited for deep learning models where the loss landscape can be highly non-convex.\n",
        "\n",
        "       Key Features:\n",
        "           - Adaptive Learning Rates: Adjusts learning rates based on Fisher Information, potentially leading to faster\n",
        "           convergence by taking more informed steps.\n",
        "           - Curvature-aware Updates: Utilizes the curvature of the loss landscape to modulate the update steps, aiming\n",
        "           to improve optimization efficiency.\n",
        "           - Support for Custom Modules: Designed to work with a wide range of neural network architectures by allowing\n",
        "           flexible mappings between parameters and modules.\n",
        "\n",
        "       Usage:\n",
        "       AdaFisher should be used similarly to other PyTorch optimizers, with the additional step of preparing the model\n",
        "       by registering necessary hooks to compute Fisher Information:\n",
        "\n",
        "       ```python\n",
        "       import torch\n",
        "       model = MyModel()\n",
        "       optimizer = AdaFisher(model, lr=1e-3, beta=0.9, gamma=[0.92, 0.008], Lambda=1e-3, weight_decay=0)\n",
        "\n",
        "       for input, target in dataset:\n",
        "           optimizer.zero_grad()\n",
        "           output = model(input)\n",
        "           loss = loss_fn(output, target)\n",
        "           loss.backward()\n",
        "           optimizer.step()\n",
        "       ```\n",
        "       \"\"\"\n",
        "    def __init__(self,\n",
        "                 model: Module,\n",
        "                 lr: float = 1e-3,\n",
        "                 beta: float = 0.9,\n",
        "                 Lambda: float = 1e-3,\n",
        "                 gamma: float = 0.8,\n",
        "                 TCov: int = 100,\n",
        "                 weight_decay: float = 0,\n",
        "                 dist: bool = False\n",
        "                ):\n",
        "        super(AdaFisher, self).__init__(model,\n",
        "                                        lr = lr,\n",
        "                                        beta = beta,\n",
        "                                        Lambda = Lambda,\n",
        "                                        gamma = gamma,\n",
        "                                        TCov = TCov,\n",
        "                                        dist = dist,\n",
        "                                        weight_decay = weight_decay)\n",
        "\n",
        "    @no_grad()\n",
        "    def _step(self, hyperparameters: Dict[str, float], param: Parameter, F_tilde: Tensor):\n",
        "        \"\"\"\n",
        "        Updates a single parameter using AdaFisher optimization step.\n",
        "\n",
        "        Args:\n",
        "            hyperparameters: Dict containing 'lr', 'beta', and 'weight_decay'\n",
        "            param: Parameter tensor to update\n",
        "            F_tilde: Fisher information tensor for parameter\n",
        "\n",
        "        Note:\n",
        "            Uses bias-corrected moving averages and Fisher information for adaptive updates.\n",
        "            Modifies parameter in-place.\n",
        "        \"\"\"\n",
        "        grad = param.grad\n",
        "        state = self.state[param]\n",
        "        # State initialization\n",
        "        if len(state) == 0:\n",
        "            state['step'] = 0\n",
        "            # Exponential moving average of gradient values\n",
        "            state['exp_avg'] = zeros_like(\n",
        "                param, memory_format=preserve_format)\n",
        "        exp_avg = state['exp_avg']\n",
        "        beta = hyperparameters['beta']\n",
        "        state['step'] += 1\n",
        "        bias_correction = 1 - beta ** state['step']\n",
        "        if hyperparameters['weight_decay'] != 0:\n",
        "            grad = grad.add(param, alpha=hyperparameters['weight_decay'])\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta).add_(grad, alpha= 1 - beta)\n",
        "        step_size = hyperparameters['lr'] / bias_correction\n",
        "        # Update Rule\n",
        "        param.addcdiv_(exp_avg, F_tilde, value=-step_size)\n",
        "\n",
        "    @no_grad()\n",
        "    def step(self, closure: Union[None, Callable[[], Tensor]] = None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step across all parameter groups of the model.\n",
        "        This method iterates over each parameter group, updating parameters based on their gradients, a set of\n",
        "        hyperparameters, and update tensors calculated for each corresponding module. The update process carefully\n",
        "        tracks the relationship between parameters and their corresponding modules, using index tracking and dimension\n",
        "        checks to ensure updates are applied accurately.\n",
        "\n",
        "        Arguments:\n",
        "            - closure (callable, optional): A closure that reevaluates the model and returns the loss. Currently,\n",
        "            the use of closure is not supported, and attempting to provide one will raise a NotImplementedError.\n",
        "\n",
        "        The update process involves:\n",
        "            1. Iterating through each parameter group and its corresponding hyperparameters.\n",
        "            2. For each parameter, checking if a gradient is available. If not, it skips to the next parameter.\n",
        "            3. Using index tracking to associate parameters with their corresponding modules, considering cases\n",
        "            where parameters do not have gradients or are part of buffer counts.\n",
        "            4. Performing a dimensionality check to ensure compatibility between the parameter and module,\n",
        "            then calculating the update curvature tensor.\n",
        "            5. Applying the update curvature using a custom step function that incorporates the calculated update\n",
        "            tensor and hyperparameters.\n",
        "        \"\"\"\n",
        "        if closure is not None:\n",
        "            raise NotImplementedError(\"Closure not supported.\")\n",
        "        for group in self.param_groups:\n",
        "            idx_param, idx_module, buffer_count = 0, 0, 0\n",
        "            param, hyperparameters = group['params'], {\"weight_decay\": group['weight_decay'], \"beta\": group['beta'], \"lr\": group['lr']}\n",
        "            for _ in range(len(self.modules)):\n",
        "                if param[idx_param].grad is None:\n",
        "                    idx_param += 1\n",
        "                    if param[idx_param].ndim > 1:\n",
        "                        idx_module += 1\n",
        "                    else:\n",
        "                        buffer_count += 1\n",
        "                    if buffer_count == 2:\n",
        "                        idx_module += 1\n",
        "                        buffer_count = 0\n",
        "                    continue\n",
        "                m = self.modules[idx_module]\n",
        "                if self._check_dim(param, idx_module, idx_param):\n",
        "                    F_tilde = self._get_F_tilde(m)\n",
        "                    idx_module += 1\n",
        "                else:\n",
        "                    F_tilde = ones_like(param[idx_param])\n",
        "                if isinstance(F_tilde, list):\n",
        "                    for F_tilde_i in F_tilde:\n",
        "                        self._step(hyperparameters, param[idx_param], F_tilde_i)\n",
        "                        idx_param += 1\n",
        "                else:\n",
        "                    self._step(hyperparameters, param[idx_param], F_tilde)\n",
        "                    idx_param += 1\n",
        "        self.steps += 1\n",
        "\n",
        "\n",
        "class AdaFisherW(AdaFisherBackBone):\n",
        "    \"\"\"AdaFisherW Optimizer: An adaptive learning rate optimizer that leverages Fisher Information for parameter updates.\n",
        "\n",
        "   The AdaFisherW optimizer extends traditional optimization techniques by incorporating Fisher Information to\n",
        "   adaptively adjust the learning rates based on the curvature of the loss landscape. This approach aims to enhance\n",
        "   convergence stability and speed by scaling updates according to the inverse curvature of the parameter space,\n",
        "   making it particularly suited for deep learning models where the loss landscape can be highly non-convex.\n",
        "\n",
        "   Key Features:\n",
        "       - Adaptive Learning Rates: Adjusts learning rates based on Fisher Information, potentially leading to faster\n",
        "       convergence by taking more informed steps.\n",
        "       - Curvature-aware Updates: Utilizes the curvature of the loss landscape to modulate the update steps, aiming\n",
        "       to improve optimization efficiency.\n",
        "       - Support for Custom Modules: Designed to work with a wide range of neural network architectures by allowing\n",
        "       flexible mappings between parameters and modules.\n",
        "\n",
        "   Usage:\n",
        "   AdaFisherW should be used similarly to other PyTorch optimizers, with the additional step of preparing the model\n",
        "   by registering necessary hooks to compute Fisher Information:\n",
        "\n",
        "   ```python\n",
        "   import torch\n",
        "   model = MyModel()\n",
        "   optimizer = AdaFisherW(model, lr=1e-3, beta=0.9, gamma=[0.98, 0.008], Lambda=1e-3, weight_decay=0)\n",
        "\n",
        "   for input, target in dataset:\n",
        "       optimizer.zero_grad()\n",
        "       output = model(input)\n",
        "       loss = loss_fn(output, target)\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "   ```\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model: Module,\n",
        "                 lr: float = 1e-3,\n",
        "                 beta: float = 0.9,\n",
        "                 Lambda: float = 1e-3,\n",
        "                 gamma: float = 0.8,\n",
        "                 TCov: int = 100,\n",
        "                 weight_decay: float = 0,\n",
        "                 dist: bool = False\n",
        "                ):\n",
        "        super(AdaFisherW, self).__init__(model,\n",
        "                                        lr = lr,\n",
        "                                        beta = beta,\n",
        "                                        Lambda = Lambda,\n",
        "                                        gamma = gamma,\n",
        "                                        TCov = TCov,\n",
        "                                        dist = dist,\n",
        "                                        weight_decay = weight_decay)\n",
        "\n",
        "    @no_grad()\n",
        "    def _step(self, hyperparameters: Dict[str, float], param: Parameter, F_tilde: Tensor):\n",
        "        \"\"\"\n",
        "        Updates a single parameter using AdaFisherW optimization step.\n",
        "\n",
        "        Args:\n",
        "            hyperparameters: Dict containing 'lr', 'beta', and 'weight_decay'\n",
        "            param: Parameter tensor to update\n",
        "            F_tilde: Fisher information tensor for parameter\n",
        "\n",
        "        Note:\n",
        "            Uses bias-corrected moving averages and Fisher information for adaptive updates.\n",
        "            Modifies parameter in-place.\n",
        "        \"\"\"\n",
        "        grad = param.grad\n",
        "        state = self.state[param]\n",
        "        # State initialization\n",
        "        if len(state) == 0:\n",
        "            state['step'] = 0\n",
        "            # Exponential moving average of gradient values\n",
        "            state['exp_avg'] = zeros_like(\n",
        "                param, memory_format=preserve_format)\n",
        "        exp_avg = state['exp_avg']\n",
        "        beta = hyperparameters['beta']\n",
        "        state['step'] += 1\n",
        "        bias_correction = 1 - beta ** state['step']\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta).add_(grad, alpha= 1 - beta)\n",
        "        param.data -= hyperparameters['lr'] * (exp_avg / bias_correction / F_tilde + hyperparameters['weight_decay'] * param.data)\n",
        "\n",
        "\n",
        "    @no_grad()\n",
        "    def step(self, closure: Union[None, Callable[[], Tensor]] = None):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step across all parameter groups of the model.\n",
        "        This method iterates over each parameter group, updating parameters based on their gradients, a set of\n",
        "        hyperparameters, and update tensors calculated for each corresponding module. The update process carefully\n",
        "        tracks the relationship between parameters and their corresponding modules, using index tracking and dimension\n",
        "        checks to ensure updates are applied accurately.\n",
        "\n",
        "        Arguments:\n",
        "            - closure (callable, optional): A closure that reevaluates the model and returns the loss. Currently,\n",
        "            the use of closure is not supported, and attempting to provide one will raise a NotImplementedError.\n",
        "\n",
        "        The update process involves:\n",
        "            1. Iterating through each parameter group and its corresponding hyperparameters.\n",
        "            2. For each parameter, checking if a gradient is available. If not, it skips to the next parameter.\n",
        "            3. Using index tracking to associate parameters with their corresponding modules, considering cases\n",
        "            where parameters do not have gradients or are part of buffer counts.\n",
        "            4. Performing a dimensionality check to ensure compatibility between the parameter and module,\n",
        "            then calculating the update curvature tensor.\n",
        "            5. Applying the update curvature using a custom step function that incorporates the calculated update\n",
        "            tensor and hyperparameters.\n",
        "        \"\"\"\n",
        "        if closure is not None:\n",
        "            raise NotImplementedError(\"Closure not supported.\")\n",
        "        for group in self.param_groups:\n",
        "            idx_param, idx_module, buffer_count = 0, 0, 0\n",
        "            param, hyperparameters = group['params'], {\"weight_decay\": group['weight_decay'], \"beta\": group['beta'], \"lr\": group['lr']}\n",
        "            for _ in range(len(self.modules)):\n",
        "                if param[idx_param].grad is None:\n",
        "                    idx_param += 1\n",
        "                    if param[idx_param].ndim > 1:\n",
        "                        idx_module += 1\n",
        "                    else:\n",
        "                        buffer_count += 1\n",
        "                    if buffer_count == 2:\n",
        "                        idx_module += 1\n",
        "                        buffer_count = 0\n",
        "                    continue\n",
        "                m = self.modules[idx_module]\n",
        "                if self._check_dim(param, idx_module, idx_param):\n",
        "                    F_tilde = self._get_F_tilde(m)\n",
        "                    idx_module += 1\n",
        "                else:\n",
        "                    F_tilde = ones_like(param[idx_param])\n",
        "                if isinstance(F_tilde, list):\n",
        "                    for F_tilde_i in F_tilde:\n",
        "                        self._step(hyperparameters, param[idx_param], F_tilde_i)\n",
        "                        idx_param += 1\n",
        "                else:\n",
        "                    self._step(hyperparameters, param[idx_param], F_tilde)\n",
        "                    idx_param += 1\n",
        "        self.steps += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko-1JMkge4bw"
      },
      "source": [
        "Besides the classical optimization above, we are also going to use some newly-proposed optimization algorithm, which are KOALA, proposed in [paper](https://arxiv.org/pdf/2107.03331)), and also some other optimizers worth discovering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkqkk49QiNNg"
      },
      "source": [
        "The first one is KOALA, an optimization algorithm inspired by the Extended Kalman Filter (EKF). It introduces a dynamic learning rate adjustment mechanism using state estimation and covariance modeling, making it highly effective for large-scale neural network optimization.\n",
        "\n",
        "### **Mathematical Foundation**\n",
        "KOALA is based on EKF principles and consists of two main steps:\n",
        "\n",
        "1. **Predict Step:**\n",
        "   - State Prediction:\n",
        "     $\\hat{x}_k = f_k(\\hat{x}_{k-1})$, In KOALA-V,  $f_k(\\hat{x}_{k-1})=\\hat{x}_{k-1}$\n",
        "   - Covariance Prediction:\n",
        "     $\\hat{P}_k = F_k \\hat{P}_{k-1} F_k^T + Q_k$, In Koala-V, $Q_k$ is a constant scalar\n",
        "\n",
        "2. **Update Step:**\n",
        "   - Kalman Gain:\n",
        "     $K_k = \\hat{P}_k H_k^T (H_k \\hat{P}_k H_k^T + R_k)^{-1}$\n",
        "   - State Update:\n",
        "     $x_k = \\hat{x}_k + K_k (z_k - h_k(\\hat{x}_k))$\n",
        "   - Covariance Update:\n",
        "     $P_k = (I - K_k H_k) \\hat{P}_k$\n",
        "\n",
        "### **Optimization Version (KOALA-V)**\n",
        "In optimization tasks, these equations are adapted to update model parameters. The key update rule is:\n",
        "\n",
        "- $x_k = x_k - \\frac{\\hat{P}_k (\\hat{L}_k(x_k) - \\hat{L}_k^{\\text{target}})}{\\hat{P}_k \\|\\nabla \\hat{L}_k(x_k)\\|^2 + R} \\nabla \\hat{L}_k(x_k)$\n",
        "- The derivation needs two approximations (at least from my understanding):\n",
        "    - $h_k(\\hat{x}_k)=h_k(x_k)$\n",
        "    - $\\nabla \\hat{h}_k(x_k)=\\nabla \\hat{L}_k(x_k)$\n",
        "\n",
        "\n",
        "\n",
        "This dynamically scales the gradient step based on the state covariance $\\hat{P}_k$, the loss gradient norm $\\|\\nabla \\hat{L}_k(x_k)\\|^2$, and the noise covariance $R$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dw5HqdE1h_sN"
      },
      "outputs": [],
      "source": [
        "class ExpAverage(object):\n",
        "    def __init__(self, alpha, init_val=0):\n",
        "        self.val = init_val\n",
        "        self.avg = init_val\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def update(self, val):\n",
        "        self.val = val\n",
        "        self.avg = self.alpha * self.avg + (1 - self.alpha) * val\n",
        "\n",
        "    def get_avg(self):\n",
        "        return self.avg\n",
        "\n",
        "    def get_last_val(self):\n",
        "        return self.val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GemPDWm4iCrG"
      },
      "outputs": [],
      "source": [
        "class KOALABase(torch.optim.Optimizer):\n",
        "    def __init__(self, params, **kwargs):\n",
        "        defaults = dict(**kwargs)\n",
        "        super(KOALABase, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, loss: torch.FloatTensor, loss_var: torch.FloatTensor):\n",
        "        pass\n",
        "\n",
        "\n",
        "class VanillaKOALA(KOALABase):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            sigma: float = 1,\n",
        "            q: float = 1,\n",
        "            r: float = None,\n",
        "            alpha_r: float = 0.9,\n",
        "            weight_decay: float = 0.0,\n",
        "            lr: float = 1,\n",
        "            **kwargs):\n",
        "        \"\"\"\n",
        "        Implementation of the KOALA-V(Vanilla) optimizer\n",
        "\n",
        "        :param params: parameters to optimize\n",
        "        :param sigma: initial value of P_k\n",
        "        :param q: fixed constant Q_k\n",
        "        :param r: fixed constant R_k (None for online estimation)\n",
        "        :param alpha_r: smoothing coefficient for online estimation of R_k\n",
        "        :param weight_decay: weight decay\n",
        "        :param lr: learning rate\n",
        "        :param kwargs:\n",
        "        \"\"\"\n",
        "        super(VanillaKOALA, self).__init__(params, **kwargs)\n",
        "\n",
        "        self.eps = 1e-9\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            group[\"lr\"] = lr\n",
        "\n",
        "        # Initialize state\n",
        "        self.state[\"sigma\"] = sigma\n",
        "        self.state[\"q\"] = q\n",
        "        if r is not None:\n",
        "            self.state[\"r\"] = r\n",
        "        else:\n",
        "            self.state[\"r\"] = ExpAverage(alpha_r, 1.0)\n",
        "        self.state[\"weight_decay\"] = weight_decay\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self):\n",
        "        self.state[\"sigma\"] += self.state[\"q\"]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, loss: torch.FloatTensor, loss_var: torch.FloatTensor):\n",
        "        # 如果R_k是动态估计的 (ExpAverage)，则更新，否则使用固定值\n",
        "        if isinstance(self.state[\"r\"], ExpAverage):\n",
        "            self.state[\"r\"].update(loss_var)\n",
        "            cur_r = self.state[\"r\"].get_avg()\n",
        "        else:\n",
        "            cur_r = self.state[\"r\"]\n",
        "\n",
        "        max_grad_entries = list()\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None or p.grad.norm(p=2) < self.eps:\n",
        "                    continue\n",
        "\n",
        "                layer_grad = p.grad + self.state[\"weight_decay\"] * p\n",
        "                layer_grad_norm = layer_grad.norm(p=2)\n",
        "\n",
        "                s = self.state[\"sigma\"] * (layer_grad_norm ** 2) + cur_r\n",
        "\n",
        "                layer_loss = loss + 0.5 * self.state[\"weight_decay\"] * p.norm(p=2) ** 2\n",
        "                scale = group[\"lr\"] * layer_loss * self.state[\"sigma\"] / s\n",
        "                p.data.add_(-scale * p.grad)\n",
        "\n",
        "                max_grad_entries.append(layer_grad_norm ** 2 / s)\n",
        "\n",
        "        hh_approx = torch.max(torch.stack(max_grad_entries))\n",
        "\n",
        "        self.state[\"sigma\"] -= self.state[\"sigma\"] ** 2 * hh_approx\n",
        "\n",
        "\n",
        "class MomentumKOALA(KOALABase):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            sw: float = 1e-1,\n",
        "            sc: float = 0,\n",
        "            sv: float = 1e-1,\n",
        "            a: float = 0.9,\n",
        "            qw: float = 1e-2,\n",
        "            qv: float = 1e-2,\n",
        "            r: float = None,\n",
        "            alpha_r: float = 0.9,\n",
        "            weight_decay: float = 0.0,\n",
        "            lr: float = 1,\n",
        "            **kwargs):\n",
        "        \"\"\"\n",
        "        Implementation of the KOALA-M(Momentum) optimizer\n",
        "\n",
        "        :param params: parameters to optimize\n",
        "        :param sw: initial value of P_k for states\n",
        "        :param sc: initial value of out of diagonal entries of P_k\n",
        "        :param sv: initial value of P_k for velocities\n",
        "        :param a: decay coefficient for velocities\n",
        "        :param qw: fixed constant Q_k for states\n",
        "        :param qv: fixed constant Q_k for velocities\n",
        "        :param r: fixed constant R_k (None for online estimation)\n",
        "        :param alpha_r: smoothing coefficient for online estimation of R_k\n",
        "        :param weight_decay: weight decay\n",
        "        :param lr: learning rate\n",
        "        :param kwargs:\n",
        "        \"\"\"\n",
        "        super(MomentumKOALA, self).__init__(params, **kwargs)\n",
        "\n",
        "        self.eps = 1e-9\n",
        "\n",
        "        self.shared_device = self.param_groups[0][\"params\"][0].device\n",
        "        self.dtype = torch.double\n",
        "\n",
        "        # Initialize velocities and count params\n",
        "        self.total_params = 0\n",
        "        for group in self.param_groups:\n",
        "            group[\"lr\"] = lr\n",
        "            for p in group[\"params\"]:\n",
        "                self.state[p][\"vt\"] = p.new_zeros(p.shape)\n",
        "                self.state[p][\"gt\"] = p.new_zeros(p.shape)\n",
        "                self.total_params += torch.prod(torch.Tensor(list(p.size())).to(self.shared_device))\n",
        "\n",
        "        # Define state\n",
        "        self.state[\"Pt\"] = torch.Tensor([\n",
        "            [sw, sc],\n",
        "            [sc, sv]\n",
        "        ]).to(self.shared_device).to(self.dtype)\n",
        "\n",
        "        self.state[\"qw\"] = ExpAverage(0.9, qw)\n",
        "        self.state[\"qv\"] = qv\n",
        "        self.state[\"Q\"] = torch.diag(\n",
        "            torch.Tensor([self.state[\"qw\"].get_avg(), self.state[\"qv\"]])\n",
        "        ).to(self.shared_device).to(self.dtype)\n",
        "\n",
        "        if r is not None:\n",
        "            self.state[\"R\"] = r\n",
        "        else:\n",
        "            self.state[\"R\"] = ExpAverage(alpha_r, 1.0)\n",
        "\n",
        "        f = [[1, 1], [0, a]]\n",
        "        self.state[\"F\"] = torch.Tensor(f).to(self.shared_device).to(self.dtype)\n",
        "\n",
        "        self.state[\"weight_decay\"] = weight_decay\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self):\n",
        "        wdiff = list()\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                pw_diff = (self.state[p][\"gt\"] - p).norm(p=2).to(self.shared_device)\n",
        "                wdiff.append(pw_diff)\n",
        "\n",
        "                p.mul_(self.state[\"F\"][0, 0].to(p.device))\n",
        "                p.add_(self.state[p][\"vt\"] * self.state[\"F\"][0, 1].to(p.device))\n",
        "                self.state[p][\"vt\"].mul_(self.state[\"F\"][1, 1].to(p.device))\n",
        "                self.state[p][\"vt\"].add_(p * self.state[\"F\"][1, 0].to(p.device))\n",
        "\n",
        "        norm_wdiff = torch.stack(wdiff).norm(p=2) / self.total_params\n",
        "        self.state[\"qw\"].update(norm_wdiff)\n",
        "        self.state[\"Q\"] = torch.diag(\n",
        "            torch.Tensor([self.state[\"qw\"].get_avg(), self.state[\"qv\"]])\n",
        "        ).to(self.shared_device).to(self.dtype)\n",
        "\n",
        "        self.state[\"Pt\"] = torch.matmul(\n",
        "            torch.matmul(self.state[\"F\"], self.state[\"Pt\"]), self.state[\"F\"].t())\n",
        "        self.state[\"Pt\"].add_(self.state[\"Q\"])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, loss: torch.FloatTensor, loss_var: torch.FloatTensor):\n",
        "        if isinstance(self.state[\"R\"], ExpAverage):\n",
        "            self.state[\"R\"].update(loss_var.to(self.shared_device))\n",
        "            cur_r = self.state[\"R\"].get_avg()\n",
        "        else:\n",
        "            cur_r = self.state[\"R\"]\n",
        "\n",
        "        max_grad_entries = list()\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None or p.grad.norm(p=2) < self.eps:\n",
        "                    continue\n",
        "\n",
        "                layer_grad = p.grad + self.state[\"weight_decay\"] * p\n",
        "                layer_grad_norm = layer_grad.norm(p=2)\n",
        "\n",
        "                S = layer_grad_norm ** 2 * self.state[\"Pt\"][0, 0] + cur_r\n",
        "\n",
        "                layer_loss = loss.to(self.shared_device) + 0.5 * self.state[\"weight_decay\"] * p.norm(p=2) ** 2\n",
        "                K1 = self.state[\"Pt\"][0, 0] / S * layer_loss * group[\"lr\"]\n",
        "                K2 = self.state[\"Pt\"][1, 0] / S * layer_loss * group[\"lr\"]\n",
        "\n",
        "                # Update weights and velocities\n",
        "                p.sub_((K1 * layer_grad).to(p.device))\n",
        "                self.state[p][\"vt\"].sub_((K2 * layer_grad).to(p.device))\n",
        "\n",
        "                self.state[p][\"gt\"].mul_(0.9)\n",
        "                self.state[p][\"gt\"].add_(0.1 * p)\n",
        "\n",
        "                max_grad_entries.append(layer_grad_norm ** 2 / S)\n",
        "\n",
        "        hh_approx = torch.max(torch.stack(max_grad_entries))\n",
        "\n",
        "        # Update covariance\n",
        "        HHS = torch.Tensor([\n",
        "            [hh_approx, 0],\n",
        "            [0, 0]\n",
        "        ]).to(self.shared_device).to(self.dtype)\n",
        "        PHHS = torch.matmul(self.state[\"Pt\"], HHS)\n",
        "        PHHSP = torch.matmul(PHHS, self.state[\"Pt\"].t())\n",
        "        self.state[\"Pt\"] = self.state[\"Pt\"] - PHHSP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE9zmzgQ_ozC"
      },
      "source": [
        "# KOALA++\n",
        "\n",
        "Init: $v_{0}$, $H_{0}$\n",
        "\n",
        "We know: $S_{k-1}, H_{k-1}, v_{k-1}, H_{k}$\n",
        "\n",
        "for i in range(1, T):\n",
        "* $\\lambda_{k}=\\frac{H_{k}(v_{k-1}^T+QH_{K-1}^T)}{S_{k-1}}$\n",
        "* $v_{k}=(1-\\lambda_{k})v_{k-1}+(H_{k}-\\lambda_{k}H_{k-1})Q$\n",
        "* $S_{k}=v_{k}H_{k}^T+H_{k}QH_{k}^T+R$\n",
        "* $x_{k}=x_{k-1}-\\frac{(\\hat{L}_k(x_k) - \\hat{L}_k^{\\text{target}})}{S_{k}}$$(v_{k}^T+QH_{k}^T)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2QYm5KEf4Js1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class KOALAPlusPlus(KOALABase):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            sigma: float = 1,\n",
        "            q: float = 1,\n",
        "            r: float = None,\n",
        "            alpha_r: float = 0.9,\n",
        "            weight_decay: float = 0.0,\n",
        "            lr: float = 1,\n",
        "            **kwargs):\n",
        "        super(KOALAPlusPlus, self).__init__(params, **kwargs)\n",
        "        self.eps = 1e-9\n",
        "        for group in self.param_groups:\n",
        "            group[\"lr\"] = lr\n",
        "\n",
        "        # 初始化状态（常量以数值形式存储）\n",
        "        self.state = {}\n",
        "        self.state[\"sigma\"] = sigma  # σ_0\n",
        "        self.state[\"q\"] = q          # Q\n",
        "        if r is not None:\n",
        "            self.state[\"r\"] = r\n",
        "        else:\n",
        "            self.state[\"r\"] = ExpAverage(alpha_r, 1.0)\n",
        "        self.state[\"weight_decay\"] = weight_decay\n",
        "\n",
        "        # 初始化每个参数状态（存储 vk、Hk、Sk、Pk）\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                self.state[p] = {}\n",
        "                self.state[p][\"vk\"] = None\n",
        "                self.state[p][\"Hk\"] = None\n",
        "                self.state[p][\"Sk\"] = None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    # @torch.cuda.amp.autocast()\n",
        "    def update(self, loss: torch.FloatTensor, loss_var: torch.FloatTensor):\n",
        "        # 更新 r 状态\n",
        "        if isinstance(self.state[\"r\"], ExpAverage):\n",
        "            self.state[\"r\"].update(loss_var)\n",
        "            cur_r = self.state[\"r\"].get_avg()\n",
        "        else:\n",
        "            cur_r = self.state[\"r\"]\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None or p.grad.norm(2) < self.eps:\n",
        "                    continue\n",
        "\n",
        "                p_shape = p.shape\n",
        "                # 获取上次状态\n",
        "                vk_prev = self.state[p][\"vk\"]\n",
        "                Hk_prev = self.state[p][\"Hk\"]\n",
        "                Sk_prev = self.state[p][\"Sk\"]\n",
        "\n",
        "                Q = self.state[\"q\"]\n",
        "                sigma = self.state[\"sigma\"]\n",
        "\n",
        "                # 计算当前梯度 Hk（拉平成向量），使用 in-place 加法\n",
        "                Hk = p.grad.view(-1) + self.state[\"weight_decay\"] * p.view(-1)\n",
        "                # 初始化 vk_prev、Hk_prev 如不存在\n",
        "                if vk_prev is None:\n",
        "                    vk_prev = Hk.mul(sigma)\n",
        "                if Hk_prev is None:\n",
        "                    Hk_prev = Hk\n",
        "                # 如果 Sk_prev 未定义，则计算\n",
        "                if Sk_prev is None:\n",
        "                    # 计算 dot(vk_prev, Hk_prev) 和 dot(Hk_prev, Hk_prev)\n",
        "                    Sk_prev = torch.dot(vk_prev.add(Q, alpha=1.0).mul_(Hk_prev), Hk_prev).add_(cur_r)\n",
        "\n",
        "                # Compute lambda_k\n",
        "                lambdak = torch.dot(Hk, vk_prev + Q * Hk_prev) / Sk_prev\n",
        "                alpha = torch.dot(Hk, Hk_prev) / torch.dot(Hk_prev, Hk_prev)\n",
        "                r_k = torch.dot(Hk, vk_prev) * torch.dot(Hk_prev, Hk_prev) - torch.dot(Hk_prev, vk_prev) * torch.dot(Hk, Hk_prev)\n",
        "                r_k /= torch.dot(Hk_prev, Hk_prev) ** 2\n",
        "                vk = (alpha - lambdak) * vk_prev + Q * (Hk - lambdak * Hk_prev) + r_k * Hk_prev\n",
        "                # 计算新的 Sk: Sk = Pk_hat * dot(Hk, Hk) + cur_r\n",
        "                Sk_new = torch.dot(vk.add(Hk.mul(Q)), Hk).add_(cur_r)\n",
        "\n",
        "                # 更新参数：计算 layer_loss\n",
        "                layer_loss = loss + 0.5 * self.state[\"weight_decay\"] * (p.norm(2) ** 2)\n",
        "                # scale = - lr * layer_loss * Pk_hat * Hk / Sk_new\n",
        "                # scale = Hk.clone()  # 创建一个拷贝用于更新\n",
        "                # scale.div_(Sk_new).mul_(-group[\"lr\"] * layer_loss)\n",
        "                scale = - group[\"lr\"] * layer_loss * (vk + Q * Hk) / Sk_new\n",
        "                p.data.add_(scale.view(p_shape))\n",
        "\n",
        "                # 更新状态\n",
        "                self.state[p][\"vk\"] = vk\n",
        "                self.state[p][\"Hk\"] = Hk\n",
        "                self.state[p][\"Sk\"] = Sk_new\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VmU_bdv4lOB"
      },
      "source": [
        "## Part 1: Image Classification\n",
        "\n",
        "The first task is to use CIFAR10 dataset to do image classification. We wil choose ResNet-based models for the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yXWBjNYyHTo_"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "\n",
        "\n",
        "def _change_to_cifar(m):\n",
        "    m.conv1 = nn.Conv2d(3, m.conv1.out_channels, kernel_size=3, padding=1, bias=False)\n",
        "    nn.init.kaiming_normal_(m.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
        "    m.maxpool = nn.Identity()\n",
        "    m.relu = nn.ReLU(inplace=False)\n",
        "    return m\n",
        "\n",
        "\n",
        "def _resnet18(num_classes, is_cifar=False, **kwargs):\n",
        "    model = torchvision.models.resnet.ResNet(\n",
        "        torchvision.models.resnet.BasicBlock,\n",
        "        [2, 2, 2, 2],\n",
        "        num_classes=num_classes,\n",
        "        **kwargs)\n",
        "    if is_cifar:\n",
        "        model = _change_to_cifar(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def _resnet50(num_classes, is_cifar=False, **kwargs):\n",
        "    model = torchvision.models.resnet.ResNet(\n",
        "        torchvision.models.resnet.Bottleneck,\n",
        "        [3, 4, 6, 3],\n",
        "        num_classes=num_classes,\n",
        "        **kwargs)\n",
        "    if is_cifar:\n",
        "        model = _change_to_cifar(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def _wide_resnet_50_2(num_classes, is_cifar=False, **kwargs):\n",
        "    model = torchvision.models.resnet.ResNet(\n",
        "        torchvision.models.resnet.Bottleneck,\n",
        "        [3, 4, 6, 3],\n",
        "        num_classes=num_classes,\n",
        "        width_per_group=64 * 2,\n",
        "        **kwargs)\n",
        "    if is_cifar:\n",
        "        model = _change_to_cifar(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YDfAbJf1CnCL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def get_avg(self):\n",
        "        return self.avg\n",
        "\n",
        "    def get_last_val(self):\n",
        "        return self.val\n",
        "\n",
        "def _accuracy(output, target, top_k=(1,)):\n",
        "    max_k = max(top_k)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(max_k, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in top_k:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        wrong_k = batch_size - correct_k\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, scheduler, epoch, num_epochs,\n",
        "          is_koala=False, print_freq=50, train_loss_history=None, train_top1_history=None, train_top5_history=None):\n",
        "    # print(type(optimizer))\n",
        "    # print(is_koala)\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # Switch to train mode\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # Measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input = input.cuda(non_blocking=True)\n",
        "        target = target.cuda(non_blocking=True)\n",
        "\n",
        "        if is_koala:\n",
        "            optimizer.predict()\n",
        "\n",
        "        # Compute output and loss\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "        loss_mean = loss.mean()\n",
        "\n",
        "        # Compute gradient and perform update step\n",
        "        optimizer.zero_grad()\n",
        "        loss_mean.backward()\n",
        "        if is_koala:\n",
        "            # print(\"asdjajsdh\")\n",
        "            loss_var = torch.mean(torch.pow(loss, 2))\n",
        "            optimizer.update(loss_mean, loss_var)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "\n",
        "        # Measure accuracy (Top-1 and Top-5 error) and record loss\n",
        "        err1, err5 = _accuracy(output.data, target, top_k=(1, 5))\n",
        "        losses.update(loss_mean.item(), input.size(0))\n",
        "        top1.update(err1.item(), input.size(0))\n",
        "        top5.update(err5.item(), input.size(0))\n",
        "\n",
        "        # Measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print(f'Epoch: [{epoch}/{num_epochs}][{i}/{len(train_loader)}]\\t'\n",
        "                  f'Time {batch_time.get_last_val():.3f} ({batch_time.get_avg():.3f})\\t'\n",
        "                  f'Data {data_time.get_last_val():.3f} ({data_time.get_avg():.3f})\\t'\n",
        "                  f'Loss {losses.get_last_val():.4f} ({losses.get_avg():.4f})\\t'\n",
        "                  f'Top 1-err {top1.get_last_val():.4f} ({top1.get_avg():.4f})\\t'\n",
        "                  f'Top 5-err {top5.get_last_val():.4f} ({top5.get_avg():.4f})')\n",
        "\n",
        "    # Update scheduler after epoch (not after each batch)\n",
        "    scheduler.step()\n",
        "\n",
        "    # Store metrics for visualization\n",
        "    train_loss_history.append(losses.get_avg())\n",
        "    train_top1_history.append(top1.get_avg())\n",
        "    train_top5_history.append(top5.get_avg())\n",
        "\n",
        "    return losses.get_avg(), top1.get_avg(), top5.get_avg()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch, num_epochs, print_freq=50,\n",
        "             val_loss_history=None, val_top1_history=None, val_top5_history=None):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # Switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        input = input.cuda(non_blocking=True)\n",
        "        target = target.cuda(non_blocking=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "            loss = torch.mean(criterion(output, target))\n",
        "\n",
        "        # Measure accuracy (Top-1 and Top-5 error) and record loss\n",
        "        err1, err5 = _accuracy(output.data, target, top_k=(1, 5))\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(err1.item(), input.size(0))\n",
        "        top5.update(err5.item(), input.size(0))\n",
        "\n",
        "        # Measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print(f'Test (on val set): [{epoch}/{num_epochs}][{i}/{len(val_loader)}]\\t'\n",
        "                  f'Time {batch_time.get_last_val():.3f} ({batch_time.get_avg():.3f})\\t'\n",
        "                  f'Loss {losses.get_last_val():.4f} ({losses.get_avg():.4f})\\t'\n",
        "                  f'Top 1-err {top1.get_last_val():.4f} ({top1.get_avg():.4f})\\t'\n",
        "                  f'Top 5-err {top5.get_last_val():.4f} ({top5.get_avg():.4f})')\n",
        "\n",
        "    print(f'* Epoch: [{epoch}/{num_epochs}]\\t'\n",
        "          f'Top 1-err {top1.get_avg():.3f}\\t'\n",
        "          f'Top 5-err {top5.get_avg():.3f}\\t'\n",
        "          f'Test Loss {losses.get_avg():.3f}')\n",
        "\n",
        "    # Store metrics for visualization\n",
        "    val_loss_history.append(losses.get_avg())\n",
        "    val_top1_history.append(top1.get_avg())\n",
        "    val_top5_history.append(top5.get_avg())\n",
        "\n",
        "    return losses.get_avg(), top1.get_avg(), top5.get_avg()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oHeoxT9wGICn"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import final\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def Trainer(model, optim_name, dataset_name='cifar10', max_epochs=100, random_seed=0):\n",
        "    \"\"\"\n",
        "    Train and evaluate the model on CIFAR-10 dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        optim_name: Name of the optimizer to use\n",
        "        max_epochs: Number of epochs\n",
        "\n",
        "    Returns:\n",
        "        train_loss_history: List of training loss per epoch\n",
        "        val_loss_history: List of validation loss per epoch\n",
        "        train_top1_history: List of training Top-1 error per epoch\n",
        "        val_top1_history: List of validation Top-1 error per epoch\n",
        "        train_top5_history: List of training Top-5 error per epoch\n",
        "        val_top5_history: List of validation Top-5 error per epoch\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    set_random_seed(random_seed)\n",
        "\n",
        "    # Configure optimizer\n",
        "    if optim_name == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
        "    elif optim_name == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.0003, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0005)\n",
        "    elif optim_name == 'KOALA-V':\n",
        "        optimizer = VanillaKOALA(\n",
        "            params=model.parameters(),\n",
        "            sigma=0.1, q=1, r=None, alpha_r=0.9,\n",
        "            weight_decay=0.0005, lr=0.1\n",
        "        )\n",
        "    elif optim_name == 'KOALA-M':\n",
        "        optimizer = MomentumKOALA(\n",
        "            params=model.parameters(),\n",
        "            sw=0.1, sc=0.0, sv=0.1, a=0.9,\n",
        "            qw=0.01, qv=0.01, r=None, alpha_r=0.9,\n",
        "            weight_decay=0.0005, lr=1.0\n",
        "        )\n",
        "    elif optim_name == 'KOALA-P':\n",
        "        optimizer = KOALAPlusPlus(\n",
        "            params=model.parameters(),\n",
        "            sigma=0.1, q=0.1, r=None, alpha_r=0.9,\n",
        "            weight_decay=0.0005, lr=1.0\n",
        "        )\n",
        "    elif optim_name == 'AdaFisher':\n",
        "         optimizer = AdaFisher(model, lr=1e-3, beta=0.9, gamma=0.8, Lambda=1e-3, weight_decay=5e-4)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer name: {optim_name}\")\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if max_epochs == 100:\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
        "    elif max_epochs == 200:\n",
        "        # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
        "        # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-4)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown max_epoch: {max_epochs}\")\n",
        "\n",
        "    # scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-4)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Configure dataset\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "        std=[x / 255.0 for x in [63.0, 62.1, 66.7]]\n",
        "    )\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "    if dataset_name == 'cifar10':\n",
        "      train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train),\n",
        "        batch_size=128, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "      val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root=\"./data\", train=False, transform=transform_test),\n",
        "        batch_size=128, shuffle=False, num_workers=4, pin_memory=True\n",
        "      )\n",
        "\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown dataset name: {dataset_name}\")\n",
        "    # Initialize history storage\n",
        "    train_loss_history, val_loss_history = [], []\n",
        "    train_top1_history, val_top1_history = [], []\n",
        "    train_top5_history, val_top5_history = [], []\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(max_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_loss, train_top1, train_top5 = train(\n",
        "            train_loader, model, criterion, optimizer, scheduler, epoch, max_epochs,\n",
        "            is_koala=(\"KOALA\" in optim_name),\n",
        "            train_loss_history=train_loss_history,\n",
        "            train_top1_history=train_top1_history,\n",
        "            train_top5_history=train_top5_history\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_top1, val_top5 = validate(\n",
        "            val_loader, model, criterion, epoch, max_epochs,\n",
        "            val_loss_history=val_loss_history,\n",
        "            val_top1_history=val_top1_history,\n",
        "            val_top5_history=val_top5_history\n",
        "        )\n",
        "\n",
        "    # ------------------------ #\n",
        "    # Visualization           #\n",
        "    # ------------------------ #\n",
        "    epochs = range(1, max_epochs + 1)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_loss_history, label=\"Train Loss\", marker=\"o\")\n",
        "    plt.plot(epochs, val_loss_history, label=\"Validation Loss\", marker=\"s\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Training and Validation Loss ({optim_name})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Top-1 Error\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_top1_history, label=\"Train Top-1 Error\", marker=\"o\")\n",
        "    plt.plot(epochs, val_top1_history, label=\"Validation Top-1 Error\", marker=\"s\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Error (%)\")\n",
        "    plt.title(f\"Training and Validation Top-1 Error ({optim_name})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Top-5 Error\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_top5_history, label=\"Train Top-5 Error\", marker=\"o\")\n",
        "    plt.plot(epochs, val_top5_history, label=\"Validation Top-5 Error\", marker=\"s\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Error (%)\")\n",
        "    plt.title(f\"Training and Validation Top-5 Error ({optim_name})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(f\"Finished training with {optim_name}.\")\n",
        "    final_val_top1 = val_top1_history[-1]\n",
        "    # get the best val top-1 error and its corresponding top-5 error\n",
        "    best_val_top1 = min(val_top1_history)\n",
        "    best_val_top5 = val_top5_history[val_top1_history.index(best_val_top1)]\n",
        "    print(f\"Best val top1 error: {best_val_top1}\")\n",
        "    print(f\"Final val top1 error: {final_val_top1}\")\n",
        "    final_val_top5 = val_top5_history[-1]\n",
        "    print(f\"Best val top5 error: {best_val_top5}\")\n",
        "    print(f\"Final val top5 error: {final_val_top5}\")\n",
        "    # val_loss history\n",
        "    # return train_loss_history, val_loss_history, train_top1_history, val_top1_history, train_top5_history, val_top5_history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SZV0fdS1JVRT"
      },
      "outputs": [],
      "source": [
        "'''ResNet in PyTorch.\n",
        "\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = out + self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out = out + self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "        self.layer1 = self._make_layer(block, self.in_planes, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes: int = 10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "\n",
        "def ResNet34(num_classes: int = 10):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
        "\n",
        "\n",
        "def ResNet50(num_classes: int = 10):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
        "\n",
        "\n",
        "def ResNet101(num_classes: int = 10):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
        "\n",
        "\n",
        "def ResNet152(num_classes: int = 10):\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BNNL99m1JXEJ"
      },
      "outputs": [],
      "source": [
        "model1 = ResNet18(num_classes=10)\n",
        "model2 = ResNet50(num_classes=10)\n",
        "model3 = ResNet101(num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QjsU83BHo_qu",
        "outputId": "c1b3faad-c516-42b7-9151-3b342e6a990e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 79.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/200\n",
            "Epoch: [0/200][0/390]\tTime 2.089 (2.089)\tData 0.159 (0.159)\tLoss 2.4386 (2.4386)\tTop 1-err 92.9688 (92.9688)\tTop 5-err 53.1250 (53.1250)\n",
            "Epoch: [0/200][50/390]\tTime 0.050 (0.091)\tData 0.000 (0.003)\tLoss 2.7794 (6.9092)\tTop 1-err 89.8438 (89.9050)\tTop 5-err 46.0938 (48.9124)\n",
            "Epoch: [0/200][100/390]\tTime 0.050 (0.071)\tData 0.000 (0.002)\tLoss 2.3695 (4.8879)\tTop 1-err 87.5000 (89.1476)\tTop 5-err 43.7500 (46.6352)\n",
            "Epoch: [0/200][150/390]\tTime 0.050 (0.064)\tData 0.000 (0.001)\tLoss 2.3070 (4.1889)\tTop 1-err 85.9375 (88.4882)\tTop 5-err 41.4062 (44.8262)\n",
            "Epoch: [0/200][200/390]\tTime 0.050 (0.060)\tData 0.000 (0.001)\tLoss 2.3520 (3.7776)\tTop 1-err 89.0625 (87.7643)\tTop 5-err 31.2500 (43.1048)\n",
            "Epoch: [0/200][250/390]\tTime 0.050 (0.058)\tData 0.000 (0.001)\tLoss 2.2136 (3.4819)\tTop 1-err 78.1250 (86.7810)\tTop 5-err 24.2188 (41.4094)\n",
            "Epoch: [0/200][300/390]\tTime 0.050 (0.057)\tData 0.000 (0.001)\tLoss 2.1774 (3.2762)\tTop 1-err 74.2188 (86.0517)\tTop 5-err 31.2500 (40.0903)\n",
            "Epoch: [0/200][350/390]\tTime 0.050 (0.056)\tData 0.000 (0.001)\tLoss 2.1428 (3.1230)\tTop 1-err 81.2500 (85.1941)\tTop 5-err 31.2500 (38.7620)\n",
            "Test (on val set): [0/200][0/79]\tTime 0.142 (0.142)\tLoss 2.0426 (2.0426)\tTop 1-err 68.7500 (68.7500)\tTop 5-err 22.6562 (22.6562)\n",
            "Test (on val set): [0/200][50/79]\tTime 0.016 (0.019)\tLoss 2.0745 (2.0952)\tTop 1-err 82.8125 (78.3241)\tTop 5-err 22.6562 (26.3634)\n",
            "* Epoch: [0/200]\tTop 1-err 78.270\tTop 5-err 26.040\tTest Loss 2.097\n",
            "\n",
            "Epoch 2/200\n",
            "Epoch: [1/200][0/390]\tTime 0.192 (0.192)\tData 0.133 (0.133)\tLoss 2.1674 (2.1674)\tTop 1-err 81.2500 (81.2500)\tTop 5-err 31.2500 (31.2500)\n",
            "Epoch: [1/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 2.0237 (2.0834)\tTop 1-err 75.7812 (77.6042)\tTop 5-err 29.6875 (25.1685)\n",
            "Epoch: [1/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 2.0053 (2.0614)\tTop 1-err 74.2188 (76.5934)\tTop 5-err 21.8750 (23.7005)\n",
            "Epoch: [1/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.8609 (2.0262)\tTop 1-err 67.9688 (75.4915)\tTop 5-err 15.6250 (22.4803)\n",
            "Epoch: [1/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 2.0457 (2.0054)\tTop 1-err 71.8750 (74.6152)\tTop 5-err 21.0938 (21.6068)\n",
            "Epoch: [1/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.8327 (1.9854)\tTop 1-err 72.6562 (74.0102)\tTop 5-err 13.2812 (20.8323)\n",
            "Epoch: [1/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 1.9212 (1.9704)\tTop 1-err 72.6562 (73.4193)\tTop 5-err 21.8750 (20.3644)\n",
            "Epoch: [1/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 1.8289 (1.9555)\tTop 1-err 73.4375 (72.9768)\tTop 5-err 14.0625 (19.7360)\n",
            "Test (on val set): [1/200][0/79]\tTime 0.138 (0.138)\tLoss 1.7246 (1.7246)\tTop 1-err 63.2812 (63.2812)\tTop 5-err 14.0625 (14.0625)\n",
            "Test (on val set): [1/200][50/79]\tTime 0.016 (0.018)\tLoss 1.8341 (1.7629)\tTop 1-err 71.0938 (65.2267)\tTop 5-err 18.7500 (13.3272)\n",
            "* Epoch: [1/200]\tTop 1-err 65.150\tTop 5-err 13.560\tTest Loss 1.763\n",
            "\n",
            "Epoch 3/200\n",
            "Epoch: [2/200][0/390]\tTime 0.220 (0.220)\tData 0.151 (0.151)\tLoss 1.8805 (1.8805)\tTop 1-err 67.9688 (67.9688)\tTop 5-err 17.1875 (17.1875)\n",
            "Epoch: [2/200][50/390]\tTime 0.052 (0.055)\tData 0.000 (0.003)\tLoss 1.7223 (1.8418)\tTop 1-err 71.0938 (68.7960)\tTop 5-err 10.9375 (15.1654)\n",
            "Epoch: [2/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 1.7264 (1.8315)\tTop 1-err 66.4062 (68.1931)\tTop 5-err 13.2812 (15.2537)\n",
            "Epoch: [2/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.7595 (1.8035)\tTop 1-err 63.2812 (67.2030)\tTop 5-err 15.6250 (14.5851)\n",
            "Epoch: [2/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.6491 (1.7879)\tTop 1-err 62.5000 (66.4024)\tTop 5-err 12.5000 (14.1830)\n",
            "Epoch: [2/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.7587 (1.7773)\tTop 1-err 60.1562 (65.8896)\tTop 5-err 17.1875 (13.8695)\n",
            "Epoch: [2/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.6272 (1.7644)\tTop 1-err 59.3750 (65.3239)\tTop 5-err 12.5000 (13.5304)\n",
            "Epoch: [2/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.7065 (1.7565)\tTop 1-err 64.8438 (65.0663)\tTop 5-err 8.5938 (13.2991)\n",
            "Test (on val set): [2/200][0/79]\tTime 0.124 (0.124)\tLoss 1.6004 (1.6004)\tTop 1-err 63.2812 (63.2812)\tTop 5-err 9.3750 (9.3750)\n",
            "Test (on val set): [2/200][50/79]\tTime 0.016 (0.018)\tLoss 1.7234 (1.6558)\tTop 1-err 64.0625 (62.7145)\tTop 5-err 14.8438 (10.5086)\n",
            "* Epoch: [2/200]\tTop 1-err 62.770\tTop 5-err 10.930\tTest Loss 1.659\n",
            "\n",
            "Epoch 4/200\n",
            "Epoch: [3/200][0/390]\tTime 0.198 (0.198)\tData 0.130 (0.130)\tLoss 1.6941 (1.6941)\tTop 1-err 59.3750 (59.3750)\tTop 5-err 12.5000 (12.5000)\n",
            "Epoch: [3/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 1.5563 (1.6488)\tTop 1-err 57.8125 (60.4933)\tTop 5-err 6.2500 (10.6158)\n",
            "Epoch: [3/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 1.6298 (1.6592)\tTop 1-err 58.5938 (61.2624)\tTop 5-err 10.9375 (10.9452)\n",
            "Epoch: [3/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.6874 (1.6467)\tTop 1-err 64.8438 (60.9116)\tTop 5-err 12.5000 (10.6892)\n",
            "Epoch: [3/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.5701 (1.6349)\tTop 1-err 57.0312 (60.6538)\tTop 5-err 7.8125 (10.4555)\n",
            "Epoch: [3/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.4882 (1.6300)\tTop 1-err 53.9062 (60.3897)\tTop 5-err 8.5938 (10.3990)\n",
            "Epoch: [3/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.6368 (1.6224)\tTop 1-err 64.0625 (60.1069)\tTop 5-err 8.5938 (10.2808)\n",
            "Epoch: [3/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 1.6233 (1.6161)\tTop 1-err 61.7188 (59.7534)\tTop 5-err 6.2500 (10.1830)\n",
            "Test (on val set): [3/200][0/79]\tTime 0.130 (0.130)\tLoss 1.5254 (1.5254)\tTop 1-err 54.6875 (54.6875)\tTop 5-err 10.1562 (10.1562)\n",
            "Test (on val set): [3/200][50/79]\tTime 0.016 (0.018)\tLoss 1.6618 (1.5401)\tTop 1-err 63.2812 (56.0968)\tTop 5-err 11.7188 (8.7623)\n",
            "* Epoch: [3/200]\tTop 1-err 56.250\tTop 5-err 8.750\tTest Loss 1.535\n",
            "\n",
            "Epoch 5/200\n",
            "Epoch: [4/200][0/390]\tTime 0.190 (0.190)\tData 0.129 (0.129)\tLoss 1.5220 (1.5220)\tTop 1-err 57.0312 (57.0312)\tTop 5-err 7.8125 (7.8125)\n",
            "Epoch: [4/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 1.4431 (1.5289)\tTop 1-err 48.4375 (55.9130)\tTop 5-err 10.9375 (8.8235)\n",
            "Epoch: [4/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 1.5303 (1.5352)\tTop 1-err 53.1250 (56.1185)\tTop 5-err 9.3750 (9.0656)\n",
            "Epoch: [4/200][150/390]\tTime 0.056 (0.052)\tData 0.000 (0.001)\tLoss 1.5317 (1.5240)\tTop 1-err 54.6875 (55.7740)\tTop 5-err 7.8125 (8.7800)\n",
            "Epoch: [4/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.5841 (1.5235)\tTop 1-err 67.1875 (55.6942)\tTop 5-err 8.5938 (8.8581)\n",
            "Epoch: [4/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.4804 (1.5191)\tTop 1-err 59.3750 (55.6773)\tTop 5-err 5.4688 (8.7681)\n",
            "Epoch: [4/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.4473 (1.5140)\tTop 1-err 53.9062 (55.5804)\tTop 5-err 3.9062 (8.5860)\n",
            "Epoch: [4/200][350/390]\tTime 0.050 (0.051)\tData 0.001 (0.001)\tLoss 1.4399 (1.5048)\tTop 1-err 55.4688 (55.2907)\tTop 5-err 8.5938 (8.3934)\n",
            "Test (on val set): [4/200][0/79]\tTime 0.124 (0.124)\tLoss 1.4017 (1.4017)\tTop 1-err 54.6875 (54.6875)\tTop 5-err 7.8125 (7.8125)\n",
            "Test (on val set): [4/200][50/79]\tTime 0.016 (0.018)\tLoss 1.4213 (1.4667)\tTop 1-err 54.6875 (52.9718)\tTop 5-err 5.4688 (7.5674)\n",
            "* Epoch: [4/200]\tTop 1-err 52.940\tTop 5-err 7.740\tTest Loss 1.468\n",
            "\n",
            "Epoch 6/200\n",
            "Epoch: [5/200][0/390]\tTime 0.196 (0.196)\tData 0.126 (0.126)\tLoss 1.4787 (1.4787)\tTop 1-err 53.1250 (53.1250)\tTop 5-err 8.5938 (8.5938)\n",
            "Epoch: [5/200][50/390]\tTime 0.050 (0.055)\tData 0.000 (0.003)\tLoss 1.3262 (1.4158)\tTop 1-err 49.2188 (50.6127)\tTop 5-err 6.2500 (7.5214)\n",
            "Epoch: [5/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 1.3582 (1.3960)\tTop 1-err 49.2188 (49.8453)\tTop 5-err 7.0312 (7.3793)\n",
            "Epoch: [5/200][150/390]\tTime 0.050 (0.052)\tData 0.000 (0.001)\tLoss 1.5243 (1.3888)\tTop 1-err 57.0312 (49.7206)\tTop 5-err 9.3750 (7.1916)\n",
            "Epoch: [5/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.2274 (1.3741)\tTop 1-err 44.5312 (49.1721)\tTop 5-err 3.1250 (6.9924)\n",
            "Epoch: [5/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.2804 (1.3749)\tTop 1-err 46.0938 (49.3993)\tTop 5-err 5.4688 (6.9783)\n",
            "Epoch: [5/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.3378 (1.3592)\tTop 1-err 39.0625 (48.7334)\tTop 5-err 8.5938 (6.7769)\n",
            "Epoch: [5/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.2675 (1.3480)\tTop 1-err 50.7812 (48.4108)\tTop 5-err 2.3438 (6.5839)\n",
            "Test (on val set): [5/200][0/79]\tTime 0.131 (0.131)\tLoss 1.1483 (1.1483)\tTop 1-err 39.8438 (39.8438)\tTop 5-err 3.9062 (3.9062)\n",
            "Test (on val set): [5/200][50/79]\tTime 0.016 (0.018)\tLoss 1.4534 (1.3167)\tTop 1-err 53.1250 (46.1091)\tTop 5-err 6.2500 (5.4688)\n",
            "* Epoch: [5/200]\tTop 1-err 46.580\tTop 5-err 5.610\tTest Loss 1.319\n",
            "\n",
            "Epoch 7/200\n",
            "Epoch: [6/200][0/390]\tTime 0.189 (0.189)\tData 0.129 (0.129)\tLoss 1.2461 (1.2461)\tTop 1-err 42.1875 (42.1875)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [6/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 1.2044 (1.2646)\tTop 1-err 45.3125 (45.9406)\tTop 5-err 3.9062 (5.4688)\n",
            "Epoch: [6/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 1.1158 (1.2596)\tTop 1-err 42.9688 (45.6838)\tTop 5-err 5.4688 (5.2754)\n",
            "Epoch: [6/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.2011 (1.2311)\tTop 1-err 44.5312 (44.8624)\tTop 5-err 3.9062 (5.0755)\n",
            "Epoch: [6/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.3626 (1.2229)\tTop 1-err 47.6562 (44.3874)\tTop 5-err 8.5938 (5.0801)\n",
            "Epoch: [6/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.3115 (1.2167)\tTop 1-err 48.4375 (44.0737)\tTop 5-err 6.2500 (4.9832)\n",
            "Epoch: [6/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 1.2577 (1.2135)\tTop 1-err 43.7500 (43.7967)\tTop 5-err 8.5938 (4.9834)\n",
            "Epoch: [6/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 1.2374 (1.2055)\tTop 1-err 44.5312 (43.4540)\tTop 5-err 7.0312 (4.8967)\n",
            "Test (on val set): [6/200][0/79]\tTime 0.140 (0.140)\tLoss 0.9544 (0.9544)\tTop 1-err 38.2812 (38.2812)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [6/200][50/79]\tTime 0.016 (0.018)\tLoss 1.0685 (1.0901)\tTop 1-err 38.2812 (38.8480)\tTop 5-err 3.9062 (3.6458)\n",
            "* Epoch: [6/200]\tTop 1-err 38.760\tTop 5-err 3.740\tTest Loss 1.084\n",
            "\n",
            "Epoch 8/200\n",
            "Epoch: [7/200][0/390]\tTime 0.197 (0.197)\tData 0.128 (0.128)\tLoss 1.0334 (1.0334)\tTop 1-err 38.2812 (38.2812)\tTop 5-err 2.3438 (2.3438)\n",
            "Epoch: [7/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 1.2132 (1.0862)\tTop 1-err 42.9688 (38.9553)\tTop 5-err 3.1250 (3.9522)\n",
            "Epoch: [7/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 1.0750 (1.0928)\tTop 1-err 40.6250 (39.3796)\tTop 5-err 3.9062 (3.8985)\n",
            "Epoch: [7/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.2215 (1.0950)\tTop 1-err 42.1875 (39.4661)\tTop 5-err 4.6875 (3.8545)\n",
            "Epoch: [7/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.1463 (1.0933)\tTop 1-err 40.6250 (39.1908)\tTop 5-err 5.4688 (3.8169)\n",
            "Epoch: [7/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.0331 (1.0828)\tTop 1-err 41.4062 (38.8291)\tTop 5-err 2.3438 (3.7164)\n",
            "Epoch: [7/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.0149 (1.0742)\tTop 1-err 35.9375 (38.5278)\tTop 5-err 1.5625 (3.6337)\n",
            "Epoch: [7/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.0850 (1.0685)\tTop 1-err 37.5000 (38.2523)\tTop 5-err 3.9062 (3.6458)\n",
            "Test (on val set): [7/200][0/79]\tTime 0.123 (0.123)\tLoss 1.1825 (1.1825)\tTop 1-err 48.4375 (48.4375)\tTop 5-err 3.1250 (3.1250)\n",
            "Test (on val set): [7/200][50/79]\tTime 0.016 (0.018)\tLoss 1.2575 (1.1353)\tTop 1-err 41.4062 (40.0123)\tTop 5-err 2.3438 (3.5539)\n",
            "* Epoch: [7/200]\tTop 1-err 40.230\tTop 5-err 3.650\tTest Loss 1.137\n",
            "\n",
            "Epoch 9/200\n",
            "Epoch: [8/200][0/390]\tTime 0.206 (0.206)\tData 0.135 (0.135)\tLoss 0.9346 (0.9346)\tTop 1-err 32.8125 (32.8125)\tTop 5-err 3.9062 (3.9062)\n",
            "Epoch: [8/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.8495 (0.9930)\tTop 1-err 30.4688 (35.0797)\tTop 5-err 3.9062 (3.1863)\n",
            "Epoch: [8/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 1.0714 (0.9819)\tTop 1-err 35.9375 (34.8314)\tTop 5-err 6.2500 (3.1173)\n",
            "Epoch: [8/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.9148 (0.9861)\tTop 1-err 35.9375 (35.1769)\tTop 5-err 0.7812 (3.0784)\n",
            "Epoch: [8/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.8778 (0.9837)\tTop 1-err 31.2500 (34.9075)\tTop 5-err 1.5625 (3.0706)\n",
            "Epoch: [8/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 1.0234 (0.9812)\tTop 1-err 38.2812 (34.8730)\tTop 5-err 4.6875 (3.0565)\n",
            "Epoch: [8/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.8910 (0.9756)\tTop 1-err 32.0312 (34.5956)\tTop 5-err 2.3438 (3.0445)\n",
            "Epoch: [8/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.8447 (0.9725)\tTop 1-err 28.1250 (34.5597)\tTop 5-err 3.1250 (3.0404)\n",
            "Test (on val set): [8/200][0/79]\tTime 0.133 (0.133)\tLoss 0.8500 (0.8500)\tTop 1-err 33.5938 (33.5938)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [8/200][50/79]\tTime 0.016 (0.018)\tLoss 1.0668 (0.9796)\tTop 1-err 36.7188 (34.5129)\tTop 5-err 3.1250 (2.6501)\n",
            "* Epoch: [8/200]\tTop 1-err 35.020\tTop 5-err 2.800\tTest Loss 0.989\n",
            "\n",
            "Epoch 10/200\n",
            "Epoch: [9/200][0/390]\tTime 0.187 (0.187)\tData 0.129 (0.129)\tLoss 0.8470 (0.8470)\tTop 1-err 28.1250 (28.1250)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [9/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.8956 (0.9126)\tTop 1-err 32.0312 (32.3223)\tTop 5-err 1.5625 (2.3744)\n",
            "Epoch: [9/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.8475 (0.9018)\tTop 1-err 33.5938 (31.9152)\tTop 5-err 3.1250 (2.3515)\n",
            "Epoch: [9/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.9678 (0.9033)\tTop 1-err 35.1562 (31.9640)\tTop 5-err 2.3438 (2.5248)\n",
            "Epoch: [9/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7936 (0.9022)\tTop 1-err 25.7812 (31.8447)\tTop 5-err 3.1250 (2.5303)\n",
            "Epoch: [9/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.9530 (0.9013)\tTop 1-err 35.1562 (31.8818)\tTop 5-err 6.2500 (2.4838)\n",
            "Epoch: [9/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.9599 (0.9038)\tTop 1-err 33.5938 (32.0624)\tTop 5-err 2.3438 (2.4917)\n",
            "Epoch: [9/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.7208 (0.9015)\tTop 1-err 25.7812 (32.0557)\tTop 5-err 0.7812 (2.4706)\n",
            "Test (on val set): [9/200][0/79]\tTime 0.133 (0.133)\tLoss 0.8764 (0.8764)\tTop 1-err 28.9062 (28.9062)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [9/200][50/79]\tTime 0.016 (0.018)\tLoss 1.0420 (0.9653)\tTop 1-err 34.3750 (34.0227)\tTop 5-err 1.5625 (2.7114)\n",
            "* Epoch: [9/200]\tTop 1-err 34.450\tTop 5-err 2.870\tTest Loss 0.974\n",
            "\n",
            "Epoch 11/200\n",
            "Epoch: [10/200][0/390]\tTime 0.199 (0.199)\tData 0.130 (0.130)\tLoss 0.9636 (0.9636)\tTop 1-err 35.1562 (35.1562)\tTop 5-err 3.1250 (3.1250)\n",
            "Epoch: [10/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.8707 (0.8519)\tTop 1-err 28.9062 (30.2849)\tTop 5-err 3.1250 (1.9301)\n",
            "Epoch: [10/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.6786 (0.8470)\tTop 1-err 24.2188 (29.7184)\tTop 5-err 2.3438 (2.0730)\n",
            "Epoch: [10/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.8889 (0.8439)\tTop 1-err 28.1250 (29.7444)\tTop 5-err 1.5625 (2.1058)\n",
            "Epoch: [10/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7451 (0.8378)\tTop 1-err 25.7812 (29.4815)\tTop 5-err 0.7812 (2.1183)\n",
            "Epoch: [10/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.8642 (0.8344)\tTop 1-err 31.2500 (29.4229)\tTop 5-err 3.1250 (2.1383)\n",
            "Epoch: [10/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6359 (0.8312)\tTop 1-err 21.0938 (29.2904)\tTop 5-err 0.7812 (2.1361)\n",
            "Epoch: [10/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.8154 (0.8279)\tTop 1-err 25.0000 (29.2134)\tTop 5-err 3.1250 (2.0633)\n",
            "Test (on val set): [10/200][0/79]\tTime 0.140 (0.140)\tLoss 0.7623 (0.7623)\tTop 1-err 26.5625 (26.5625)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [10/200][50/79]\tTime 0.016 (0.019)\tLoss 0.9670 (0.8891)\tTop 1-err 29.6875 (30.8058)\tTop 5-err 2.3438 (2.2212)\n",
            "* Epoch: [10/200]\tTop 1-err 30.950\tTop 5-err 2.260\tTest Loss 0.896\n",
            "\n",
            "Epoch 12/200\n",
            "Epoch: [11/200][0/390]\tTime 0.191 (0.191)\tData 0.134 (0.134)\tLoss 0.7648 (0.7648)\tTop 1-err 29.6875 (29.6875)\tTop 5-err 3.1250 (3.1250)\n",
            "Epoch: [11/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.7955 (0.7698)\tTop 1-err 29.6875 (27.0221)\tTop 5-err 2.3438 (1.8995)\n",
            "Epoch: [11/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.8967 (0.7718)\tTop 1-err 32.0312 (27.2587)\tTop 5-err 0.7812 (1.7868)\n",
            "Epoch: [11/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.8177 (0.7775)\tTop 1-err 31.2500 (27.3541)\tTop 5-err 2.3438 (1.7850)\n",
            "Epoch: [11/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7905 (0.7806)\tTop 1-err 28.9062 (27.4409)\tTop 5-err 3.9062 (1.8579)\n",
            "Epoch: [11/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5762 (0.7740)\tTop 1-err 18.7500 (27.1352)\tTop 5-err 0.7812 (1.7804)\n",
            "Epoch: [11/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.8112 (0.7680)\tTop 1-err 26.5625 (26.8246)\tTop 5-err 3.1250 (1.7727)\n",
            "Epoch: [11/200][350/390]\tTime 0.051 (0.050)\tData 0.000 (0.001)\tLoss 0.7776 (0.7634)\tTop 1-err 25.7812 (26.7116)\tTop 5-err 2.3438 (1.7339)\n",
            "Test (on val set): [11/200][0/79]\tTime 0.128 (0.128)\tLoss 1.1070 (1.1070)\tTop 1-err 36.7188 (36.7188)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [11/200][50/79]\tTime 0.016 (0.018)\tLoss 1.2544 (1.1482)\tTop 1-err 39.0625 (37.4694)\tTop 5-err 0.7812 (3.7224)\n",
            "* Epoch: [11/200]\tTop 1-err 37.440\tTop 5-err 3.680\tTest Loss 1.146\n",
            "\n",
            "Epoch 13/200\n",
            "Epoch: [12/200][0/390]\tTime 0.189 (0.189)\tData 0.131 (0.131)\tLoss 0.6512 (0.6512)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 2.3438 (2.3438)\n",
            "Epoch: [12/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.6057 (0.7297)\tTop 1-err 20.3125 (25.9038)\tTop 5-err 0.0000 (1.3634)\n",
            "Epoch: [12/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.7056 (0.7157)\tTop 1-err 25.0000 (25.2088)\tTop 5-err 1.5625 (1.3537)\n",
            "Epoch: [12/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6795 (0.7158)\tTop 1-err 25.7812 (25.1759)\tTop 5-err 2.3438 (1.4435)\n",
            "Epoch: [12/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7302 (0.7136)\tTop 1-err 26.5625 (24.9650)\tTop 5-err 1.5625 (1.4576)\n",
            "Epoch: [12/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6861 (0.7128)\tTop 1-err 25.7812 (24.9346)\tTop 5-err 0.0000 (1.5096)\n",
            "Epoch: [12/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5835 (0.7090)\tTop 1-err 24.2188 (24.8495)\tTop 5-err 0.7812 (1.5158)\n",
            "Epoch: [12/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.8456 (0.7098)\tTop 1-err 28.1250 (24.8843)\tTop 5-err 3.9062 (1.5336)\n",
            "Test (on val set): [12/200][0/79]\tTime 0.135 (0.135)\tLoss 0.9336 (0.9336)\tTop 1-err 31.2500 (31.2500)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [12/200][50/79]\tTime 0.016 (0.018)\tLoss 1.2849 (1.0249)\tTop 1-err 35.9375 (32.7359)\tTop 5-err 6.2500 (3.5080)\n",
            "* Epoch: [12/200]\tTop 1-err 33.350\tTop 5-err 3.430\tTest Loss 1.028\n",
            "\n",
            "Epoch 14/200\n",
            "Epoch: [13/200][0/390]\tTime 0.201 (0.201)\tData 0.132 (0.132)\tLoss 0.7260 (0.7260)\tTop 1-err 27.3438 (27.3438)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [13/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.6926 (0.6826)\tTop 1-err 27.3438 (23.8817)\tTop 5-err 0.7812 (1.6238)\n",
            "Epoch: [13/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.6215 (0.6728)\tTop 1-err 24.2188 (23.4839)\tTop 5-err 0.7812 (1.5084)\n",
            "Epoch: [13/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7036 (0.6656)\tTop 1-err 29.6875 (23.1788)\tTop 5-err 0.7812 (1.4435)\n",
            "Epoch: [13/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7337 (0.6706)\tTop 1-err 24.2188 (23.3870)\tTop 5-err 2.3438 (1.4576)\n",
            "Epoch: [13/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7398 (0.6689)\tTop 1-err 23.4375 (23.2943)\tTop 5-err 4.6875 (1.4255)\n",
            "Epoch: [13/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6803 (0.6669)\tTop 1-err 21.0938 (23.2325)\tTop 5-err 2.3438 (1.3990)\n",
            "Epoch: [13/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6365 (0.6669)\tTop 1-err 21.8750 (23.1815)\tTop 5-err 2.3438 (1.3778)\n",
            "Test (on val set): [13/200][0/79]\tTime 0.133 (0.133)\tLoss 0.6847 (0.6847)\tTop 1-err 26.5625 (26.5625)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [13/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7608 (0.7728)\tTop 1-err 24.2188 (25.5821)\tTop 5-err 0.7812 (1.5625)\n",
            "* Epoch: [13/200]\tTop 1-err 25.310\tTop 5-err 1.610\tTest Loss 0.772\n",
            "\n",
            "Epoch 15/200\n",
            "Epoch: [14/200][0/390]\tTime 0.188 (0.188)\tData 0.130 (0.130)\tLoss 0.7798 (0.7798)\tTop 1-err 25.7812 (25.7812)\tTop 5-err 2.3438 (2.3438)\n",
            "Epoch: [14/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.6917 (0.6402)\tTop 1-err 24.2188 (21.9210)\tTop 5-err 0.7812 (1.3174)\n",
            "Epoch: [14/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.6006 (0.6275)\tTop 1-err 21.0938 (21.7048)\tTop 5-err 0.7812 (1.2608)\n",
            "Epoch: [14/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4883 (0.6273)\tTop 1-err 18.7500 (21.6887)\tTop 5-err 0.7812 (1.2314)\n",
            "Epoch: [14/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5365 (0.6306)\tTop 1-err 17.9688 (21.7701)\tTop 5-err 0.7812 (1.2826)\n",
            "Epoch: [14/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6037 (0.6320)\tTop 1-err 16.4062 (21.7474)\tTop 5-err 0.0000 (1.2637)\n",
            "Epoch: [14/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5476 (0.6283)\tTop 1-err 18.7500 (21.6284)\tTop 5-err 1.5625 (1.2484)\n",
            "Epoch: [14/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5927 (0.6292)\tTop 1-err 24.2188 (21.6747)\tTop 5-err 1.5625 (1.2687)\n",
            "Test (on val set): [14/200][0/79]\tTime 0.134 (0.134)\tLoss 0.6999 (0.6999)\tTop 1-err 27.3438 (27.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [14/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8083 (0.7227)\tTop 1-err 25.7812 (24.8621)\tTop 5-err 3.1250 (1.3634)\n",
            "* Epoch: [14/200]\tTop 1-err 25.060\tTop 5-err 1.270\tTest Loss 0.730\n",
            "\n",
            "Epoch 16/200\n",
            "Epoch: [15/200][0/390]\tTime 0.189 (0.189)\tData 0.129 (0.129)\tLoss 0.5922 (0.5922)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 2.3438 (2.3438)\n",
            "Epoch: [15/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.6103 (0.6331)\tTop 1-err 18.7500 (21.4308)\tTop 5-err 0.7812 (1.2255)\n",
            "Epoch: [15/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.7791 (0.6245)\tTop 1-err 25.0000 (21.5192)\tTop 5-err 1.5625 (1.1216)\n",
            "Epoch: [15/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6912 (0.6170)\tTop 1-err 23.4375 (21.3990)\tTop 5-err 0.7812 (1.1486)\n",
            "Epoch: [15/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5434 (0.6118)\tTop 1-err 21.0938 (21.1987)\tTop 5-err 0.0000 (1.1194)\n",
            "Epoch: [15/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5899 (0.6085)\tTop 1-err 18.7500 (21.1747)\tTop 5-err 0.7812 (1.0769)\n",
            "Epoch: [15/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5524 (0.6115)\tTop 1-err 20.3125 (21.2780)\tTop 5-err 0.7812 (1.1031)\n",
            "Epoch: [15/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6010 (0.6120)\tTop 1-err 18.7500 (21.2496)\tTop 5-err 0.0000 (1.1107)\n",
            "Test (on val set): [15/200][0/79]\tTime 0.124 (0.124)\tLoss 0.6241 (0.6241)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [15/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8566 (0.7056)\tTop 1-err 31.2500 (24.0656)\tTop 5-err 2.3438 (1.6238)\n",
            "* Epoch: [15/200]\tTop 1-err 24.490\tTop 5-err 1.680\tTest Loss 0.712\n",
            "\n",
            "Epoch 17/200\n",
            "Epoch: [16/200][0/390]\tTime 0.207 (0.207)\tData 0.135 (0.135)\tLoss 0.6425 (0.6425)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [16/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.6196 (0.5906)\tTop 1-err 18.7500 (19.8836)\tTop 5-err 1.5625 (1.0876)\n",
            "Epoch: [16/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.6018 (0.6057)\tTop 1-err 17.1875 (20.8308)\tTop 5-err 0.0000 (1.1371)\n",
            "Epoch: [16/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5855 (0.6040)\tTop 1-err 19.5312 (20.8351)\tTop 5-err 0.0000 (1.0865)\n",
            "Epoch: [16/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6510 (0.5950)\tTop 1-err 21.0938 (20.4796)\tTop 5-err 3.1250 (1.0689)\n",
            "Epoch: [16/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5779 (0.5967)\tTop 1-err 21.8750 (20.5210)\tTop 5-err 0.7812 (1.1018)\n",
            "Epoch: [16/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5946 (0.5950)\tTop 1-err 21.0938 (20.4578)\tTop 5-err 0.7812 (1.1187)\n",
            "Epoch: [16/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5587 (0.5953)\tTop 1-err 19.5312 (20.5128)\tTop 5-err 1.5625 (1.0906)\n",
            "Test (on val set): [16/200][0/79]\tTime 0.128 (0.128)\tLoss 0.5890 (0.5890)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [16/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7142 (0.6802)\tTop 1-err 21.0938 (23.7286)\tTop 5-err 3.1250 (1.3634)\n",
            "* Epoch: [16/200]\tTop 1-err 23.540\tTop 5-err 1.400\tTest Loss 0.678\n",
            "\n",
            "Epoch 18/200\n",
            "Epoch: [17/200][0/390]\tTime 0.191 (0.191)\tData 0.129 (0.129)\tLoss 0.5418 (0.5418)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [17/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5165 (0.5520)\tTop 1-err 17.9688 (19.1176)\tTop 5-err 1.5625 (0.9957)\n",
            "Epoch: [17/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.7347 (0.5654)\tTop 1-err 25.7812 (19.4616)\tTop 5-err 0.0000 (1.0520)\n",
            "Epoch: [17/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6418 (0.5698)\tTop 1-err 20.3125 (19.5261)\tTop 5-err 1.5625 (1.0555)\n",
            "Epoch: [17/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6517 (0.5750)\tTop 1-err 25.7812 (19.7178)\tTop 5-err 1.5625 (1.0417)\n",
            "Epoch: [17/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.7038 (0.5728)\tTop 1-err 21.8750 (19.6931)\tTop 5-err 2.3438 (1.0365)\n",
            "Epoch: [17/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6303 (0.5739)\tTop 1-err 19.5312 (19.7207)\tTop 5-err 2.3438 (1.0200)\n",
            "Epoch: [17/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5561 (0.5719)\tTop 1-err 21.0938 (19.6514)\tTop 5-err 0.7812 (1.0350)\n",
            "Test (on val set): [17/200][0/79]\tTime 0.126 (0.126)\tLoss 0.7020 (0.7020)\tTop 1-err 25.7812 (25.7812)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [17/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7641 (0.6935)\tTop 1-err 31.2500 (24.1422)\tTop 5-err 2.3438 (1.3787)\n",
            "* Epoch: [17/200]\tTop 1-err 24.030\tTop 5-err 1.400\tTest Loss 0.699\n",
            "\n",
            "Epoch 19/200\n",
            "Epoch: [18/200][0/390]\tTime 0.192 (0.192)\tData 0.129 (0.129)\tLoss 0.6360 (0.6360)\tTop 1-err 24.2188 (24.2188)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [18/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4958 (0.5526)\tTop 1-err 17.9688 (18.8419)\tTop 5-err 0.0000 (0.9804)\n",
            "Epoch: [18/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.5829 (0.5542)\tTop 1-err 17.9688 (18.9975)\tTop 5-err 1.5625 (1.0133)\n",
            "Epoch: [18/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.7010 (0.5503)\tTop 1-err 22.6562 (18.8224)\tTop 5-err 0.7812 (1.0192)\n",
            "Epoch: [18/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4945 (0.5505)\tTop 1-err 17.1875 (18.9405)\tTop 5-err 0.7812 (0.9562)\n",
            "Epoch: [18/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4512 (0.5499)\tTop 1-err 13.2812 (18.9399)\tTop 5-err 0.7812 (0.9275)\n",
            "Epoch: [18/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5162 (0.5535)\tTop 1-err 17.9688 (19.0978)\tTop 5-err 0.7812 (0.9577)\n",
            "Epoch: [18/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4821 (0.5592)\tTop 1-err 16.4062 (19.2597)\tTop 5-err 0.7812 (0.9793)\n",
            "Test (on val set): [18/200][0/79]\tTime 0.124 (0.124)\tLoss 0.6934 (0.6934)\tTop 1-err 24.2188 (24.2188)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [18/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8785 (0.7664)\tTop 1-err 27.3438 (26.7616)\tTop 5-err 2.3438 (1.4093)\n",
            "* Epoch: [18/200]\tTop 1-err 26.560\tTop 5-err 1.440\tTest Loss 0.761\n",
            "\n",
            "Epoch 20/200\n",
            "Epoch: [19/200][0/390]\tTime 0.201 (0.201)\tData 0.132 (0.132)\tLoss 0.4843 (0.4843)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [19/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5414 (0.5489)\tTop 1-err 19.5312 (18.8419)\tTop 5-err 0.7812 (0.7047)\n",
            "Epoch: [19/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5930 (0.5523)\tTop 1-err 23.4375 (19.3224)\tTop 5-err 1.5625 (0.8663)\n",
            "Epoch: [19/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5340 (0.5523)\tTop 1-err 15.6250 (19.2105)\tTop 5-err 1.5625 (0.9054)\n",
            "Epoch: [19/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5368 (0.5535)\tTop 1-err 21.8750 (19.2631)\tTop 5-err 0.7812 (0.8668)\n",
            "Epoch: [19/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6432 (0.5515)\tTop 1-err 22.6562 (19.1048)\tTop 5-err 0.0000 (0.8809)\n",
            "Epoch: [19/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.7489 (0.5522)\tTop 1-err 25.0000 (19.1471)\tTop 5-err 0.7812 (0.8565)\n",
            "Epoch: [19/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.7170 (0.5521)\tTop 1-err 26.5625 (19.0705)\tTop 5-err 2.3438 (0.8881)\n",
            "Test (on val set): [19/200][0/79]\tTime 0.130 (0.130)\tLoss 0.6555 (0.6555)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [19/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7703 (0.6895)\tTop 1-err 25.7812 (23.2996)\tTop 5-err 1.5625 (1.3634)\n",
            "* Epoch: [19/200]\tTop 1-err 23.510\tTop 5-err 1.350\tTest Loss 0.697\n",
            "\n",
            "Epoch 21/200\n",
            "Epoch: [20/200][0/390]\tTime 0.200 (0.200)\tData 0.129 (0.129)\tLoss 0.5567 (0.5567)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [20/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.7625 (0.5496)\tTop 1-err 26.5625 (18.9032)\tTop 5-err 2.3438 (0.9498)\n",
            "Epoch: [20/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4703 (0.5289)\tTop 1-err 17.9688 (18.2008)\tTop 5-err 0.0000 (0.9824)\n",
            "Epoch: [20/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6087 (0.5359)\tTop 1-err 22.6562 (18.4240)\tTop 5-err 0.0000 (0.9986)\n",
            "Epoch: [20/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5474 (0.5349)\tTop 1-err 17.9688 (18.3769)\tTop 5-err 1.5625 (0.9678)\n",
            "Epoch: [20/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5209 (0.5369)\tTop 1-err 21.8750 (18.4543)\tTop 5-err 0.7812 (0.9618)\n",
            "Epoch: [20/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6072 (0.5409)\tTop 1-err 21.8750 (18.5605)\tTop 5-err 0.0000 (0.9837)\n",
            "Epoch: [20/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4685 (0.5389)\tTop 1-err 15.6250 (18.4451)\tTop 5-err 1.5625 (0.9571)\n",
            "Test (on val set): [20/200][0/79]\tTime 0.124 (0.124)\tLoss 0.7883 (0.7883)\tTop 1-err 22.6562 (22.6562)\tTop 5-err 3.9062 (3.9062)\n",
            "Test (on val set): [20/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8109 (0.8389)\tTop 1-err 28.1250 (28.6305)\tTop 5-err 1.5625 (1.7004)\n",
            "* Epoch: [20/200]\tTop 1-err 28.610\tTop 5-err 1.620\tTest Loss 0.835\n",
            "\n",
            "Epoch 22/200\n",
            "Epoch: [21/200][0/390]\tTime 0.204 (0.204)\tData 0.131 (0.131)\tLoss 0.4558 (0.4558)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [21/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4412 (0.4939)\tTop 1-err 12.5000 (16.9118)\tTop 5-err 0.7812 (0.7659)\n",
            "Epoch: [21/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.5313 (0.5250)\tTop 1-err 14.0625 (18.1312)\tTop 5-err 2.3438 (0.8586)\n",
            "Epoch: [21/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6346 (0.5217)\tTop 1-err 21.8750 (18.0826)\tTop 5-err 0.7812 (0.7864)\n",
            "Epoch: [21/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6215 (0.5252)\tTop 1-err 17.9688 (18.1359)\tTop 5-err 2.3438 (0.8668)\n",
            "Epoch: [21/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4863 (0.5246)\tTop 1-err 14.0625 (18.1648)\tTop 5-err 3.1250 (0.8902)\n",
            "Epoch: [21/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5218 (0.5266)\tTop 1-err 17.9688 (18.1686)\tTop 5-err 0.0000 (0.9136)\n",
            "Epoch: [21/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4785 (0.5268)\tTop 1-err 17.1875 (18.1535)\tTop 5-err 0.7812 (0.9059)\n",
            "Test (on val set): [21/200][0/79]\tTime 0.122 (0.122)\tLoss 0.6160 (0.6160)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [21/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7920 (0.7162)\tTop 1-err 24.2188 (23.3609)\tTop 5-err 1.5625 (2.0987)\n",
            "* Epoch: [21/200]\tTop 1-err 23.210\tTop 5-err 2.080\tTest Loss 0.716\n",
            "\n",
            "Epoch 23/200\n",
            "Epoch: [22/200][0/390]\tTime 0.191 (0.191)\tData 0.132 (0.132)\tLoss 0.5826 (0.5826)\tTop 1-err 21.8750 (21.8750)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [22/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5248 (0.5157)\tTop 1-err 16.4062 (17.7696)\tTop 5-err 0.7812 (0.9191)\n",
            "Epoch: [22/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.7883 (0.5121)\tTop 1-err 25.0000 (17.5124)\tTop 5-err 2.3438 (0.8509)\n",
            "Epoch: [22/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5246 (0.5151)\tTop 1-err 16.4062 (17.6118)\tTop 5-err 0.7812 (0.8433)\n",
            "Epoch: [22/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3778 (0.5150)\tTop 1-err 13.2812 (17.7122)\tTop 5-err 0.7812 (0.8201)\n",
            "Epoch: [22/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5374 (0.5172)\tTop 1-err 17.9688 (17.8162)\tTop 5-err 0.0000 (0.8342)\n",
            "Epoch: [22/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4191 (0.5165)\tTop 1-err 15.6250 (17.7326)\tTop 5-err 1.5625 (0.8332)\n",
            "Epoch: [22/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4678 (0.5189)\tTop 1-err 17.1875 (17.7551)\tTop 5-err 1.5625 (0.8814)\n",
            "Test (on val set): [22/200][0/79]\tTime 0.133 (0.133)\tLoss 0.8257 (0.8257)\tTop 1-err 21.8750 (21.8750)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [22/200][50/79]\tTime 0.016 (0.018)\tLoss 0.9263 (0.8176)\tTop 1-err 28.1250 (25.6127)\tTop 5-err 3.1250 (1.4859)\n",
            "* Epoch: [22/200]\tTop 1-err 25.420\tTop 5-err 1.420\tTest Loss 0.804\n",
            "\n",
            "Epoch 24/200\n",
            "Epoch: [23/200][0/390]\tTime 0.197 (0.197)\tData 0.137 (0.137)\tLoss 0.4879 (0.4879)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [23/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3986 (0.5131)\tTop 1-err 13.2812 (17.6624)\tTop 5-err 0.7812 (0.8578)\n",
            "Epoch: [23/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5213 (0.5090)\tTop 1-err 17.9688 (17.5743)\tTop 5-err 0.0000 (0.8586)\n",
            "Epoch: [23/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4334 (0.5077)\tTop 1-err 13.2812 (17.4410)\tTop 5-err 0.7812 (0.8485)\n",
            "Epoch: [23/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5410 (0.5063)\tTop 1-err 17.9688 (17.3430)\tTop 5-err 0.0000 (0.8046)\n",
            "Epoch: [23/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5083 (0.5078)\tTop 1-err 21.0938 (17.4801)\tTop 5-err 0.0000 (0.7844)\n",
            "Epoch: [23/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5573 (0.5097)\tTop 1-err 20.3125 (17.6002)\tTop 5-err 2.3438 (0.7735)\n",
            "Epoch: [23/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5491 (0.5123)\tTop 1-err 19.5312 (17.7284)\tTop 5-err 0.7812 (0.8013)\n",
            "Test (on val set): [23/200][0/79]\tTime 0.137 (0.137)\tLoss 0.6767 (0.6767)\tTop 1-err 24.2188 (24.2188)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [23/200][50/79]\tTime 0.016 (0.019)\tLoss 0.7183 (0.6250)\tTop 1-err 20.3125 (20.8487)\tTop 5-err 2.3438 (1.2561)\n",
            "* Epoch: [23/200]\tTop 1-err 21.330\tTop 5-err 1.140\tTest Loss 0.631\n",
            "\n",
            "Epoch 25/200\n",
            "Epoch: [24/200][0/390]\tTime 0.193 (0.193)\tData 0.132 (0.132)\tLoss 0.5564 (0.5564)\tTop 1-err 23.4375 (23.4375)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [24/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4208 (0.5094)\tTop 1-err 17.1875 (17.7390)\tTop 5-err 0.7812 (0.7966)\n",
            "Epoch: [24/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5924 (0.4991)\tTop 1-err 20.3125 (17.2494)\tTop 5-err 0.7812 (0.8431)\n",
            "Epoch: [24/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5801 (0.5105)\tTop 1-err 17.1875 (17.7204)\tTop 5-err 1.5625 (0.8640)\n",
            "Epoch: [24/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4457 (0.5038)\tTop 1-err 17.9688 (17.4479)\tTop 5-err 0.7812 (0.8590)\n",
            "Epoch: [24/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4949 (0.5030)\tTop 1-err 14.0625 (17.4521)\tTop 5-err 0.0000 (0.8435)\n",
            "Epoch: [24/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5210 (0.4983)\tTop 1-err 18.7500 (17.2524)\tTop 5-err 0.7812 (0.8280)\n",
            "Epoch: [24/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5896 (0.4978)\tTop 1-err 20.3125 (17.2009)\tTop 5-err 1.5625 (0.8280)\n",
            "Test (on val set): [24/200][0/79]\tTime 0.123 (0.123)\tLoss 0.5774 (0.5774)\tTop 1-err 22.6562 (22.6562)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [24/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7944 (0.7108)\tTop 1-err 26.5625 (23.2690)\tTop 5-err 1.5625 (1.2714)\n",
            "* Epoch: [24/200]\tTop 1-err 23.450\tTop 5-err 1.220\tTest Loss 0.699\n",
            "\n",
            "Epoch 26/200\n",
            "Epoch: [25/200][0/390]\tTime 0.193 (0.193)\tData 0.135 (0.135)\tLoss 0.3995 (0.3995)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [25/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4704 (0.4837)\tTop 1-err 17.1875 (17.0190)\tTop 5-err 0.0000 (0.6587)\n",
            "Epoch: [25/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.4367 (0.4881)\tTop 1-err 15.6250 (17.1488)\tTop 5-err 0.0000 (0.6343)\n",
            "Epoch: [25/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6455 (0.4915)\tTop 1-err 25.0000 (17.0944)\tTop 5-err 2.3438 (0.6571)\n",
            "Epoch: [25/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6758 (0.4917)\tTop 1-err 23.4375 (17.0981)\tTop 5-err 1.5625 (0.6530)\n",
            "Epoch: [25/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4613 (0.4977)\tTop 1-err 15.6250 (17.2684)\tTop 5-err 2.3438 (0.7065)\n",
            "Epoch: [25/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5600 (0.4965)\tTop 1-err 20.3125 (17.1693)\tTop 5-err 0.7812 (0.7060)\n",
            "Epoch: [25/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4445 (0.4914)\tTop 1-err 12.5000 (16.9338)\tTop 5-err 0.0000 (0.7011)\n",
            "Test (on val set): [25/200][0/79]\tTime 0.124 (0.124)\tLoss 0.5645 (0.5645)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [25/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6601 (0.6332)\tTop 1-err 23.4375 (21.7371)\tTop 5-err 0.7812 (0.9038)\n",
            "* Epoch: [25/200]\tTop 1-err 21.350\tTop 5-err 0.800\tTest Loss 0.624\n",
            "\n",
            "Epoch 27/200\n",
            "Epoch: [26/200][0/390]\tTime 0.204 (0.204)\tData 0.133 (0.133)\tLoss 0.5534 (0.5534)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [26/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3757 (0.4992)\tTop 1-err 11.7188 (17.2641)\tTop 5-err 1.5625 (0.7659)\n",
            "Epoch: [26/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.5861 (0.4870)\tTop 1-err 23.4375 (16.8626)\tTop 5-err 0.0000 (0.7194)\n",
            "Epoch: [26/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4866 (0.4797)\tTop 1-err 14.0625 (16.6029)\tTop 5-err 0.7812 (0.7399)\n",
            "Epoch: [26/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6757 (0.4809)\tTop 1-err 21.8750 (16.7600)\tTop 5-err 1.5625 (0.7502)\n",
            "Epoch: [26/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4263 (0.4832)\tTop 1-err 14.0625 (16.7735)\tTop 5-err 0.7812 (0.7563)\n",
            "Epoch: [26/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5850 (0.4835)\tTop 1-err 23.4375 (16.8189)\tTop 5-err 0.0000 (0.7008)\n",
            "Epoch: [26/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4827 (0.4848)\tTop 1-err 15.6250 (16.8447)\tTop 5-err 1.5625 (0.6944)\n",
            "Test (on val set): [26/200][0/79]\tTime 0.131 (0.131)\tLoss 0.5408 (0.5408)\tTop 1-err 21.8750 (21.8750)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [26/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8540 (0.7166)\tTop 1-err 24.2188 (22.9779)\tTop 5-err 2.3438 (1.2868)\n",
            "* Epoch: [26/200]\tTop 1-err 23.130\tTop 5-err 1.290\tTest Loss 0.723\n",
            "\n",
            "Epoch 28/200\n",
            "Epoch: [27/200][0/390]\tTime 0.197 (0.197)\tData 0.140 (0.140)\tLoss 0.4854 (0.4854)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [27/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5255 (0.4685)\tTop 1-err 17.1875 (16.5441)\tTop 5-err 0.7812 (0.4289)\n",
            "Epoch: [27/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5070 (0.4727)\tTop 1-err 18.7500 (16.4295)\tTop 5-err 0.0000 (0.6033)\n",
            "Epoch: [27/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4943 (0.4711)\tTop 1-err 20.3125 (16.4425)\tTop 5-err 0.0000 (0.5795)\n",
            "Epoch: [27/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4330 (0.4682)\tTop 1-err 17.1875 (16.3091)\tTop 5-err 0.0000 (0.5636)\n",
            "Epoch: [27/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4528 (0.4705)\tTop 1-err 17.1875 (16.2351)\tTop 5-err 0.7812 (0.5603)\n",
            "Epoch: [27/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5519 (0.4726)\tTop 1-err 19.5312 (16.2817)\tTop 5-err 2.3438 (0.6151)\n",
            "Epoch: [27/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4852 (0.4764)\tTop 1-err 14.8438 (16.3862)\tTop 5-err 1.5625 (0.6677)\n",
            "Test (on val set): [27/200][0/79]\tTime 0.126 (0.126)\tLoss 0.5365 (0.5365)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [27/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5566 (0.5752)\tTop 1-err 20.3125 (19.5925)\tTop 5-err 2.3438 (1.4706)\n",
            "* Epoch: [27/200]\tTop 1-err 19.600\tTop 5-err 1.310\tTest Loss 0.577\n",
            "\n",
            "Epoch 29/200\n",
            "Epoch: [28/200][0/390]\tTime 0.192 (0.192)\tData 0.133 (0.133)\tLoss 0.4613 (0.4613)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 2.3438 (2.3438)\n",
            "Epoch: [28/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4368 (0.4662)\tTop 1-err 14.8438 (16.2377)\tTop 5-err 0.0000 (0.6740)\n",
            "Epoch: [28/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5594 (0.4736)\tTop 1-err 18.7500 (16.2361)\tTop 5-err 0.0000 (0.7194)\n",
            "Epoch: [28/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4753 (0.4744)\tTop 1-err 17.1875 (16.3079)\tTop 5-err 1.5625 (0.7088)\n",
            "Epoch: [28/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4486 (0.4694)\tTop 1-err 14.0625 (16.0331)\tTop 5-err 0.7812 (0.6919)\n",
            "Epoch: [28/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4943 (0.4746)\tTop 1-err 11.7188 (16.1759)\tTop 5-err 2.3438 (0.7346)\n",
            "Epoch: [28/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3636 (0.4695)\tTop 1-err 9.3750 (15.9572)\tTop 5-err 0.0000 (0.7138)\n",
            "Epoch: [28/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.6198 (0.4705)\tTop 1-err 22.6562 (16.0524)\tTop 5-err 2.3438 (0.7367)\n",
            "Test (on val set): [28/200][0/79]\tTime 0.129 (0.129)\tLoss 0.6049 (0.6049)\tTop 1-err 21.8750 (21.8750)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [28/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8340 (0.6713)\tTop 1-err 25.0000 (21.9516)\tTop 5-err 0.7812 (1.1795)\n",
            "* Epoch: [28/200]\tTop 1-err 22.130\tTop 5-err 1.100\tTest Loss 0.665\n",
            "\n",
            "Epoch 30/200\n",
            "Epoch: [29/200][0/390]\tTime 0.191 (0.191)\tData 0.133 (0.133)\tLoss 0.5249 (0.5249)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [29/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4384 (0.4592)\tTop 1-err 16.4062 (15.8088)\tTop 5-err 0.7812 (0.7353)\n",
            "Epoch: [29/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4392 (0.4630)\tTop 1-err 15.6250 (15.9963)\tTop 5-err 0.7812 (0.6730)\n",
            "Epoch: [29/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3564 (0.4617)\tTop 1-err 10.1562 (15.9406)\tTop 5-err 0.7812 (0.6571)\n",
            "Epoch: [29/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4902 (0.4622)\tTop 1-err 17.9688 (15.8815)\tTop 5-err 1.5625 (0.6957)\n",
            "Epoch: [29/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4391 (0.4609)\tTop 1-err 14.8438 (15.9518)\tTop 5-err 0.0000 (0.7128)\n",
            "Epoch: [29/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4140 (0.4624)\tTop 1-err 12.5000 (16.0818)\tTop 5-err 0.7812 (0.7319)\n",
            "Epoch: [29/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4858 (0.4658)\tTop 1-err 18.7500 (16.2037)\tTop 5-err 0.0000 (0.7323)\n",
            "Test (on val set): [29/200][0/79]\tTime 0.125 (0.125)\tLoss 0.5794 (0.5794)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [29/200][50/79]\tTime 0.016 (0.018)\tLoss 0.9001 (0.8404)\tTop 1-err 25.0000 (24.8009)\tTop 5-err 1.5625 (1.7310)\n",
            "* Epoch: [29/200]\tTop 1-err 25.140\tTop 5-err 1.640\tTest Loss 0.833\n",
            "\n",
            "Epoch 31/200\n",
            "Epoch: [30/200][0/390]\tTime 0.191 (0.191)\tData 0.131 (0.131)\tLoss 0.3212 (0.3212)\tTop 1-err 8.5938 (8.5938)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [30/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3944 (0.4277)\tTop 1-err 13.2812 (14.5833)\tTop 5-err 0.7812 (0.6127)\n",
            "Epoch: [30/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5370 (0.4426)\tTop 1-err 17.9688 (15.1841)\tTop 5-err 1.5625 (0.5647)\n",
            "Epoch: [30/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3655 (0.4469)\tTop 1-err 11.7188 (15.5060)\tTop 5-err 2.3438 (0.5898)\n",
            "Epoch: [30/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4370 (0.4550)\tTop 1-err 14.8438 (15.7688)\tTop 5-err 1.5625 (0.6219)\n",
            "Epoch: [30/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3973 (0.4608)\tTop 1-err 15.6250 (15.9456)\tTop 5-err 0.0000 (0.6318)\n",
            "Epoch: [30/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4753 (0.4608)\tTop 1-err 14.0625 (15.9417)\tTop 5-err 0.7812 (0.6307)\n",
            "Epoch: [30/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4088 (0.4606)\tTop 1-err 14.8438 (15.9500)\tTop 5-err 0.0000 (0.6477)\n",
            "Test (on val set): [30/200][0/79]\tTime 0.136 (0.136)\tLoss 0.6044 (0.6044)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [30/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6069 (0.5869)\tTop 1-err 19.5312 (20.6801)\tTop 5-err 2.3438 (1.1029)\n",
            "* Epoch: [30/200]\tTop 1-err 20.450\tTop 5-err 1.020\tTest Loss 0.589\n",
            "\n",
            "Epoch 32/200\n",
            "Epoch: [31/200][0/390]\tTime 0.192 (0.192)\tData 0.133 (0.133)\tLoss 0.4577 (0.4577)\tTop 1-err 10.9375 (10.9375)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [31/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4718 (0.4580)\tTop 1-err 17.9688 (15.8241)\tTop 5-err 0.7812 (0.5821)\n",
            "Epoch: [31/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3923 (0.4569)\tTop 1-err 14.0625 (15.5786)\tTop 5-err 0.7812 (0.6652)\n",
            "Epoch: [31/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4979 (0.4653)\tTop 1-err 16.4062 (15.8423)\tTop 5-err 1.5625 (0.7243)\n",
            "Epoch: [31/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4243 (0.4617)\tTop 1-err 16.4062 (15.7377)\tTop 5-err 0.7812 (0.6413)\n",
            "Epoch: [31/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4892 (0.4613)\tTop 1-err 15.6250 (15.8242)\tTop 5-err 0.0000 (0.6599)\n",
            "Epoch: [31/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3288 (0.4629)\tTop 1-err 10.1562 (15.9183)\tTop 5-err 0.0000 (0.7086)\n",
            "Epoch: [31/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4866 (0.4610)\tTop 1-err 14.0625 (15.8409)\tTop 5-err 0.0000 (0.6766)\n",
            "Test (on val set): [31/200][0/79]\tTime 0.125 (0.125)\tLoss 0.6057 (0.6057)\tTop 1-err 23.4375 (23.4375)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [31/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5890 (0.5862)\tTop 1-err 17.1875 (19.9295)\tTop 5-err 0.7812 (1.0417)\n",
            "* Epoch: [31/200]\tTop 1-err 20.110\tTop 5-err 1.020\tTest Loss 0.586\n",
            "\n",
            "Epoch 33/200\n",
            "Epoch: [32/200][0/390]\tTime 0.205 (0.205)\tData 0.146 (0.146)\tLoss 0.4569 (0.4569)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [32/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5674 (0.4335)\tTop 1-err 19.5312 (14.8897)\tTop 5-err 0.7812 (0.6281)\n",
            "Epoch: [32/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3062 (0.4499)\tTop 1-err 9.3750 (15.3388)\tTop 5-err 0.7812 (0.6498)\n",
            "Epoch: [32/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4125 (0.4479)\tTop 1-err 10.9375 (15.3715)\tTop 5-err 0.7812 (0.6209)\n",
            "Epoch: [32/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5049 (0.4480)\tTop 1-err 18.7500 (15.3879)\tTop 5-err 0.7812 (0.5986)\n",
            "Epoch: [32/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5621 (0.4529)\tTop 1-err 25.0000 (15.5752)\tTop 5-err 0.0000 (0.5976)\n",
            "Epoch: [32/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4835 (0.4554)\tTop 1-err 19.5312 (15.6717)\tTop 5-err 0.0000 (0.6099)\n",
            "Epoch: [32/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4259 (0.4588)\tTop 1-err 13.2812 (15.8543)\tTop 5-err 0.7812 (0.6455)\n",
            "Test (on val set): [32/200][0/79]\tTime 0.143 (0.143)\tLoss 0.5759 (0.5759)\tTop 1-err 21.8750 (21.8750)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [32/200][50/79]\tTime 0.016 (0.019)\tLoss 0.5433 (0.5958)\tTop 1-err 19.5312 (18.9798)\tTop 5-err 0.7812 (1.2714)\n",
            "* Epoch: [32/200]\tTop 1-err 18.660\tTop 5-err 1.190\tTest Loss 0.588\n",
            "\n",
            "Epoch 34/200\n",
            "Epoch: [33/200][0/390]\tTime 0.207 (0.207)\tData 0.138 (0.138)\tLoss 0.3179 (0.3179)\tTop 1-err 10.9375 (10.9375)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [33/200][50/390]\tTime 0.056 (0.054)\tData 0.000 (0.003)\tLoss 0.3110 (0.4293)\tTop 1-err 12.5000 (14.8284)\tTop 5-err 0.0000 (0.6434)\n",
            "Epoch: [33/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.7190 (0.4267)\tTop 1-err 23.4375 (14.6813)\tTop 5-err 2.3438 (0.6188)\n",
            "Epoch: [33/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4788 (0.4361)\tTop 1-err 16.4062 (14.9472)\tTop 5-err 0.0000 (0.6312)\n",
            "Epoch: [33/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5413 (0.4415)\tTop 1-err 17.1875 (15.0692)\tTop 5-err 0.0000 (0.5947)\n",
            "Epoch: [33/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4606 (0.4416)\tTop 1-err 17.9688 (15.0710)\tTop 5-err 0.7812 (0.6069)\n",
            "Epoch: [33/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2799 (0.4436)\tTop 1-err 8.5938 (15.0799)\tTop 5-err 0.0000 (0.6203)\n",
            "Epoch: [33/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3592 (0.4458)\tTop 1-err 14.8438 (15.1309)\tTop 5-err 0.0000 (0.6299)\n",
            "Test (on val set): [33/200][0/79]\tTime 0.124 (0.124)\tLoss 0.5921 (0.5921)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [33/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6077 (0.6248)\tTop 1-err 18.7500 (21.0631)\tTop 5-err 0.7812 (1.2714)\n",
            "* Epoch: [33/200]\tTop 1-err 21.330\tTop 5-err 1.120\tTest Loss 0.625\n",
            "\n",
            "Epoch 35/200\n",
            "Epoch: [34/200][0/390]\tTime 0.188 (0.188)\tData 0.128 (0.128)\tLoss 0.4922 (0.4922)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [34/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4179 (0.4361)\tTop 1-err 14.8438 (14.9816)\tTop 5-err 0.0000 (0.5668)\n",
            "Epoch: [34/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3594 (0.4420)\tTop 1-err 10.9375 (15.2228)\tTop 5-err 0.0000 (0.5492)\n",
            "Epoch: [34/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2285 (0.4396)\tTop 1-err 6.2500 (15.0921)\tTop 5-err 0.7812 (0.5329)\n",
            "Epoch: [34/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5505 (0.4468)\tTop 1-err 17.9688 (15.3102)\tTop 5-err 0.7812 (0.5947)\n",
            "Epoch: [34/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4160 (0.4464)\tTop 1-err 14.8438 (15.3355)\tTop 5-err 0.0000 (0.6256)\n",
            "Epoch: [34/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5047 (0.4446)\tTop 1-err 17.9688 (15.2538)\tTop 5-err 0.0000 (0.6229)\n",
            "Epoch: [34/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4550 (0.4498)\tTop 1-err 14.8438 (15.4558)\tTop 5-err 1.5625 (0.6499)\n",
            "Test (on val set): [34/200][0/79]\tTime 0.133 (0.133)\tLoss 0.6746 (0.6746)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [34/200][50/79]\tTime 0.016 (0.018)\tLoss 0.9524 (0.7732)\tTop 1-err 25.0000 (23.1464)\tTop 5-err 1.5625 (1.5319)\n",
            "* Epoch: [34/200]\tTop 1-err 23.140\tTop 5-err 1.520\tTest Loss 0.764\n",
            "\n",
            "Epoch 36/200\n",
            "Epoch: [35/200][0/390]\tTime 0.201 (0.201)\tData 0.129 (0.129)\tLoss 0.4757 (0.4757)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [35/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4740 (0.4221)\tTop 1-err 18.7500 (14.6906)\tTop 5-err 0.0000 (0.6127)\n",
            "Epoch: [35/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3454 (0.4371)\tTop 1-err 11.7188 (15.1609)\tTop 5-err 0.7812 (0.6111)\n",
            "Epoch: [35/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3970 (0.4434)\tTop 1-err 13.2812 (15.3508)\tTop 5-err 0.7812 (0.6209)\n",
            "Epoch: [35/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4105 (0.4391)\tTop 1-err 13.2812 (15.1625)\tTop 5-err 0.7812 (0.6219)\n",
            "Epoch: [35/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4440 (0.4376)\tTop 1-err 12.5000 (15.0865)\tTop 5-err 0.0000 (0.6069)\n",
            "Epoch: [35/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4879 (0.4380)\tTop 1-err 17.1875 (15.1033)\tTop 5-err 0.7812 (0.6203)\n",
            "Epoch: [35/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5278 (0.4418)\tTop 1-err 14.8438 (15.2177)\tTop 5-err 0.0000 (0.5987)\n",
            "Test (on val set): [35/200][0/79]\tTime 0.132 (0.132)\tLoss 0.5623 (0.5623)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [35/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4945 (0.6488)\tTop 1-err 18.7500 (20.6801)\tTop 5-err 0.0000 (1.7463)\n",
            "* Epoch: [35/200]\tTop 1-err 20.990\tTop 5-err 1.620\tTest Loss 0.649\n",
            "\n",
            "Epoch 37/200\n",
            "Epoch: [36/200][0/390]\tTime 0.192 (0.192)\tData 0.132 (0.132)\tLoss 0.4008 (0.4008)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [36/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4011 (0.4061)\tTop 1-err 16.4062 (14.2923)\tTop 5-err 0.0000 (0.4442)\n",
            "Epoch: [36/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4930 (0.4259)\tTop 1-err 14.8438 (14.8051)\tTop 5-err 0.7812 (0.5105)\n",
            "Epoch: [36/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4053 (0.4255)\tTop 1-err 15.6250 (14.6213)\tTop 5-err 0.0000 (0.5277)\n",
            "Epoch: [36/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6276 (0.4321)\tTop 1-err 22.6562 (14.7582)\tTop 5-err 1.5625 (0.5364)\n",
            "Epoch: [36/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4213 (0.4341)\tTop 1-err 14.8438 (14.8220)\tTop 5-err 0.0000 (0.5696)\n",
            "Epoch: [36/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4076 (0.4347)\tTop 1-err 11.7188 (14.9060)\tTop 5-err 0.7812 (0.5684)\n",
            "Epoch: [36/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4242 (0.4376)\tTop 1-err 17.9688 (15.0174)\tTop 5-err 0.0000 (0.5876)\n",
            "Test (on val set): [36/200][0/79]\tTime 0.121 (0.121)\tLoss 0.5756 (0.5756)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [36/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5033 (0.5445)\tTop 1-err 17.9688 (18.1985)\tTop 5-err 0.7812 (0.9191)\n",
            "* Epoch: [36/200]\tTop 1-err 18.110\tTop 5-err 0.880\tTest Loss 0.539\n",
            "\n",
            "Epoch 38/200\n",
            "Epoch: [37/200][0/390]\tTime 0.193 (0.193)\tData 0.136 (0.136)\tLoss 0.4904 (0.4904)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [37/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4065 (0.4407)\tTop 1-err 14.8438 (15.4565)\tTop 5-err 0.0000 (0.5208)\n",
            "Epoch: [37/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5154 (0.4314)\tTop 1-err 16.4062 (15.0758)\tTop 5-err 0.7812 (0.5569)\n",
            "Epoch: [37/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4207 (0.4264)\tTop 1-err 13.2812 (14.7403)\tTop 5-err 0.7812 (0.5433)\n",
            "Epoch: [37/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4421 (0.4244)\tTop 1-err 14.8438 (14.6067)\tTop 5-err 0.7812 (0.5675)\n",
            "Epoch: [37/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3815 (0.4293)\tTop 1-err 10.1562 (14.7161)\tTop 5-err 0.7812 (0.5914)\n",
            "Epoch: [37/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3790 (0.4304)\tTop 1-err 10.1562 (14.7399)\tTop 5-err 0.0000 (0.5918)\n",
            "Epoch: [37/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3285 (0.4326)\tTop 1-err 10.9375 (14.8215)\tTop 5-err 0.0000 (0.5809)\n",
            "Test (on val set): [37/200][0/79]\tTime 0.124 (0.124)\tLoss 0.6455 (0.6455)\tTop 1-err 23.4375 (23.4375)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [37/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4320 (0.5937)\tTop 1-err 13.2812 (20.0368)\tTop 5-err 1.5625 (1.4246)\n",
            "* Epoch: [37/200]\tTop 1-err 20.280\tTop 5-err 1.290\tTest Loss 0.601\n",
            "\n",
            "Epoch 39/200\n",
            "Epoch: [38/200][0/390]\tTime 0.197 (0.197)\tData 0.127 (0.127)\tLoss 0.4337 (0.4337)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [38/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5273 (0.4030)\tTop 1-err 18.7500 (13.7868)\tTop 5-err 1.5625 (0.5208)\n",
            "Epoch: [38/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3474 (0.4130)\tTop 1-err 10.1562 (13.9387)\tTop 5-err 0.0000 (0.5337)\n",
            "Epoch: [38/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3888 (0.4240)\tTop 1-err 14.0625 (14.3471)\tTop 5-err 0.7812 (0.5795)\n",
            "Epoch: [38/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3733 (0.4262)\tTop 1-err 13.2812 (14.4512)\tTop 5-err 1.5625 (0.5908)\n",
            "Epoch: [38/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3600 (0.4268)\tTop 1-err 10.9375 (14.5605)\tTop 5-err 1.5625 (0.5852)\n",
            "Epoch: [38/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3273 (0.4248)\tTop 1-err 11.7188 (14.4856)\tTop 5-err 0.0000 (0.5944)\n",
            "Epoch: [38/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4525 (0.4280)\tTop 1-err 15.6250 (14.5900)\tTop 5-err 1.5625 (0.5987)\n",
            "Test (on val set): [38/200][0/79]\tTime 0.135 (0.135)\tLoss 0.4508 (0.4508)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [38/200][50/79]\tTime 0.016 (0.019)\tLoss 0.7842 (0.5805)\tTop 1-err 21.8750 (19.0411)\tTop 5-err 1.5625 (0.9957)\n",
            "* Epoch: [38/200]\tTop 1-err 18.860\tTop 5-err 0.940\tTest Loss 0.567\n",
            "\n",
            "Epoch 40/200\n",
            "Epoch: [39/200][0/390]\tTime 0.206 (0.206)\tData 0.135 (0.135)\tLoss 0.4334 (0.4334)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [39/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3071 (0.4071)\tTop 1-err 11.7188 (14.2004)\tTop 5-err 0.0000 (0.5515)\n",
            "Epoch: [39/200][100/390]\tTime 0.053 (0.052)\tData 0.000 (0.002)\tLoss 0.5522 (0.4271)\tTop 1-err 17.1875 (14.6658)\tTop 5-err 0.7812 (0.6111)\n",
            "Epoch: [39/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3406 (0.4227)\tTop 1-err 14.0625 (14.5902)\tTop 5-err 0.0000 (0.6260)\n",
            "Epoch: [39/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4675 (0.4257)\tTop 1-err 17.9688 (14.5717)\tTop 5-err 0.0000 (0.6025)\n",
            "Epoch: [39/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3897 (0.4282)\tTop 1-err 13.2812 (14.7628)\tTop 5-err 0.0000 (0.5883)\n",
            "Epoch: [39/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5051 (0.4272)\tTop 1-err 15.6250 (14.7347)\tTop 5-err 0.0000 (0.5451)\n",
            "Epoch: [39/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5064 (0.4300)\tTop 1-err 17.1875 (14.8170)\tTop 5-err 0.7812 (0.5542)\n",
            "Test (on val set): [39/200][0/79]\tTime 0.127 (0.127)\tLoss 0.5989 (0.5989)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [39/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6094 (0.6205)\tTop 1-err 21.0938 (21.2776)\tTop 5-err 0.7812 (0.9038)\n",
            "* Epoch: [39/200]\tTop 1-err 21.630\tTop 5-err 0.800\tTest Loss 0.623\n",
            "\n",
            "Epoch 41/200\n",
            "Epoch: [40/200][0/390]\tTime 0.194 (0.194)\tData 0.135 (0.135)\tLoss 0.3092 (0.3092)\tTop 1-err 10.9375 (10.9375)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [40/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3796 (0.4108)\tTop 1-err 13.2812 (14.4914)\tTop 5-err 0.0000 (0.5362)\n",
            "Epoch: [40/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4111 (0.4157)\tTop 1-err 10.1562 (14.6117)\tTop 5-err 0.0000 (0.5260)\n",
            "Epoch: [40/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3812 (0.4164)\tTop 1-err 14.0625 (14.7454)\tTop 5-err 0.0000 (0.5484)\n",
            "Epoch: [40/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4226 (0.4216)\tTop 1-err 14.0625 (14.7816)\tTop 5-err 1.5625 (0.5636)\n",
            "Epoch: [40/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4203 (0.4186)\tTop 1-err 14.8438 (14.5543)\tTop 5-err 0.0000 (0.5447)\n",
            "Epoch: [40/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4101 (0.4229)\tTop 1-err 16.4062 (14.6958)\tTop 5-err 0.7812 (0.5554)\n",
            "Epoch: [40/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3747 (0.4270)\tTop 1-err 11.7188 (14.8616)\tTop 5-err 0.7812 (0.5631)\n",
            "Test (on val set): [40/200][0/79]\tTime 0.121 (0.121)\tLoss 0.3872 (0.3872)\tTop 1-err 14.0625 (14.0625)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [40/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7898 (0.6043)\tTop 1-err 23.4375 (19.6232)\tTop 5-err 0.7812 (1.2255)\n",
            "* Epoch: [40/200]\tTop 1-err 20.060\tTop 5-err 1.270\tTest Loss 0.607\n",
            "\n",
            "Epoch 42/200\n",
            "Epoch: [41/200][0/390]\tTime 0.208 (0.208)\tData 0.138 (0.138)\tLoss 0.4422 (0.4422)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [41/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.3884 (0.3924)\tTop 1-err 12.5000 (13.3425)\tTop 5-err 0.0000 (0.4442)\n",
            "Epoch: [41/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.4575 (0.3955)\tTop 1-err 17.1875 (13.4978)\tTop 5-err 0.7812 (0.4409)\n",
            "Epoch: [41/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5195 (0.4041)\tTop 1-err 14.8438 (13.7055)\tTop 5-err 1.5625 (0.4967)\n",
            "Epoch: [41/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3621 (0.4028)\tTop 1-err 15.6250 (13.6622)\tTop 5-err 0.0000 (0.5053)\n",
            "Epoch: [41/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5992 (0.4074)\tTop 1-err 25.0000 (13.9193)\tTop 5-err 0.7812 (0.5167)\n",
            "Epoch: [41/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4405 (0.4096)\tTop 1-err 10.1562 (14.0599)\tTop 5-err 0.7812 (0.5009)\n",
            "Epoch: [41/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4119 (0.4161)\tTop 1-err 17.9688 (14.2628)\tTop 5-err 0.0000 (0.5119)\n",
            "Test (on val set): [41/200][0/79]\tTime 0.124 (0.124)\tLoss 0.6890 (0.6890)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [41/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5303 (0.5901)\tTop 1-err 18.7500 (19.6078)\tTop 5-err 1.5625 (1.1336)\n",
            "* Epoch: [41/200]\tTop 1-err 19.620\tTop 5-err 1.010\tTest Loss 0.586\n",
            "\n",
            "Epoch 43/200\n",
            "Epoch: [42/200][0/390]\tTime 0.190 (0.190)\tData 0.130 (0.130)\tLoss 0.3180 (0.3180)\tTop 1-err 10.1562 (10.1562)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [42/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3474 (0.4233)\tTop 1-err 13.2812 (14.4301)\tTop 5-err 0.0000 (0.5821)\n",
            "Epoch: [42/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4751 (0.4072)\tTop 1-err 17.9688 (14.1321)\tTop 5-err 0.0000 (0.5260)\n",
            "Epoch: [42/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2747 (0.4033)\tTop 1-err 10.1562 (13.7986)\tTop 5-err 0.7812 (0.5433)\n",
            "Epoch: [42/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3678 (0.4121)\tTop 1-err 13.2812 (14.0586)\tTop 5-err 0.0000 (0.5442)\n",
            "Epoch: [42/200][250/390]\tTime 0.051 (0.051)\tData 0.000 (0.001)\tLoss 0.4678 (0.4184)\tTop 1-err 16.4062 (14.3644)\tTop 5-err 0.0000 (0.6007)\n",
            "Epoch: [42/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4173 (0.4227)\tTop 1-err 16.4062 (14.5712)\tTop 5-err 0.7812 (0.6099)\n",
            "Epoch: [42/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4238 (0.4253)\tTop 1-err 14.8438 (14.6590)\tTop 5-err 0.0000 (0.5965)\n",
            "Test (on val set): [42/200][0/79]\tTime 0.129 (0.129)\tLoss 0.5434 (0.5434)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [42/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6278 (0.5538)\tTop 1-err 19.5312 (17.6471)\tTop 5-err 1.5625 (1.0417)\n",
            "* Epoch: [42/200]\tTop 1-err 17.620\tTop 5-err 0.940\tTest Loss 0.549\n",
            "\n",
            "Epoch 44/200\n",
            "Epoch: [43/200][0/390]\tTime 0.192 (0.192)\tData 0.134 (0.134)\tLoss 0.3402 (0.3402)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [43/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4985 (0.4064)\tTop 1-err 19.5312 (13.7868)\tTop 5-err 0.0000 (0.6127)\n",
            "Epoch: [43/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3992 (0.3983)\tTop 1-err 10.9375 (13.5210)\tTop 5-err 1.5625 (0.5260)\n",
            "Epoch: [43/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3666 (0.4041)\tTop 1-err 10.1562 (13.7831)\tTop 5-err 0.7812 (0.5381)\n",
            "Epoch: [43/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3149 (0.4092)\tTop 1-err 11.7188 (14.0664)\tTop 5-err 0.0000 (0.5597)\n",
            "Epoch: [43/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5343 (0.4112)\tTop 1-err 21.0938 (14.1434)\tTop 5-err 0.0000 (0.5758)\n",
            "Epoch: [43/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5926 (0.4194)\tTop 1-err 18.7500 (14.3714)\tTop 5-err 0.0000 (0.5918)\n",
            "Epoch: [43/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3492 (0.4175)\tTop 1-err 12.5000 (14.2495)\tTop 5-err 0.0000 (0.5943)\n",
            "Test (on val set): [43/200][0/79]\tTime 0.124 (0.124)\tLoss 0.5637 (0.5637)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [43/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5715 (0.5751)\tTop 1-err 22.6562 (18.5815)\tTop 5-err 0.0000 (1.3174)\n",
            "* Epoch: [43/200]\tTop 1-err 18.660\tTop 5-err 1.280\tTest Loss 0.572\n",
            "\n",
            "Epoch 45/200\n",
            "Epoch: [44/200][0/390]\tTime 0.188 (0.188)\tData 0.128 (0.128)\tLoss 0.4481 (0.4481)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [44/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3412 (0.3943)\tTop 1-err 14.8438 (13.6336)\tTop 5-err 0.0000 (0.5362)\n",
            "Epoch: [44/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3981 (0.3994)\tTop 1-err 12.5000 (13.8150)\tTop 5-err 0.7812 (0.5415)\n",
            "Epoch: [44/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3542 (0.4033)\tTop 1-err 13.2812 (13.8711)\tTop 5-err 0.0000 (0.5381)\n",
            "Epoch: [44/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3632 (0.4075)\tTop 1-err 10.1562 (14.0858)\tTop 5-err 0.7812 (0.5519)\n",
            "Epoch: [44/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4871 (0.4058)\tTop 1-err 14.8438 (13.9816)\tTop 5-err 0.7812 (0.5416)\n",
            "Epoch: [44/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4072 (0.4075)\tTop 1-err 16.4062 (14.1611)\tTop 5-err 2.3438 (0.5347)\n",
            "Epoch: [44/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4564 (0.4122)\tTop 1-err 19.5312 (14.3029)\tTop 5-err 0.7812 (0.5587)\n",
            "Test (on val set): [44/200][0/79]\tTime 0.122 (0.122)\tLoss 0.4738 (0.4738)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [44/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7726 (0.5395)\tTop 1-err 22.6562 (17.9841)\tTop 5-err 2.3438 (0.6893)\n",
            "* Epoch: [44/200]\tTop 1-err 18.040\tTop 5-err 0.720\tTest Loss 0.542\n",
            "\n",
            "Epoch 46/200\n",
            "Epoch: [45/200][0/390]\tTime 0.197 (0.197)\tData 0.138 (0.138)\tLoss 0.4130 (0.4130)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [45/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3741 (0.3919)\tTop 1-err 13.2812 (13.1127)\tTop 5-err 1.5625 (0.4749)\n",
            "Epoch: [45/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4716 (0.4017)\tTop 1-err 16.4062 (13.6603)\tTop 5-err 1.5625 (0.5183)\n",
            "Epoch: [45/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4724 (0.4062)\tTop 1-err 14.8438 (13.9383)\tTop 5-err 0.7812 (0.5691)\n",
            "Epoch: [45/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4425 (0.4100)\tTop 1-err 14.0625 (14.1713)\tTop 5-err 0.0000 (0.5519)\n",
            "Epoch: [45/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4178 (0.4092)\tTop 1-err 14.8438 (14.1559)\tTop 5-err 0.7812 (0.5540)\n",
            "Epoch: [45/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3641 (0.4101)\tTop 1-err 14.0625 (14.1300)\tTop 5-err 0.0000 (0.5736)\n",
            "Epoch: [45/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4719 (0.4144)\tTop 1-err 11.7188 (14.2673)\tTop 5-err 1.5625 (0.5809)\n",
            "Test (on val set): [45/200][0/79]\tTime 0.134 (0.134)\tLoss 0.5082 (0.5082)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [45/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5968 (0.6487)\tTop 1-err 23.4375 (20.2819)\tTop 5-err 2.3438 (1.4553)\n",
            "* Epoch: [45/200]\tTop 1-err 20.690\tTop 5-err 1.430\tTest Loss 0.654\n",
            "\n",
            "Epoch 47/200\n",
            "Epoch: [46/200][0/390]\tTime 0.206 (0.206)\tData 0.137 (0.137)\tLoss 0.3507 (0.3507)\tTop 1-err 9.3750 (9.3750)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [46/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3495 (0.4014)\tTop 1-err 13.2812 (13.4191)\tTop 5-err 1.5625 (0.5821)\n",
            "Epoch: [46/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3836 (0.4035)\tTop 1-err 11.7188 (13.5984)\tTop 5-err 0.7812 (0.6343)\n",
            "Epoch: [46/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5168 (0.4065)\tTop 1-err 17.9688 (13.7159)\tTop 5-err 1.5625 (0.6416)\n",
            "Epoch: [46/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4408 (0.4048)\tTop 1-err 17.1875 (13.6855)\tTop 5-err 0.0000 (0.6102)\n",
            "Epoch: [46/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2973 (0.4056)\tTop 1-err 8.5938 (13.6548)\tTop 5-err 0.7812 (0.6069)\n",
            "Epoch: [46/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5048 (0.4087)\tTop 1-err 18.7500 (13.8159)\tTop 5-err 0.7812 (0.5918)\n",
            "Epoch: [46/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3808 (0.4108)\tTop 1-err 14.8438 (13.9223)\tTop 5-err 0.7812 (0.6165)\n",
            "Test (on val set): [46/200][0/79]\tTime 0.131 (0.131)\tLoss 0.5582 (0.5582)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [46/200][50/79]\tTime 0.016 (0.019)\tLoss 0.6625 (0.5999)\tTop 1-err 22.6562 (19.3627)\tTop 5-err 0.7812 (0.9804)\n",
            "* Epoch: [46/200]\tTop 1-err 19.180\tTop 5-err 0.960\tTest Loss 0.598\n",
            "\n",
            "Epoch 48/200\n",
            "Epoch: [47/200][0/390]\tTime 0.193 (0.193)\tData 0.133 (0.133)\tLoss 0.3220 (0.3220)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [47/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3998 (0.3987)\tTop 1-err 10.9375 (13.2812)\tTop 5-err 0.0000 (0.6281)\n",
            "Epoch: [47/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5120 (0.4039)\tTop 1-err 18.7500 (13.6139)\tTop 5-err 0.7812 (0.5569)\n",
            "Epoch: [47/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2730 (0.4043)\tTop 1-err 9.3750 (13.6745)\tTop 5-err 0.7812 (0.5381)\n",
            "Epoch: [47/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4364 (0.4068)\tTop 1-err 14.0625 (13.8604)\tTop 5-err 0.7812 (0.5364)\n",
            "Epoch: [47/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3826 (0.4044)\tTop 1-err 14.0625 (13.8322)\tTop 5-err 0.0000 (0.5229)\n",
            "Epoch: [47/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4758 (0.4035)\tTop 1-err 17.9688 (13.8367)\tTop 5-err 1.5625 (0.5113)\n",
            "Epoch: [47/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3395 (0.4045)\tTop 1-err 9.3750 (13.9156)\tTop 5-err 0.0000 (0.4963)\n",
            "Test (on val set): [47/200][0/79]\tTime 0.138 (0.138)\tLoss 0.4073 (0.4073)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [47/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4478 (0.6011)\tTop 1-err 13.2812 (19.3474)\tTop 5-err 0.0000 (1.4706)\n",
            "* Epoch: [47/200]\tTop 1-err 19.540\tTop 5-err 1.370\tTest Loss 0.598\n",
            "\n",
            "Epoch 49/200\n",
            "Epoch: [48/200][0/390]\tTime 0.190 (0.190)\tData 0.130 (0.130)\tLoss 0.3967 (0.3967)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [48/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5467 (0.3875)\tTop 1-err 22.6562 (13.5570)\tTop 5-err 0.7812 (0.4442)\n",
            "Epoch: [48/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.2827 (0.3981)\tTop 1-err 10.9375 (13.7686)\tTop 5-err 0.0000 (0.4873)\n",
            "Epoch: [48/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4287 (0.4009)\tTop 1-err 17.1875 (13.9021)\tTop 5-err 0.7812 (0.5019)\n",
            "Epoch: [48/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4534 (0.4071)\tTop 1-err 17.9688 (14.1014)\tTop 5-err 0.7812 (0.5014)\n",
            "Epoch: [48/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3283 (0.4122)\tTop 1-err 11.7188 (14.2119)\tTop 5-err 0.0000 (0.5260)\n",
            "Epoch: [48/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3876 (0.4158)\tTop 1-err 10.1562 (14.2961)\tTop 5-err 0.0000 (0.5451)\n",
            "Epoch: [48/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4751 (0.4134)\tTop 1-err 16.4062 (14.2250)\tTop 5-err 1.5625 (0.5275)\n",
            "Test (on val set): [48/200][0/79]\tTime 0.127 (0.127)\tLoss 0.5368 (0.5368)\tTop 1-err 21.8750 (21.8750)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [48/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6013 (0.6954)\tTop 1-err 19.5312 (22.1967)\tTop 5-err 0.0000 (1.5165)\n",
            "* Epoch: [48/200]\tTop 1-err 22.330\tTop 5-err 1.410\tTest Loss 0.694\n",
            "\n",
            "Epoch 50/200\n",
            "Epoch: [49/200][0/390]\tTime 0.188 (0.188)\tData 0.130 (0.130)\tLoss 0.4764 (0.4764)\tTop 1-err 14.0625 (14.0625)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [49/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4416 (0.3774)\tTop 1-err 18.7500 (12.9596)\tTop 5-err 0.0000 (0.4136)\n",
            "Epoch: [49/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4254 (0.3961)\tTop 1-err 14.0625 (13.5752)\tTop 5-err 0.0000 (0.4950)\n",
            "Epoch: [49/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4084 (0.4032)\tTop 1-err 14.0625 (13.7210)\tTop 5-err 0.7812 (0.4863)\n",
            "Epoch: [49/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5367 (0.4039)\tTop 1-err 15.6250 (13.8526)\tTop 5-err 0.7812 (0.4859)\n",
            "Epoch: [49/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3217 (0.4023)\tTop 1-err 11.7188 (13.7575)\tTop 5-err 0.0000 (0.4824)\n",
            "Epoch: [49/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3905 (0.4013)\tTop 1-err 14.8438 (13.7407)\tTop 5-err 0.7812 (0.4931)\n",
            "Epoch: [49/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2796 (0.4059)\tTop 1-err 9.3750 (13.8377)\tTop 5-err 0.0000 (0.5208)\n",
            "Test (on val set): [49/200][0/79]\tTime 0.137 (0.137)\tLoss 0.4789 (0.4789)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [49/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5871 (0.5299)\tTop 1-err 18.7500 (17.4786)\tTop 5-err 0.0000 (0.7200)\n",
            "* Epoch: [49/200]\tTop 1-err 17.450\tTop 5-err 0.760\tTest Loss 0.521\n",
            "\n",
            "Epoch 51/200\n",
            "Epoch: [50/200][0/390]\tTime 0.189 (0.189)\tData 0.129 (0.129)\tLoss 0.4085 (0.4085)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [50/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3931 (0.3841)\tTop 1-err 14.0625 (12.8830)\tTop 5-err 0.0000 (0.4902)\n",
            "Epoch: [50/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4405 (0.3946)\tTop 1-err 17.9688 (13.3354)\tTop 5-err 0.0000 (0.4564)\n",
            "Epoch: [50/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3112 (0.3910)\tTop 1-err 8.5938 (13.1364)\tTop 5-err 0.0000 (0.4656)\n",
            "Epoch: [50/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3447 (0.3951)\tTop 1-err 12.5000 (13.3357)\tTop 5-err 0.0000 (0.4586)\n",
            "Epoch: [50/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4942 (0.4026)\tTop 1-err 14.0625 (13.5863)\tTop 5-err 0.7812 (0.5198)\n",
            "Epoch: [50/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3917 (0.4038)\tTop 1-err 11.7188 (13.7017)\tTop 5-err 0.0000 (0.5139)\n",
            "Epoch: [50/200][350/390]\tTime 0.051 (0.050)\tData 0.000 (0.001)\tLoss 0.3968 (0.4055)\tTop 1-err 12.5000 (13.7954)\tTop 5-err 0.0000 (0.5164)\n",
            "Test (on val set): [50/200][0/79]\tTime 0.136 (0.136)\tLoss 0.5965 (0.5965)\tTop 1-err 23.4375 (23.4375)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [50/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6757 (0.6276)\tTop 1-err 22.6562 (21.7678)\tTop 5-err 1.5625 (0.7659)\n",
            "* Epoch: [50/200]\tTop 1-err 21.830\tTop 5-err 0.740\tTest Loss 0.623\n",
            "\n",
            "Epoch 52/200\n",
            "Epoch: [51/200][0/390]\tTime 0.224 (0.224)\tData 0.163 (0.163)\tLoss 0.4511 (0.4511)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [51/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5099 (0.3886)\tTop 1-err 22.6562 (13.7102)\tTop 5-err 0.7812 (0.3830)\n",
            "Epoch: [51/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.4479 (0.3836)\tTop 1-err 12.5000 (13.3586)\tTop 5-err 1.5625 (0.3790)\n",
            "Epoch: [51/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3779 (0.4056)\tTop 1-err 14.0625 (14.1505)\tTop 5-err 0.7812 (0.4191)\n",
            "Epoch: [51/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4312 (0.4097)\tTop 1-err 15.6250 (14.2530)\tTop 5-err 0.0000 (0.4314)\n",
            "Epoch: [51/200][250/390]\tTime 0.057 (0.051)\tData 0.000 (0.001)\tLoss 0.3613 (0.4137)\tTop 1-err 9.3750 (14.3551)\tTop 5-err 0.7812 (0.4575)\n",
            "Epoch: [51/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3946 (0.4146)\tTop 1-err 14.8438 (14.3662)\tTop 5-err 0.0000 (0.4698)\n",
            "Epoch: [51/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2365 (0.4125)\tTop 1-err 7.0312 (14.1849)\tTop 5-err 0.0000 (0.5008)\n",
            "Test (on val set): [51/200][0/79]\tTime 0.139 (0.139)\tLoss 0.6754 (0.6754)\tTop 1-err 22.6562 (22.6562)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [51/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6341 (0.7358)\tTop 1-err 20.3125 (22.9320)\tTop 5-err 0.7812 (1.4400)\n",
            "* Epoch: [51/200]\tTop 1-err 22.580\tTop 5-err 1.410\tTest Loss 0.732\n",
            "\n",
            "Epoch 53/200\n",
            "Epoch: [52/200][0/390]\tTime 0.189 (0.189)\tData 0.129 (0.129)\tLoss 0.3771 (0.3771)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [52/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2719 (0.3907)\tTop 1-err 10.9375 (13.1893)\tTop 5-err 0.7812 (0.5055)\n",
            "Epoch: [52/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3993 (0.3864)\tTop 1-err 14.0625 (13.2194)\tTop 5-err 0.0000 (0.4796)\n",
            "Epoch: [52/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3600 (0.4028)\tTop 1-err 13.2812 (13.8711)\tTop 5-err 0.0000 (0.5329)\n",
            "Epoch: [52/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2648 (0.4004)\tTop 1-err 8.5938 (13.7477)\tTop 5-err 0.0000 (0.5169)\n",
            "Epoch: [52/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4548 (0.4029)\tTop 1-err 11.7188 (13.8010)\tTop 5-err 0.7812 (0.5198)\n",
            "Epoch: [52/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3664 (0.4006)\tTop 1-err 14.0625 (13.7121)\tTop 5-err 0.0000 (0.5217)\n",
            "Epoch: [52/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4290 (0.4019)\tTop 1-err 14.0625 (13.8065)\tTop 5-err 0.7812 (0.4986)\n",
            "Test (on val set): [52/200][0/79]\tTime 0.142 (0.142)\tLoss 0.6112 (0.6112)\tTop 1-err 22.6562 (22.6562)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [52/200][50/79]\tTime 0.016 (0.019)\tLoss 0.6636 (0.6854)\tTop 1-err 24.2188 (21.3082)\tTop 5-err 0.0000 (1.6850)\n",
            "* Epoch: [52/200]\tTop 1-err 21.340\tTop 5-err 1.640\tTest Loss 0.684\n",
            "\n",
            "Epoch 54/200\n",
            "Epoch: [53/200][0/390]\tTime 0.198 (0.198)\tData 0.137 (0.137)\tLoss 0.4224 (0.4224)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [53/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5008 (0.3768)\tTop 1-err 17.1875 (12.2855)\tTop 5-err 1.5625 (0.6127)\n",
            "Epoch: [53/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5933 (0.3942)\tTop 1-err 21.8750 (13.5520)\tTop 5-err 0.7812 (0.5569)\n",
            "Epoch: [53/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4592 (0.3963)\tTop 1-err 14.0625 (13.5658)\tTop 5-err 0.7812 (0.4863)\n",
            "Epoch: [53/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3478 (0.3945)\tTop 1-err 9.3750 (13.4950)\tTop 5-err 0.0000 (0.4975)\n",
            "Epoch: [53/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3888 (0.3990)\tTop 1-err 11.7188 (13.6267)\tTop 5-err 0.0000 (0.5042)\n",
            "Epoch: [53/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5244 (0.4032)\tTop 1-err 16.4062 (13.7926)\tTop 5-err 0.0000 (0.5139)\n",
            "Epoch: [53/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3710 (0.4039)\tTop 1-err 15.6250 (13.8800)\tTop 5-err 0.0000 (0.5164)\n",
            "Test (on val set): [53/200][0/79]\tTime 0.124 (0.124)\tLoss 0.4781 (0.4781)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [53/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4849 (0.4942)\tTop 1-err 14.8438 (16.4522)\tTop 5-err 1.5625 (0.8119)\n",
            "* Epoch: [53/200]\tTop 1-err 16.720\tTop 5-err 0.760\tTest Loss 0.504\n",
            "\n",
            "Epoch 55/200\n",
            "Epoch: [54/200][0/390]\tTime 0.198 (0.198)\tData 0.128 (0.128)\tLoss 0.3169 (0.3169)\tTop 1-err 9.3750 (9.3750)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [54/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2990 (0.3780)\tTop 1-err 11.7188 (13.0515)\tTop 5-err 0.0000 (0.5208)\n",
            "Epoch: [54/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4245 (0.4031)\tTop 1-err 13.2812 (13.6293)\tTop 5-err 0.0000 (0.5105)\n",
            "Epoch: [54/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3591 (0.4000)\tTop 1-err 10.9375 (13.6796)\tTop 5-err 0.7812 (0.5381)\n",
            "Epoch: [54/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3629 (0.3985)\tTop 1-err 10.1562 (13.6738)\tTop 5-err 0.7812 (0.4975)\n",
            "Epoch: [54/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4075 (0.4036)\tTop 1-err 11.7188 (13.9100)\tTop 5-err 0.0000 (0.5105)\n",
            "Epoch: [54/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3793 (0.3996)\tTop 1-err 16.4062 (13.7718)\tTop 5-err 0.0000 (0.5061)\n",
            "Epoch: [54/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3483 (0.4030)\tTop 1-err 12.5000 (13.8466)\tTop 5-err 0.0000 (0.5075)\n",
            "Test (on val set): [54/200][0/79]\tTime 0.125 (0.125)\tLoss 0.5521 (0.5521)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [54/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6592 (0.7352)\tTop 1-err 17.9688 (21.9516)\tTop 5-err 3.9062 (2.1599)\n",
            "* Epoch: [54/200]\tTop 1-err 22.060\tTop 5-err 2.090\tTest Loss 0.740\n",
            "\n",
            "Epoch 56/200\n",
            "Epoch: [55/200][0/390]\tTime 0.194 (0.194)\tData 0.138 (0.138)\tLoss 0.3662 (0.3662)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [55/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3522 (0.3719)\tTop 1-err 10.9375 (12.3928)\tTop 5-err 0.7812 (0.4289)\n",
            "Epoch: [55/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4676 (0.3793)\tTop 1-err 14.8438 (12.8171)\tTop 5-err 1.5625 (0.5260)\n",
            "Epoch: [55/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4792 (0.3859)\tTop 1-err 16.4062 (12.9967)\tTop 5-err 0.7812 (0.5433)\n",
            "Epoch: [55/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4215 (0.3909)\tTop 1-err 15.6250 (13.2346)\tTop 5-err 1.5625 (0.5519)\n",
            "Epoch: [55/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4024 (0.3897)\tTop 1-err 11.7188 (13.2346)\tTop 5-err 0.0000 (0.5291)\n",
            "Epoch: [55/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3555 (0.3875)\tTop 1-err 13.2812 (13.2060)\tTop 5-err 0.0000 (0.5087)\n",
            "Epoch: [55/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5079 (0.3920)\tTop 1-err 12.5000 (13.4348)\tTop 5-err 1.5625 (0.5097)\n",
            "Test (on val set): [55/200][0/79]\tTime 0.125 (0.125)\tLoss 0.4938 (0.4938)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [55/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6069 (0.5798)\tTop 1-err 17.9688 (18.6275)\tTop 5-err 1.5625 (1.3174)\n",
            "* Epoch: [55/200]\tTop 1-err 18.810\tTop 5-err 1.270\tTest Loss 0.577\n",
            "\n",
            "Epoch 57/200\n",
            "Epoch: [56/200][0/390]\tTime 0.188 (0.188)\tData 0.131 (0.131)\tLoss 0.2902 (0.2902)\tTop 1-err 8.5938 (8.5938)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [56/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4096 (0.3861)\tTop 1-err 14.0625 (13.2506)\tTop 5-err 0.0000 (0.3830)\n",
            "Epoch: [56/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3739 (0.3988)\tTop 1-err 11.7188 (13.5210)\tTop 5-err 1.5625 (0.5105)\n",
            "Epoch: [56/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4397 (0.3945)\tTop 1-err 10.1562 (13.2347)\tTop 5-err 0.0000 (0.4553)\n",
            "Epoch: [56/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4406 (0.3916)\tTop 1-err 17.1875 (13.1802)\tTop 5-err 0.7812 (0.4664)\n",
            "Epoch: [56/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5317 (0.3902)\tTop 1-err 14.0625 (13.1879)\tTop 5-err 1.5625 (0.4949)\n",
            "Epoch: [56/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3485 (0.3931)\tTop 1-err 13.2812 (13.3072)\tTop 5-err 0.7812 (0.4931)\n",
            "Epoch: [56/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3310 (0.3938)\tTop 1-err 10.1562 (13.4237)\tTop 5-err 0.7812 (0.4919)\n",
            "Test (on val set): [56/200][0/79]\tTime 0.131 (0.131)\tLoss 0.6159 (0.6159)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [56/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6340 (0.6410)\tTop 1-err 19.5312 (20.5882)\tTop 5-err 0.0000 (1.5319)\n",
            "* Epoch: [56/200]\tTop 1-err 20.390\tTop 5-err 1.400\tTest Loss 0.634\n",
            "\n",
            "Epoch 58/200\n",
            "Epoch: [57/200][0/390]\tTime 0.190 (0.190)\tData 0.131 (0.131)\tLoss 0.4694 (0.4694)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [57/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3056 (0.3980)\tTop 1-err 10.1562 (13.3119)\tTop 5-err 0.0000 (0.5821)\n",
            "Epoch: [57/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4844 (0.3922)\tTop 1-err 14.8438 (13.2658)\tTop 5-err 0.0000 (0.5569)\n",
            "Epoch: [57/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5968 (0.3939)\tTop 1-err 16.4062 (13.3899)\tTop 5-err 0.7812 (0.5174)\n",
            "Epoch: [57/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3953 (0.3959)\tTop 1-err 10.1562 (13.5339)\tTop 5-err 2.3438 (0.4975)\n",
            "Epoch: [57/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3784 (0.3955)\tTop 1-err 13.2812 (13.5303)\tTop 5-err 0.7812 (0.5011)\n",
            "Epoch: [57/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3981 (0.3929)\tTop 1-err 12.5000 (13.4733)\tTop 5-err 0.7812 (0.4828)\n",
            "Epoch: [57/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4427 (0.3939)\tTop 1-err 16.4062 (13.5305)\tTop 5-err 0.7812 (0.5164)\n",
            "Test (on val set): [57/200][0/79]\tTime 0.129 (0.129)\tLoss 0.5830 (0.5830)\tTop 1-err 21.0938 (21.0938)\tTop 5-err 3.9062 (3.9062)\n",
            "Test (on val set): [57/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6778 (0.6906)\tTop 1-err 23.4375 (23.2537)\tTop 5-err 0.7812 (1.8382)\n",
            "* Epoch: [57/200]\tTop 1-err 23.160\tTop 5-err 1.680\tTest Loss 0.685\n",
            "\n",
            "Epoch 59/200\n",
            "Epoch: [58/200][0/390]\tTime 0.200 (0.200)\tData 0.130 (0.130)\tLoss 0.4142 (0.4142)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [58/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2812 (0.3560)\tTop 1-err 10.9375 (12.0251)\tTop 5-err 0.0000 (0.4136)\n",
            "Epoch: [58/200][100/390]\tTime 0.051 (0.052)\tData 0.000 (0.002)\tLoss 0.3283 (0.3795)\tTop 1-err 10.1562 (12.8868)\tTop 5-err 0.7812 (0.4796)\n",
            "Epoch: [58/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3117 (0.3817)\tTop 1-err 7.0312 (13.0070)\tTop 5-err 0.0000 (0.4863)\n",
            "Epoch: [58/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4037 (0.3853)\tTop 1-err 13.2812 (13.0791)\tTop 5-err 1.5625 (0.4781)\n",
            "Epoch: [58/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3772 (0.3866)\tTop 1-err 14.0625 (13.2501)\tTop 5-err 0.7812 (0.4887)\n",
            "Epoch: [58/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3703 (0.3869)\tTop 1-err 14.8438 (13.2709)\tTop 5-err 1.5625 (0.4906)\n",
            "Epoch: [58/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4530 (0.3906)\tTop 1-err 15.6250 (13.3547)\tTop 5-err 0.7812 (0.5075)\n",
            "Test (on val set): [58/200][0/79]\tTime 0.133 (0.133)\tLoss 0.4657 (0.4657)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [58/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5815 (0.4885)\tTop 1-err 15.6250 (16.3450)\tTop 5-err 0.7812 (0.8272)\n",
            "* Epoch: [58/200]\tTop 1-err 16.300\tTop 5-err 0.780\tTest Loss 0.482\n",
            "\n",
            "Epoch 60/200\n",
            "Epoch: [59/200][0/390]\tTime 0.205 (0.205)\tData 0.146 (0.146)\tLoss 0.4251 (0.4251)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [59/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4317 (0.3943)\tTop 1-err 17.9688 (13.3885)\tTop 5-err 0.7812 (0.4902)\n",
            "Epoch: [59/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3277 (0.3840)\tTop 1-err 13.2812 (13.0183)\tTop 5-err 0.7812 (0.4718)\n",
            "Epoch: [59/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3638 (0.3872)\tTop 1-err 12.5000 (13.0484)\tTop 5-err 0.0000 (0.4656)\n",
            "Epoch: [59/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3323 (0.3879)\tTop 1-err 10.9375 (13.1491)\tTop 5-err 0.0000 (0.5169)\n",
            "Epoch: [59/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4096 (0.3915)\tTop 1-err 13.2812 (13.2346)\tTop 5-err 1.5625 (0.5042)\n",
            "Epoch: [59/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3527 (0.3919)\tTop 1-err 10.9375 (13.2890)\tTop 5-err 0.0000 (0.5113)\n",
            "Epoch: [59/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2818 (0.3896)\tTop 1-err 12.5000 (13.2234)\tTop 5-err 0.0000 (0.5053)\n",
            "Test (on val set): [59/200][0/79]\tTime 0.124 (0.124)\tLoss 0.3816 (0.3816)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [59/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5369 (0.4582)\tTop 1-err 20.3125 (15.5331)\tTop 5-err 0.7812 (0.6434)\n",
            "* Epoch: [59/200]\tTop 1-err 15.220\tTop 5-err 0.600\tTest Loss 0.454\n",
            "\n",
            "Epoch 61/200\n",
            "Epoch: [60/200][0/390]\tTime 0.194 (0.194)\tData 0.128 (0.128)\tLoss 0.4282 (0.4282)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [60/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3764 (0.3524)\tTop 1-err 14.8438 (11.9792)\tTop 5-err 0.0000 (0.4902)\n",
            "Epoch: [60/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4183 (0.3672)\tTop 1-err 14.0625 (12.3530)\tTop 5-err 0.7812 (0.4332)\n",
            "Epoch: [60/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4564 (0.3795)\tTop 1-err 15.6250 (12.9915)\tTop 5-err 0.7812 (0.4294)\n",
            "Epoch: [60/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3444 (0.3826)\tTop 1-err 12.5000 (13.0597)\tTop 5-err 0.0000 (0.4159)\n",
            "Epoch: [60/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4602 (0.3819)\tTop 1-err 14.0625 (13.0011)\tTop 5-err 0.0000 (0.4171)\n",
            "Epoch: [60/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3284 (0.3840)\tTop 1-err 12.5000 (13.1515)\tTop 5-err 2.3438 (0.4360)\n",
            "Epoch: [60/200][350/390]\tTime 0.052 (0.051)\tData 0.000 (0.001)\tLoss 0.5481 (0.3900)\tTop 1-err 19.5312 (13.3480)\tTop 5-err 0.7812 (0.4630)\n",
            "Test (on val set): [60/200][0/79]\tTime 0.125 (0.125)\tLoss 0.5854 (0.5854)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [60/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8012 (0.7495)\tTop 1-err 19.5312 (21.7984)\tTop 5-err 0.7812 (2.0067)\n",
            "* Epoch: [60/200]\tTop 1-err 22.000\tTop 5-err 1.770\tTest Loss 0.744\n",
            "\n",
            "Epoch 62/200\n",
            "Epoch: [61/200][0/390]\tTime 0.193 (0.193)\tData 0.134 (0.134)\tLoss 0.3156 (0.3156)\tTop 1-err 7.0312 (7.0312)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [61/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2967 (0.3681)\tTop 1-err 10.1562 (12.9136)\tTop 5-err 0.7812 (0.4596)\n",
            "Epoch: [61/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3966 (0.3744)\tTop 1-err 14.0625 (13.0569)\tTop 5-err 0.7812 (0.4177)\n",
            "Epoch: [61/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4021 (0.3728)\tTop 1-err 11.7188 (12.9863)\tTop 5-err 2.3438 (0.4501)\n",
            "Epoch: [61/200][200/390]\tTime 0.053 (0.051)\tData 0.000 (0.001)\tLoss 0.4457 (0.3776)\tTop 1-err 16.4062 (13.1180)\tTop 5-err 1.5625 (0.4392)\n",
            "Epoch: [61/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4629 (0.3800)\tTop 1-err 15.6250 (13.1630)\tTop 5-err 0.0000 (0.4575)\n",
            "Epoch: [61/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5270 (0.3831)\tTop 1-err 19.5312 (13.2890)\tTop 5-err 0.7812 (0.4542)\n",
            "Epoch: [61/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3795 (0.3863)\tTop 1-err 10.9375 (13.3080)\tTop 5-err 3.1250 (0.4874)\n",
            "Test (on val set): [61/200][0/79]\tTime 0.132 (0.132)\tLoss 0.7048 (0.7048)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [61/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7328 (0.7711)\tTop 1-err 23.4375 (25.4136)\tTop 5-err 0.7812 (1.7157)\n",
            "* Epoch: [61/200]\tTop 1-err 25.710\tTop 5-err 1.710\tTest Loss 0.780\n",
            "\n",
            "Epoch 63/200\n",
            "Epoch: [62/200][0/390]\tTime 0.187 (0.187)\tData 0.129 (0.129)\tLoss 0.2900 (0.2900)\tTop 1-err 9.3750 (9.3750)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [62/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3466 (0.3562)\tTop 1-err 12.5000 (12.2243)\tTop 5-err 0.0000 (0.6127)\n",
            "Epoch: [62/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5822 (0.3730)\tTop 1-err 19.5312 (12.9796)\tTop 5-err 0.7812 (0.5183)\n",
            "Epoch: [62/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3762 (0.3763)\tTop 1-err 14.0625 (12.9243)\tTop 5-err 0.7812 (0.4656)\n",
            "Epoch: [62/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2739 (0.3813)\tTop 1-err 9.3750 (13.1141)\tTop 5-err 0.0000 (0.4314)\n",
            "Epoch: [62/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4068 (0.3850)\tTop 1-err 13.2812 (13.1816)\tTop 5-err 0.0000 (0.4482)\n",
            "Epoch: [62/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5898 (0.3857)\tTop 1-err 21.0938 (13.1722)\tTop 5-err 0.7812 (0.4464)\n",
            "Epoch: [62/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3819 (0.3888)\tTop 1-err 13.2812 (13.2657)\tTop 5-err 0.0000 (0.4541)\n",
            "Test (on val set): [62/200][0/79]\tTime 0.132 (0.132)\tLoss 0.5249 (0.5249)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [62/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5810 (0.6255)\tTop 1-err 17.1875 (19.5312)\tTop 5-err 0.7812 (1.4706)\n",
            "* Epoch: [62/200]\tTop 1-err 19.210\tTop 5-err 1.440\tTest Loss 0.618\n",
            "\n",
            "Epoch 64/200\n",
            "Epoch: [63/200][0/390]\tTime 0.193 (0.193)\tData 0.131 (0.131)\tLoss 0.2912 (0.2912)\tTop 1-err 8.5938 (8.5938)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [63/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3220 (0.3751)\tTop 1-err 9.3750 (13.1127)\tTop 5-err 0.0000 (0.4442)\n",
            "Epoch: [63/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3776 (0.3746)\tTop 1-err 14.0625 (12.9409)\tTop 5-err 0.7812 (0.5183)\n",
            "Epoch: [63/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3270 (0.3775)\tTop 1-err 12.5000 (13.0588)\tTop 5-err 0.0000 (0.4915)\n",
            "Epoch: [63/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4171 (0.3827)\tTop 1-err 13.2812 (13.2307)\tTop 5-err 0.0000 (0.4586)\n",
            "Epoch: [63/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2835 (0.3853)\tTop 1-err 8.5938 (13.2408)\tTop 5-err 0.7812 (0.4887)\n",
            "Epoch: [63/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2637 (0.3865)\tTop 1-err 10.1562 (13.2501)\tTop 5-err 0.0000 (0.4776)\n",
            "Epoch: [63/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3413 (0.3874)\tTop 1-err 10.9375 (13.2679)\tTop 5-err 0.0000 (0.4919)\n",
            "Test (on val set): [63/200][0/79]\tTime 0.124 (0.124)\tLoss 0.5741 (0.5741)\tTop 1-err 21.8750 (21.8750)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [63/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6236 (0.6206)\tTop 1-err 21.0938 (19.6232)\tTop 5-err 2.3438 (0.9191)\n",
            "* Epoch: [63/200]\tTop 1-err 19.250\tTop 5-err 0.810\tTest Loss 0.610\n",
            "\n",
            "Epoch 65/200\n",
            "Epoch: [64/200][0/390]\tTime 0.190 (0.190)\tData 0.131 (0.131)\tLoss 0.3081 (0.3081)\tTop 1-err 8.5938 (8.5938)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [64/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4934 (0.3557)\tTop 1-err 17.1875 (12.2855)\tTop 5-err 1.5625 (0.3983)\n",
            "Epoch: [64/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5298 (0.3660)\tTop 1-err 15.6250 (12.5851)\tTop 5-err 0.7812 (0.3713)\n",
            "Epoch: [64/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4770 (0.3845)\tTop 1-err 16.4062 (13.1726)\tTop 5-err 0.7812 (0.4708)\n",
            "Epoch: [64/200][200/390]\tTime 0.055 (0.051)\tData 0.000 (0.001)\tLoss 0.4555 (0.3845)\tTop 1-err 17.9688 (13.1297)\tTop 5-err 1.5625 (0.4742)\n",
            "Epoch: [64/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3095 (0.3828)\tTop 1-err 9.3750 (13.1007)\tTop 5-err 0.7812 (0.4513)\n",
            "Epoch: [64/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3711 (0.3845)\tTop 1-err 10.1562 (13.2086)\tTop 5-err 0.0000 (0.4438)\n",
            "Epoch: [64/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3752 (0.3881)\tTop 1-err 12.5000 (13.4126)\tTop 5-err 0.7812 (0.4452)\n",
            "Test (on val set): [64/200][0/79]\tTime 0.125 (0.125)\tLoss 0.4534 (0.4534)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [64/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4607 (0.4870)\tTop 1-err 13.2812 (16.0846)\tTop 5-err 0.7812 (0.7200)\n",
            "* Epoch: [64/200]\tTop 1-err 15.960\tTop 5-err 0.710\tTest Loss 0.491\n",
            "\n",
            "Epoch 66/200\n",
            "Epoch: [65/200][0/390]\tTime 0.200 (0.200)\tData 0.131 (0.131)\tLoss 0.3392 (0.3392)\tTop 1-err 10.1562 (10.1562)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [65/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3609 (0.3681)\tTop 1-err 11.7188 (12.9442)\tTop 5-err 0.7812 (0.3676)\n",
            "Epoch: [65/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3398 (0.3658)\tTop 1-err 13.2812 (12.5696)\tTop 5-err 0.0000 (0.4409)\n",
            "Epoch: [65/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2551 (0.3654)\tTop 1-err 9.3750 (12.7225)\tTop 5-err 0.0000 (0.4294)\n",
            "Epoch: [65/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3964 (0.3683)\tTop 1-err 15.6250 (12.7526)\tTop 5-err 0.7812 (0.4742)\n",
            "Epoch: [65/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4910 (0.3690)\tTop 1-err 17.1875 (12.7521)\tTop 5-err 0.0000 (0.4731)\n",
            "Epoch: [65/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2884 (0.3729)\tTop 1-err 11.7188 (12.8686)\tTop 5-err 0.0000 (0.5139)\n",
            "Epoch: [65/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3841 (0.3778)\tTop 1-err 14.0625 (13.0142)\tTop 5-err 0.7812 (0.5097)\n",
            "Test (on val set): [65/200][0/79]\tTime 0.132 (0.132)\tLoss 0.4705 (0.4705)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [65/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7062 (0.5685)\tTop 1-err 23.4375 (18.9032)\tTop 5-err 0.7812 (1.2255)\n",
            "* Epoch: [65/200]\tTop 1-err 19.110\tTop 5-err 1.180\tTest Loss 0.569\n",
            "\n",
            "Epoch 67/200\n",
            "Epoch: [66/200][0/390]\tTime 0.202 (0.202)\tData 0.130 (0.130)\tLoss 0.3730 (0.3730)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [66/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2374 (0.3640)\tTop 1-err 7.8125 (13.0055)\tTop 5-err 0.0000 (0.2757)\n",
            "Epoch: [66/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.2762 (0.3754)\tTop 1-err 11.7188 (13.2580)\tTop 5-err 0.0000 (0.3326)\n",
            "Epoch: [66/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3591 (0.3779)\tTop 1-err 13.2812 (13.2864)\tTop 5-err 0.0000 (0.3415)\n",
            "Epoch: [66/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3914 (0.3848)\tTop 1-err 13.2812 (13.4562)\tTop 5-err 0.7812 (0.3770)\n",
            "Epoch: [66/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4875 (0.3834)\tTop 1-err 12.5000 (13.3684)\tTop 5-err 0.7812 (0.3797)\n",
            "Epoch: [66/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2850 (0.3802)\tTop 1-err 10.9375 (13.1670)\tTop 5-err 0.0000 (0.3841)\n",
            "Epoch: [66/200][350/390]\tTime 0.050 (0.050)\tData 0.001 (0.001)\tLoss 0.4593 (0.3833)\tTop 1-err 12.5000 (13.2679)\tTop 5-err 2.3438 (0.3962)\n",
            "Test (on val set): [66/200][0/79]\tTime 0.130 (0.130)\tLoss 0.5113 (0.5113)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [66/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6670 (0.6698)\tTop 1-err 20.3125 (21.0018)\tTop 5-err 1.5625 (1.1489)\n",
            "* Epoch: [66/200]\tTop 1-err 20.940\tTop 5-err 1.100\tTest Loss 0.667\n",
            "\n",
            "Epoch 68/200\n",
            "Epoch: [67/200][0/390]\tTime 0.198 (0.198)\tData 0.142 (0.142)\tLoss 0.4944 (0.4944)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [67/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3562 (0.3584)\tTop 1-err 13.2812 (11.9179)\tTop 5-err 1.5625 (0.4902)\n",
            "Epoch: [67/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3801 (0.3617)\tTop 1-err 14.8438 (12.3530)\tTop 5-err 0.0000 (0.4332)\n",
            "Epoch: [67/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4507 (0.3743)\tTop 1-err 15.6250 (12.8156)\tTop 5-err 0.7812 (0.4708)\n",
            "Epoch: [67/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3742 (0.3785)\tTop 1-err 12.5000 (12.9586)\tTop 5-err 3.1250 (0.4703)\n",
            "Epoch: [67/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4141 (0.3783)\tTop 1-err 14.0625 (12.9731)\tTop 5-err 0.0000 (0.4793)\n",
            "Epoch: [67/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3627 (0.3799)\tTop 1-err 13.2812 (13.1333)\tTop 5-err 0.0000 (0.4724)\n",
            "Epoch: [67/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2404 (0.3804)\tTop 1-err 13.2812 (13.1410)\tTop 5-err 0.0000 (0.4763)\n",
            "Test (on val set): [67/200][0/79]\tTime 0.124 (0.124)\tLoss 0.4832 (0.4832)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [67/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5209 (0.5887)\tTop 1-err 18.7500 (18.5049)\tTop 5-err 0.7812 (1.0570)\n",
            "* Epoch: [67/200]\tTop 1-err 18.630\tTop 5-err 1.010\tTest Loss 0.586\n",
            "\n",
            "Epoch 69/200\n",
            "Epoch: [68/200][0/390]\tTime 0.189 (0.189)\tData 0.131 (0.131)\tLoss 0.3673 (0.3673)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [68/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3771 (0.3813)\tTop 1-err 13.2812 (13.5570)\tTop 5-err 0.7812 (0.3983)\n",
            "Epoch: [68/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3061 (0.3718)\tTop 1-err 12.5000 (13.1498)\tTop 5-err 0.0000 (0.3945)\n",
            "Epoch: [68/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4371 (0.3764)\tTop 1-err 13.2812 (13.2399)\tTop 5-err 1.5625 (0.3829)\n",
            "Epoch: [68/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5992 (0.3739)\tTop 1-err 20.3125 (13.1802)\tTop 5-err 1.5625 (0.3887)\n",
            "Epoch: [68/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5369 (0.3765)\tTop 1-err 17.9688 (13.2501)\tTop 5-err 2.3438 (0.4077)\n",
            "Epoch: [68/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4104 (0.3788)\tTop 1-err 14.8438 (13.3124)\tTop 5-err 0.0000 (0.4386)\n",
            "Epoch: [68/200][350/390]\tTime 0.051 (0.050)\tData 0.001 (0.001)\tLoss 0.3968 (0.3787)\tTop 1-err 16.4062 (13.2412)\tTop 5-err 0.0000 (0.4274)\n",
            "Test (on val set): [68/200][0/79]\tTime 0.127 (0.127)\tLoss 0.5724 (0.5724)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [68/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6316 (0.5613)\tTop 1-err 21.8750 (18.3058)\tTop 5-err 0.0000 (0.8885)\n",
            "* Epoch: [68/200]\tTop 1-err 18.520\tTop 5-err 0.880\tTest Loss 0.564\n",
            "\n",
            "Epoch 70/200\n",
            "Epoch: [69/200][0/390]\tTime 0.188 (0.188)\tData 0.128 (0.128)\tLoss 0.2195 (0.2195)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [69/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4353 (0.3553)\tTop 1-err 15.6250 (12.5613)\tTop 5-err 0.0000 (0.3217)\n",
            "Epoch: [69/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3485 (0.3732)\tTop 1-err 11.7188 (13.1730)\tTop 5-err 0.0000 (0.3945)\n",
            "Epoch: [69/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3313 (0.3784)\tTop 1-err 10.1562 (13.3175)\tTop 5-err 1.5625 (0.4139)\n",
            "Epoch: [69/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3167 (0.3805)\tTop 1-err 9.3750 (13.2968)\tTop 5-err 0.7812 (0.4159)\n",
            "Epoch: [69/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3531 (0.3802)\tTop 1-err 11.7188 (13.1972)\tTop 5-err 0.7812 (0.3922)\n",
            "Epoch: [69/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4451 (0.3837)\tTop 1-err 15.6250 (13.2605)\tTop 5-err 0.0000 (0.4127)\n",
            "Epoch: [69/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3074 (0.3869)\tTop 1-err 8.5938 (13.2946)\tTop 5-err 0.7812 (0.4385)\n",
            "Test (on val set): [69/200][0/79]\tTime 0.137 (0.137)\tLoss 0.4682 (0.4682)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [69/200][50/79]\tTime 0.016 (0.019)\tLoss 0.4285 (0.5613)\tTop 1-err 12.5000 (17.4173)\tTop 5-err 0.7812 (0.9038)\n",
            "* Epoch: [69/200]\tTop 1-err 17.560\tTop 5-err 0.830\tTest Loss 0.563\n",
            "\n",
            "Epoch 71/200\n",
            "Epoch: [70/200][0/390]\tTime 0.205 (0.205)\tData 0.131 (0.131)\tLoss 0.3733 (0.3733)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [70/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3151 (0.3641)\tTop 1-err 10.9375 (12.6532)\tTop 5-err 0.0000 (0.5974)\n",
            "Epoch: [70/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3484 (0.3659)\tTop 1-err 13.2812 (12.7862)\tTop 5-err 0.0000 (0.4796)\n",
            "Epoch: [70/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3352 (0.3676)\tTop 1-err 13.2812 (12.7328)\tTop 5-err 0.0000 (0.4553)\n",
            "Epoch: [70/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5302 (0.3738)\tTop 1-err 16.4062 (12.7837)\tTop 5-err 0.0000 (0.4548)\n",
            "Epoch: [70/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3811 (0.3729)\tTop 1-err 14.0625 (12.8081)\tTop 5-err 0.0000 (0.4700)\n",
            "Epoch: [70/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4460 (0.3721)\tTop 1-err 11.7188 (12.8167)\tTop 5-err 2.3438 (0.4957)\n",
            "Epoch: [70/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3777 (0.3729)\tTop 1-err 16.4062 (12.8762)\tTop 5-err 0.7812 (0.5008)\n",
            "Test (on val set): [70/200][0/79]\tTime 0.132 (0.132)\tLoss 0.8028 (0.8028)\tTop 1-err 24.2188 (24.2188)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [70/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7852 (0.8668)\tTop 1-err 25.7812 (25.3830)\tTop 5-err 0.0000 (1.8842)\n",
            "* Epoch: [70/200]\tTop 1-err 25.340\tTop 5-err 1.870\tTest Loss 0.866\n",
            "\n",
            "Epoch 72/200\n",
            "Epoch: [71/200][0/390]\tTime 0.191 (0.191)\tData 0.135 (0.135)\tLoss 0.4105 (0.4105)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [71/200][50/390]\tTime 0.052 (0.053)\tData 0.000 (0.003)\tLoss 0.4238 (0.3709)\tTop 1-err 14.8438 (12.8523)\tTop 5-err 0.0000 (0.4442)\n",
            "Epoch: [71/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.2912 (0.3640)\tTop 1-err 9.3750 (12.6470)\tTop 5-err 0.0000 (0.4486)\n",
            "Epoch: [71/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5326 (0.3702)\tTop 1-err 18.7500 (12.9191)\tTop 5-err 1.5625 (0.4553)\n",
            "Epoch: [71/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5316 (0.3736)\tTop 1-err 16.4062 (13.0752)\tTop 5-err 1.5625 (0.4548)\n",
            "Epoch: [71/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3452 (0.3772)\tTop 1-err 7.0312 (13.0136)\tTop 5-err 0.0000 (0.4482)\n",
            "Epoch: [71/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3671 (0.3787)\tTop 1-err 14.0625 (13.0451)\tTop 5-err 0.0000 (0.4438)\n",
            "Epoch: [71/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2988 (0.3772)\tTop 1-err 10.1562 (13.0186)\tTop 5-err 0.7812 (0.4496)\n",
            "Test (on val set): [71/200][0/79]\tTime 0.124 (0.124)\tLoss 0.4265 (0.4265)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [71/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6774 (0.5797)\tTop 1-err 22.6562 (19.6538)\tTop 5-err 1.5625 (0.7812)\n",
            "* Epoch: [71/200]\tTop 1-err 20.070\tTop 5-err 0.710\tTest Loss 0.583\n",
            "\n",
            "Epoch 73/200\n",
            "Epoch: [72/200][0/390]\tTime 0.189 (0.189)\tData 0.130 (0.130)\tLoss 0.3704 (0.3704)\tTop 1-err 14.0625 (14.0625)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [72/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3718 (0.3859)\tTop 1-err 14.0625 (13.3425)\tTop 5-err 2.3438 (0.5208)\n",
            "Epoch: [72/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.2814 (0.3723)\tTop 1-err 10.1562 (13.0492)\tTop 5-err 0.0000 (0.4409)\n",
            "Epoch: [72/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4133 (0.3723)\tTop 1-err 17.1875 (13.1467)\tTop 5-err 0.0000 (0.4708)\n",
            "Epoch: [72/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4081 (0.3762)\tTop 1-err 13.2812 (13.1919)\tTop 5-err 0.7812 (0.4664)\n",
            "Epoch: [72/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3476 (0.3757)\tTop 1-err 14.8438 (13.1350)\tTop 5-err 0.0000 (0.4856)\n",
            "Epoch: [72/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3661 (0.3801)\tTop 1-err 11.7188 (13.1904)\tTop 5-err 0.0000 (0.4802)\n",
            "Epoch: [72/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4337 (0.3809)\tTop 1-err 16.4062 (13.1833)\tTop 5-err 0.0000 (0.4652)\n",
            "Test (on val set): [72/200][0/79]\tTime 0.127 (0.127)\tLoss 0.5324 (0.5324)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [72/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7550 (0.6943)\tTop 1-err 17.9688 (21.8903)\tTop 5-err 2.3438 (1.0723)\n",
            "* Epoch: [72/200]\tTop 1-err 21.700\tTop 5-err 1.130\tTest Loss 0.688\n",
            "\n",
            "Epoch 74/200\n",
            "Epoch: [73/200][0/390]\tTime 0.189 (0.189)\tData 0.131 (0.131)\tLoss 0.4555 (0.4555)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [73/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3483 (0.3535)\tTop 1-err 12.5000 (12.0711)\tTop 5-err 0.0000 (0.3217)\n",
            "Epoch: [73/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3376 (0.3761)\tTop 1-err 9.3750 (12.9332)\tTop 5-err 0.0000 (0.3868)\n",
            "Epoch: [73/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3887 (0.3739)\tTop 1-err 14.0625 (12.7897)\tTop 5-err 0.0000 (0.3932)\n",
            "Epoch: [73/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3449 (0.3705)\tTop 1-err 11.7188 (12.7565)\tTop 5-err 0.0000 (0.4120)\n",
            "Epoch: [73/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2567 (0.3782)\tTop 1-err 9.3750 (12.9669)\tTop 5-err 0.7812 (0.4420)\n",
            "Epoch: [73/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3883 (0.3787)\tTop 1-err 13.2812 (12.9179)\tTop 5-err 0.0000 (0.4412)\n",
            "Epoch: [73/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3658 (0.3820)\tTop 1-err 12.5000 (13.0008)\tTop 5-err 0.0000 (0.4518)\n",
            "Test (on val set): [73/200][0/79]\tTime 0.157 (0.157)\tLoss 0.8047 (0.8047)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [73/200][50/79]\tTime 0.016 (0.019)\tLoss 0.8173 (0.7528)\tTop 1-err 25.0000 (23.2077)\tTop 5-err 1.5625 (1.4246)\n",
            "* Epoch: [73/200]\tTop 1-err 23.130\tTop 5-err 1.450\tTest Loss 0.755\n",
            "\n",
            "Epoch 75/200\n",
            "Epoch: [74/200][0/390]\tTime 0.203 (0.203)\tData 0.131 (0.131)\tLoss 0.4412 (0.4412)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [74/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3683 (0.3459)\tTop 1-err 12.5000 (11.4890)\tTop 5-err 0.0000 (0.2604)\n",
            "Epoch: [74/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.5666 (0.3597)\tTop 1-err 17.1875 (12.2834)\tTop 5-err 1.5625 (0.3403)\n",
            "Epoch: [74/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4683 (0.3641)\tTop 1-err 16.4062 (12.4120)\tTop 5-err 0.0000 (0.3880)\n",
            "Epoch: [74/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3219 (0.3677)\tTop 1-err 10.9375 (12.5544)\tTop 5-err 0.7812 (0.4275)\n",
            "Epoch: [74/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3401 (0.3692)\tTop 1-err 9.3750 (12.6556)\tTop 5-err 0.0000 (0.4358)\n",
            "Epoch: [74/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4002 (0.3736)\tTop 1-err 15.6250 (12.8011)\tTop 5-err 0.7812 (0.4386)\n",
            "Epoch: [74/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3354 (0.3757)\tTop 1-err 11.7188 (12.9006)\tTop 5-err 0.7812 (0.4318)\n",
            "Test (on val set): [74/200][0/79]\tTime 0.126 (0.126)\tLoss 0.4374 (0.4374)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [74/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5629 (0.4878)\tTop 1-err 18.7500 (16.3756)\tTop 5-err 0.0000 (0.8425)\n",
            "* Epoch: [74/200]\tTop 1-err 16.180\tTop 5-err 0.740\tTest Loss 0.478\n",
            "\n",
            "Epoch 76/200\n",
            "Epoch: [75/200][0/390]\tTime 0.191 (0.191)\tData 0.132 (0.132)\tLoss 0.3192 (0.3192)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [75/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4566 (0.3507)\tTop 1-err 17.1875 (12.1017)\tTop 5-err 1.5625 (0.3370)\n",
            "Epoch: [75/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.2915 (0.3728)\tTop 1-err 10.9375 (12.6624)\tTop 5-err 0.0000 (0.5028)\n",
            "Epoch: [75/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.6198 (0.3721)\tTop 1-err 25.7812 (12.5621)\tTop 5-err 1.5625 (0.4863)\n",
            "Epoch: [75/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4851 (0.3699)\tTop 1-err 16.4062 (12.6127)\tTop 5-err 1.5625 (0.4392)\n",
            "Epoch: [75/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4259 (0.3703)\tTop 1-err 14.0625 (12.6058)\tTop 5-err 1.5625 (0.4420)\n",
            "Epoch: [75/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3627 (0.3743)\tTop 1-err 15.6250 (12.7414)\tTop 5-err 0.0000 (0.4646)\n",
            "Epoch: [75/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3089 (0.3743)\tTop 1-err 8.5938 (12.7204)\tTop 5-err 0.0000 (0.4452)\n",
            "Test (on val set): [75/200][0/79]\tTime 0.124 (0.124)\tLoss 0.6358 (0.6358)\tTop 1-err 24.2188 (24.2188)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [75/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6073 (0.6876)\tTop 1-err 20.3125 (21.9822)\tTop 5-err 0.7812 (1.7004)\n",
            "* Epoch: [75/200]\tTop 1-err 22.020\tTop 5-err 1.630\tTest Loss 0.691\n",
            "\n",
            "Epoch 77/200\n",
            "Epoch: [76/200][0/390]\tTime 0.202 (0.202)\tData 0.142 (0.142)\tLoss 0.4166 (0.4166)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [76/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3353 (0.3485)\tTop 1-err 13.2812 (12.3315)\tTop 5-err 0.7812 (0.3217)\n",
            "Epoch: [76/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3567 (0.3619)\tTop 1-err 10.9375 (12.5619)\tTop 5-err 0.0000 (0.3713)\n",
            "Epoch: [76/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3228 (0.3690)\tTop 1-err 10.1562 (12.7535)\tTop 5-err 0.7812 (0.3829)\n",
            "Epoch: [76/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2861 (0.3666)\tTop 1-err 10.1562 (12.7254)\tTop 5-err 0.0000 (0.3848)\n",
            "Epoch: [76/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5438 (0.3693)\tTop 1-err 18.7500 (12.8019)\tTop 5-err 0.7812 (0.4077)\n",
            "Epoch: [76/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3321 (0.3676)\tTop 1-err 10.9375 (12.7777)\tTop 5-err 0.0000 (0.3997)\n",
            "Epoch: [76/200][350/390]\tTime 0.056 (0.051)\tData 0.000 (0.001)\tLoss 0.3496 (0.3695)\tTop 1-err 13.2812 (12.8161)\tTop 5-err 0.0000 (0.4140)\n",
            "Test (on val set): [76/200][0/79]\tTime 0.124 (0.124)\tLoss 0.8305 (0.8305)\tTop 1-err 23.4375 (23.4375)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [76/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8646 (0.7831)\tTop 1-err 25.0000 (23.9583)\tTop 5-err 0.0000 (1.3634)\n",
            "* Epoch: [76/200]\tTop 1-err 23.770\tTop 5-err 1.240\tTest Loss 0.777\n",
            "\n",
            "Epoch 78/200\n",
            "Epoch: [77/200][0/390]\tTime 0.203 (0.203)\tData 0.136 (0.136)\tLoss 0.3637 (0.3637)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [77/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3774 (0.3559)\tTop 1-err 10.9375 (12.0558)\tTop 5-err 0.0000 (0.3370)\n",
            "Epoch: [77/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4069 (0.3676)\tTop 1-err 14.8438 (12.5851)\tTop 5-err 0.0000 (0.4254)\n",
            "Epoch: [77/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3865 (0.3693)\tTop 1-err 14.0625 (12.6966)\tTop 5-err 0.7812 (0.4553)\n",
            "Epoch: [77/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4209 (0.3709)\tTop 1-err 14.0625 (12.7565)\tTop 5-err 1.5625 (0.4198)\n",
            "Epoch: [77/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3699 (0.3751)\tTop 1-err 11.7188 (12.8860)\tTop 5-err 0.7812 (0.4420)\n",
            "Epoch: [77/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3606 (0.3745)\tTop 1-err 14.0625 (12.8789)\tTop 5-err 0.7812 (0.4594)\n",
            "Epoch: [77/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3838 (0.3744)\tTop 1-err 12.5000 (12.8762)\tTop 5-err 1.5625 (0.4607)\n",
            "Test (on val set): [77/200][0/79]\tTime 0.131 (0.131)\tLoss 0.8349 (0.8349)\tTop 1-err 24.2188 (24.2188)\tTop 5-err 3.1250 (3.1250)\n",
            "Test (on val set): [77/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7035 (0.8614)\tTop 1-err 21.8750 (26.7157)\tTop 5-err 0.7812 (0.9651)\n",
            "* Epoch: [77/200]\tTop 1-err 26.360\tTop 5-err 0.970\tTest Loss 0.848\n",
            "\n",
            "Epoch 79/200\n",
            "Epoch: [78/200][0/390]\tTime 0.212 (0.212)\tData 0.142 (0.142)\tLoss 0.4819 (0.4819)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [78/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2249 (0.3584)\tTop 1-err 8.5938 (12.2089)\tTop 5-err 0.0000 (0.5055)\n",
            "Epoch: [78/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.2594 (0.3527)\tTop 1-err 8.5938 (12.2757)\tTop 5-err 0.7812 (0.4486)\n",
            "Epoch: [78/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4705 (0.3638)\tTop 1-err 16.4062 (12.6449)\tTop 5-err 0.0000 (0.4605)\n",
            "Epoch: [78/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4331 (0.3664)\tTop 1-err 16.4062 (12.7177)\tTop 5-err 0.0000 (0.4586)\n",
            "Epoch: [78/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4255 (0.3680)\tTop 1-err 12.5000 (12.7459)\tTop 5-err 0.0000 (0.4856)\n",
            "Epoch: [78/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2185 (0.3668)\tTop 1-err 7.8125 (12.6739)\tTop 5-err 0.0000 (0.4672)\n",
            "Epoch: [78/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2928 (0.3694)\tTop 1-err 10.1562 (12.7382)\tTop 5-err 0.0000 (0.4696)\n",
            "Test (on val set): [78/200][0/79]\tTime 0.128 (0.128)\tLoss 0.3830 (0.3830)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [78/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4755 (0.4479)\tTop 1-err 16.4062 (15.6403)\tTop 5-err 0.7812 (0.6434)\n",
            "* Epoch: [78/200]\tTop 1-err 15.400\tTop 5-err 0.550\tTest Loss 0.442\n",
            "\n",
            "Epoch 80/200\n",
            "Epoch: [79/200][0/390]\tTime 0.204 (0.204)\tData 0.132 (0.132)\tLoss 0.4228 (0.4228)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [79/200][50/390]\tTime 0.050 (0.053)\tData 0.001 (0.003)\tLoss 0.2830 (0.3629)\tTop 1-err 9.3750 (12.6072)\tTop 5-err 0.7812 (0.4596)\n",
            "Epoch: [79/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.2732 (0.3591)\tTop 1-err 8.5938 (12.2989)\tTop 5-err 1.5625 (0.4254)\n",
            "Epoch: [79/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4445 (0.3588)\tTop 1-err 14.8438 (12.4017)\tTop 5-err 0.0000 (0.3570)\n",
            "Epoch: [79/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3058 (0.3642)\tTop 1-err 14.8438 (12.5428)\tTop 5-err 0.7812 (0.3459)\n",
            "Epoch: [79/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3795 (0.3720)\tTop 1-err 14.0625 (12.8237)\tTop 5-err 0.0000 (0.3984)\n",
            "Epoch: [79/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4374 (0.3746)\tTop 1-err 14.8438 (12.8971)\tTop 5-err 0.0000 (0.4205)\n",
            "Epoch: [79/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4200 (0.3778)\tTop 1-err 14.0625 (13.0698)\tTop 5-err 0.0000 (0.4363)\n",
            "Test (on val set): [79/200][0/79]\tTime 0.130 (0.130)\tLoss 0.4507 (0.4507)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [79/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5205 (0.5043)\tTop 1-err 19.5312 (17.0650)\tTop 5-err 0.7812 (0.8272)\n",
            "* Epoch: [79/200]\tTop 1-err 17.160\tTop 5-err 0.810\tTest Loss 0.506\n",
            "\n",
            "Epoch 81/200\n",
            "Epoch: [80/200][0/390]\tTime 0.208 (0.208)\tData 0.136 (0.136)\tLoss 0.4447 (0.4447)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [80/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3167 (0.3551)\tTop 1-err 11.7188 (12.4387)\tTop 5-err 0.0000 (0.3830)\n",
            "Epoch: [80/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4177 (0.3671)\tTop 1-err 14.0625 (12.7321)\tTop 5-err 0.7812 (0.4332)\n",
            "Epoch: [80/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3013 (0.3713)\tTop 1-err 8.5938 (12.7794)\tTop 5-err 0.7812 (0.4450)\n",
            "Epoch: [80/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3122 (0.3798)\tTop 1-err 14.8438 (13.0986)\tTop 5-err 0.0000 (0.4353)\n",
            "Epoch: [80/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3865 (0.3765)\tTop 1-err 14.8438 (13.0136)\tTop 5-err 0.0000 (0.4233)\n",
            "Epoch: [80/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4024 (0.3785)\tTop 1-err 15.6250 (13.1437)\tTop 5-err 0.7812 (0.4516)\n",
            "Epoch: [80/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3097 (0.3769)\tTop 1-err 11.7188 (13.0631)\tTop 5-err 0.0000 (0.4674)\n",
            "Test (on val set): [80/200][0/79]\tTime 0.131 (0.131)\tLoss 0.8774 (0.8774)\tTop 1-err 28.1250 (28.1250)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [80/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7583 (0.8276)\tTop 1-err 23.4375 (24.2800)\tTop 5-err 1.5625 (1.7616)\n",
            "* Epoch: [80/200]\tTop 1-err 24.250\tTop 5-err 1.700\tTest Loss 0.822\n",
            "\n",
            "Epoch 82/200\n",
            "Epoch: [81/200][0/390]\tTime 0.215 (0.215)\tData 0.144 (0.144)\tLoss 0.3601 (0.3601)\tTop 1-err 10.9375 (10.9375)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [81/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3916 (0.3634)\tTop 1-err 14.8438 (12.4540)\tTop 5-err 0.0000 (0.4289)\n",
            "Epoch: [81/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.4070 (0.3667)\tTop 1-err 12.5000 (12.4768)\tTop 5-err 0.7812 (0.5028)\n",
            "Epoch: [81/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3315 (0.3655)\tTop 1-err 10.9375 (12.5983)\tTop 5-err 0.0000 (0.4760)\n",
            "Epoch: [81/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5638 (0.3668)\tTop 1-err 20.3125 (12.6555)\tTop 5-err 0.7812 (0.4586)\n",
            "Epoch: [81/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4366 (0.3685)\tTop 1-err 17.1875 (12.6743)\tTop 5-err 0.7812 (0.4544)\n",
            "Epoch: [81/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4266 (0.3713)\tTop 1-err 14.8438 (12.7492)\tTop 5-err 0.7812 (0.4594)\n",
            "Epoch: [81/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4546 (0.3739)\tTop 1-err 14.0625 (12.8606)\tTop 5-err 0.7812 (0.4518)\n",
            "Test (on val set): [81/200][0/79]\tTime 0.131 (0.131)\tLoss 0.4764 (0.4764)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [81/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5776 (0.5018)\tTop 1-err 17.9688 (16.6973)\tTop 5-err 0.7812 (0.7200)\n",
            "* Epoch: [81/200]\tTop 1-err 16.660\tTop 5-err 0.690\tTest Loss 0.502\n",
            "\n",
            "Epoch 83/200\n",
            "Epoch: [82/200][0/390]\tTime 0.201 (0.201)\tData 0.139 (0.139)\tLoss 0.3628 (0.3628)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [82/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2988 (0.3405)\tTop 1-err 10.1562 (11.6728)\tTop 5-err 0.0000 (0.3370)\n",
            "Epoch: [82/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.2864 (0.3537)\tTop 1-err 10.9375 (12.1674)\tTop 5-err 0.0000 (0.3558)\n",
            "Epoch: [82/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3436 (0.3625)\tTop 1-err 14.8438 (12.4897)\tTop 5-err 0.0000 (0.3673)\n",
            "Epoch: [82/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2512 (0.3597)\tTop 1-err 8.5938 (12.3018)\tTop 5-err 0.0000 (0.3420)\n",
            "Epoch: [82/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4548 (0.3651)\tTop 1-err 21.8750 (12.4595)\tTop 5-err 0.0000 (0.3704)\n",
            "Epoch: [82/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3854 (0.3664)\tTop 1-err 11.7188 (12.5208)\tTop 5-err 0.7812 (0.4127)\n",
            "Epoch: [82/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3616 (0.3686)\tTop 1-err 14.0625 (12.5868)\tTop 5-err 0.0000 (0.4229)\n",
            "Test (on val set): [82/200][0/79]\tTime 0.140 (0.140)\tLoss 0.5651 (0.5651)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [82/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7475 (0.6238)\tTop 1-err 22.6562 (19.1023)\tTop 5-err 0.7812 (1.0570)\n",
            "* Epoch: [82/200]\tTop 1-err 19.630\tTop 5-err 1.010\tTest Loss 0.634\n",
            "\n",
            "Epoch 84/200\n",
            "Epoch: [83/200][0/390]\tTime 0.198 (0.198)\tData 0.129 (0.129)\tLoss 0.3768 (0.3768)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [83/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3360 (0.3483)\tTop 1-err 14.0625 (12.2855)\tTop 5-err 0.0000 (0.3830)\n",
            "Epoch: [83/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.6075 (0.3534)\tTop 1-err 18.7500 (12.1829)\tTop 5-err 0.0000 (0.4409)\n",
            "Epoch: [83/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2523 (0.3566)\tTop 1-err 10.1562 (12.2879)\tTop 5-err 0.0000 (0.3984)\n",
            "Epoch: [83/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3555 (0.3566)\tTop 1-err 13.2812 (12.2668)\tTop 5-err 0.7812 (0.4003)\n",
            "Epoch: [83/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3123 (0.3641)\tTop 1-err 10.1562 (12.5529)\tTop 5-err 0.7812 (0.4171)\n",
            "Epoch: [83/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3283 (0.3672)\tTop 1-err 9.3750 (12.5831)\tTop 5-err 1.5625 (0.4386)\n",
            "Epoch: [83/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3619 (0.3683)\tTop 1-err 12.5000 (12.6603)\tTop 5-err 0.7812 (0.4296)\n",
            "Test (on val set): [83/200][0/79]\tTime 0.135 (0.135)\tLoss 0.6194 (0.6194)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [83/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6965 (0.5844)\tTop 1-err 21.0938 (19.1023)\tTop 5-err 0.7812 (1.0570)\n",
            "* Epoch: [83/200]\tTop 1-err 19.240\tTop 5-err 1.050\tTest Loss 0.586\n",
            "\n",
            "Epoch 85/200\n",
            "Epoch: [84/200][0/390]\tTime 0.194 (0.194)\tData 0.135 (0.135)\tLoss 0.3130 (0.3130)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [84/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3397 (0.3499)\tTop 1-err 10.1562 (11.9332)\tTop 5-err 0.0000 (0.4596)\n",
            "Epoch: [84/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.2613 (0.3595)\tTop 1-err 6.2500 (12.5309)\tTop 5-err 0.7812 (0.4332)\n",
            "Epoch: [84/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4458 (0.3636)\tTop 1-err 16.4062 (12.6707)\tTop 5-err 0.7812 (0.4294)\n",
            "Epoch: [84/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2341 (0.3654)\tTop 1-err 7.0312 (12.7293)\tTop 5-err 0.0000 (0.4198)\n",
            "Epoch: [84/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4772 (0.3678)\tTop 1-err 18.7500 (12.6930)\tTop 5-err 0.7812 (0.4015)\n",
            "Epoch: [84/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3138 (0.3635)\tTop 1-err 11.7188 (12.5286)\tTop 5-err 0.7812 (0.3971)\n",
            "Epoch: [84/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3512 (0.3690)\tTop 1-err 11.7188 (12.6736)\tTop 5-err 0.7812 (0.4274)\n",
            "Test (on val set): [84/200][0/79]\tTime 0.133 (0.133)\tLoss 0.6100 (0.6100)\tTop 1-err 17.9688 (17.9688)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [84/200][50/79]\tTime 0.016 (0.019)\tLoss 0.6812 (0.6287)\tTop 1-err 21.0938 (19.9295)\tTop 5-err 0.0000 (0.8732)\n",
            "* Epoch: [84/200]\tTop 1-err 19.780\tTop 5-err 0.720\tTest Loss 0.621\n",
            "\n",
            "Epoch 86/200\n",
            "Epoch: [85/200][0/390]\tTime 0.205 (0.205)\tData 0.136 (0.136)\tLoss 0.2923 (0.2923)\tTop 1-err 7.8125 (7.8125)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [85/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3520 (0.3584)\tTop 1-err 10.9375 (12.3162)\tTop 5-err 0.0000 (0.4136)\n",
            "Epoch: [85/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3745 (0.3700)\tTop 1-err 13.2812 (12.8790)\tTop 5-err 0.0000 (0.4718)\n",
            "Epoch: [85/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3447 (0.3728)\tTop 1-err 9.3750 (13.0588)\tTop 5-err 0.0000 (0.4450)\n",
            "Epoch: [85/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2350 (0.3729)\tTop 1-err 7.0312 (12.9975)\tTop 5-err 0.0000 (0.4509)\n",
            "Epoch: [85/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2722 (0.3696)\tTop 1-err 7.8125 (12.8891)\tTop 5-err 0.0000 (0.4358)\n",
            "Epoch: [85/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3619 (0.3684)\tTop 1-err 11.7188 (12.8218)\tTop 5-err 0.0000 (0.4360)\n",
            "Epoch: [85/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3999 (0.3699)\tTop 1-err 15.6250 (12.8405)\tTop 5-err 1.5625 (0.4340)\n",
            "Test (on val set): [85/200][0/79]\tTime 0.131 (0.131)\tLoss 0.7781 (0.7781)\tTop 1-err 27.3438 (27.3438)\tTop 5-err 2.3438 (2.3438)\n",
            "Test (on val set): [85/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6849 (0.7861)\tTop 1-err 22.6562 (23.9277)\tTop 5-err 0.0000 (2.3284)\n",
            "* Epoch: [85/200]\tTop 1-err 23.770\tTop 5-err 2.190\tTest Loss 0.782\n",
            "\n",
            "Epoch 87/200\n",
            "Epoch: [86/200][0/390]\tTime 0.189 (0.189)\tData 0.129 (0.129)\tLoss 0.2922 (0.2922)\tTop 1-err 9.3750 (9.3750)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [86/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2685 (0.3626)\tTop 1-err 10.9375 (12.5460)\tTop 5-err 0.0000 (0.4136)\n",
            "Epoch: [86/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.2923 (0.3588)\tTop 1-err 10.1562 (12.2679)\tTop 5-err 0.0000 (0.4177)\n",
            "Epoch: [86/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2625 (0.3634)\tTop 1-err 10.9375 (12.4276)\tTop 5-err 0.0000 (0.4191)\n",
            "Epoch: [86/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5274 (0.3636)\tTop 1-err 17.1875 (12.5272)\tTop 5-err 1.5625 (0.4120)\n",
            "Epoch: [86/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3983 (0.3618)\tTop 1-err 13.2812 (12.3879)\tTop 5-err 0.0000 (0.4233)\n",
            "Epoch: [86/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3453 (0.3637)\tTop 1-err 10.9375 (12.4092)\tTop 5-err 0.0000 (0.3997)\n",
            "Epoch: [86/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3333 (0.3655)\tTop 1-err 10.9375 (12.5156)\tTop 5-err 0.0000 (0.4229)\n",
            "Test (on val set): [86/200][0/79]\tTime 0.132 (0.132)\tLoss 0.4099 (0.4099)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [86/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5791 (0.6250)\tTop 1-err 21.0938 (19.8070)\tTop 5-err 0.0000 (1.2255)\n",
            "* Epoch: [86/200]\tTop 1-err 19.840\tTop 5-err 1.190\tTest Loss 0.617\n",
            "\n",
            "Epoch 88/200\n",
            "Epoch: [87/200][0/390]\tTime 0.202 (0.202)\tData 0.131 (0.131)\tLoss 0.4946 (0.4946)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [87/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4308 (0.3522)\tTop 1-err 13.2812 (12.4540)\tTop 5-err 0.7812 (0.3217)\n",
            "Epoch: [87/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4191 (0.3534)\tTop 1-err 14.0625 (12.2447)\tTop 5-err 0.7812 (0.3403)\n",
            "Epoch: [87/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3467 (0.3600)\tTop 1-err 9.3750 (12.5103)\tTop 5-err 0.7812 (0.4191)\n",
            "Epoch: [87/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3856 (0.3680)\tTop 1-err 15.6250 (12.8071)\tTop 5-err 0.0000 (0.4275)\n",
            "Epoch: [87/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4824 (0.3724)\tTop 1-err 15.6250 (12.8393)\tTop 5-err 0.0000 (0.4544)\n",
            "Epoch: [87/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3364 (0.3747)\tTop 1-err 12.5000 (12.9179)\tTop 5-err 0.0000 (0.4672)\n",
            "Epoch: [87/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2729 (0.3759)\tTop 1-err 10.1562 (12.9763)\tTop 5-err 0.0000 (0.4541)\n",
            "Test (on val set): [87/200][0/79]\tTime 0.134 (0.134)\tLoss 0.5066 (0.5066)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [87/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4984 (0.4947)\tTop 1-err 15.6250 (16.3603)\tTop 5-err 0.7812 (0.8425)\n",
            "* Epoch: [87/200]\tTop 1-err 16.380\tTop 5-err 0.710\tTest Loss 0.489\n",
            "\n",
            "Epoch 89/200\n",
            "Epoch: [88/200][0/390]\tTime 0.216 (0.216)\tData 0.143 (0.143)\tLoss 0.4442 (0.4442)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [88/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2982 (0.3495)\tTop 1-err 7.8125 (11.9485)\tTop 5-err 0.7812 (0.4442)\n",
            "Epoch: [88/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.2847 (0.3635)\tTop 1-err 7.0312 (12.3221)\tTop 5-err 0.7812 (0.4641)\n",
            "Epoch: [88/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3853 (0.3668)\tTop 1-err 12.5000 (12.5155)\tTop 5-err 1.5625 (0.4501)\n",
            "Epoch: [88/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3732 (0.3651)\tTop 1-err 15.6250 (12.5738)\tTop 5-err 0.0000 (0.4237)\n",
            "Epoch: [88/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3535 (0.3653)\tTop 1-err 11.7188 (12.5156)\tTop 5-err 0.0000 (0.4109)\n",
            "Epoch: [88/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4555 (0.3677)\tTop 1-err 14.0625 (12.5779)\tTop 5-err 0.7812 (0.4309)\n",
            "Epoch: [88/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5147 (0.3687)\tTop 1-err 16.4062 (12.6024)\tTop 5-err 0.0000 (0.4429)\n",
            "Test (on val set): [88/200][0/79]\tTime 0.136 (0.136)\tLoss 0.3965 (0.3965)\tTop 1-err 13.2812 (13.2812)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [88/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5311 (0.4593)\tTop 1-err 17.9688 (15.1961)\tTop 5-err 0.0000 (0.6587)\n",
            "* Epoch: [88/200]\tTop 1-err 15.240\tTop 5-err 0.600\tTest Loss 0.458\n",
            "\n",
            "Epoch 90/200\n",
            "Epoch: [89/200][0/390]\tTime 0.202 (0.202)\tData 0.132 (0.132)\tLoss 0.3497 (0.3497)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [89/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4655 (0.3519)\tTop 1-err 11.7188 (12.3468)\tTop 5-err 0.0000 (0.3064)\n",
            "Epoch: [89/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3994 (0.3460)\tTop 1-err 13.2812 (11.8580)\tTop 5-err 0.7812 (0.3249)\n",
            "Epoch: [89/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4421 (0.3542)\tTop 1-err 16.4062 (12.1120)\tTop 5-err 0.7812 (0.3880)\n",
            "Epoch: [89/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3900 (0.3572)\tTop 1-err 12.5000 (12.2007)\tTop 5-err 0.7812 (0.4237)\n",
            "Epoch: [89/200][250/390]\tTime 0.055 (0.051)\tData 0.000 (0.001)\tLoss 0.3808 (0.3556)\tTop 1-err 15.6250 (12.2074)\tTop 5-err 0.0000 (0.4171)\n",
            "Epoch: [89/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3786 (0.3536)\tTop 1-err 16.4062 (12.1081)\tTop 5-err 0.0000 (0.4049)\n",
            "Epoch: [89/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3804 (0.3570)\tTop 1-err 12.5000 (12.2062)\tTop 5-err 1.5625 (0.4207)\n",
            "Test (on val set): [89/200][0/79]\tTime 0.134 (0.134)\tLoss 0.5881 (0.5881)\tTop 1-err 19.5312 (19.5312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [89/200][50/79]\tTime 0.016 (0.019)\tLoss 0.7740 (0.5827)\tTop 1-err 23.4375 (19.0564)\tTop 5-err 0.7812 (0.6740)\n",
            "* Epoch: [89/200]\tTop 1-err 19.000\tTop 5-err 0.720\tTest Loss 0.578\n",
            "\n",
            "Epoch 91/200\n",
            "Epoch: [90/200][0/390]\tTime 0.192 (0.192)\tData 0.132 (0.132)\tLoss 0.3711 (0.3711)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [90/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2463 (0.3412)\tTop 1-err 7.8125 (11.4277)\tTop 5-err 0.0000 (0.3676)\n",
            "Epoch: [90/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.2434 (0.3630)\tTop 1-err 8.5938 (12.2061)\tTop 5-err 0.0000 (0.5183)\n",
            "Epoch: [90/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2893 (0.3600)\tTop 1-err 11.7188 (12.3293)\tTop 5-err 0.0000 (0.4398)\n",
            "Epoch: [90/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2942 (0.3583)\tTop 1-err 9.3750 (12.2007)\tTop 5-err 0.0000 (0.4042)\n",
            "Epoch: [90/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4653 (0.3645)\tTop 1-err 14.0625 (12.4813)\tTop 5-err 0.7812 (0.4295)\n",
            "Epoch: [90/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2212 (0.3650)\tTop 1-err 10.1562 (12.5286)\tTop 5-err 0.7812 (0.4179)\n",
            "Epoch: [90/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3630 (0.3618)\tTop 1-err 14.8438 (12.3843)\tTop 5-err 0.0000 (0.4363)\n",
            "Test (on val set): [90/200][0/79]\tTime 0.126 (0.126)\tLoss 0.5228 (0.5228)\tTop 1-err 16.4062 (16.4062)\tTop 5-err 0.7812 (0.7812)\n",
            "Test (on val set): [90/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6531 (0.5726)\tTop 1-err 21.0938 (18.0147)\tTop 5-err 1.5625 (1.1489)\n",
            "* Epoch: [90/200]\tTop 1-err 18.130\tTop 5-err 0.980\tTest Loss 0.572\n",
            "\n",
            "Epoch 92/200\n",
            "Epoch: [91/200][0/390]\tTime 0.207 (0.207)\tData 0.136 (0.136)\tLoss 0.3856 (0.3856)\tTop 1-err 10.9375 (10.9375)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [91/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2963 (0.3206)\tTop 1-err 10.9375 (10.8762)\tTop 5-err 0.0000 (0.4289)\n",
            "Epoch: [91/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3498 (0.3371)\tTop 1-err 11.7188 (11.5950)\tTop 5-err 0.0000 (0.3636)\n",
            "Epoch: [91/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3292 (0.3534)\tTop 1-err 12.5000 (12.1120)\tTop 5-err 1.5625 (0.3829)\n",
            "Epoch: [91/200][200/390]\tTime 0.051 (0.051)\tData 0.000 (0.001)\tLoss 0.4894 (0.3544)\tTop 1-err 15.6250 (12.0841)\tTop 5-err 0.0000 (0.4003)\n",
            "Epoch: [91/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3255 (0.3635)\tTop 1-err 9.3750 (12.4160)\tTop 5-err 0.0000 (0.4607)\n",
            "Epoch: [91/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4630 (0.3635)\tTop 1-err 14.8438 (12.4481)\tTop 5-err 0.0000 (0.4438)\n",
            "Epoch: [91/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2965 (0.3628)\tTop 1-err 9.3750 (12.4421)\tTop 5-err 0.0000 (0.4274)\n",
            "Test (on val set): [91/200][0/79]\tTime 0.131 (0.131)\tLoss 0.4714 (0.4714)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [91/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4242 (0.4597)\tTop 1-err 10.9375 (14.9357)\tTop 5-err 0.7812 (0.7659)\n",
            "* Epoch: [91/200]\tTop 1-err 15.140\tTop 5-err 0.720\tTest Loss 0.456\n",
            "\n",
            "Epoch 93/200\n",
            "Epoch: [92/200][0/390]\tTime 0.202 (0.202)\tData 0.132 (0.132)\tLoss 0.3089 (0.3089)\tTop 1-err 10.1562 (10.1562)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [92/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2753 (0.3735)\tTop 1-err 10.1562 (12.7757)\tTop 5-err 0.0000 (0.4442)\n",
            "Epoch: [92/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4483 (0.3646)\tTop 1-err 17.1875 (12.4923)\tTop 5-err 0.7812 (0.3481)\n",
            "Epoch: [92/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2845 (0.3573)\tTop 1-err 8.5938 (12.2258)\tTop 5-err 0.0000 (0.3880)\n",
            "Epoch: [92/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3894 (0.3612)\tTop 1-err 10.9375 (12.3795)\tTop 5-err 0.7812 (0.4003)\n",
            "Epoch: [92/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2407 (0.3640)\tTop 1-err 7.8125 (12.5560)\tTop 5-err 0.7812 (0.4202)\n",
            "Epoch: [92/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3891 (0.3642)\tTop 1-err 14.8438 (12.5493)\tTop 5-err 0.0000 (0.4101)\n",
            "Epoch: [92/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3679 (0.3657)\tTop 1-err 14.0625 (12.5556)\tTop 5-err 0.0000 (0.4095)\n",
            "Test (on val set): [92/200][0/79]\tTime 0.133 (0.133)\tLoss 0.5234 (0.5234)\tTop 1-err 18.7500 (18.7500)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [92/200][50/79]\tTime 0.016 (0.018)\tLoss 0.6595 (0.6500)\tTop 1-err 17.1875 (20.2359)\tTop 5-err 0.7812 (1.4553)\n",
            "* Epoch: [92/200]\tTop 1-err 20.240\tTop 5-err 1.470\tTest Loss 0.655\n",
            "\n",
            "Epoch 94/200\n",
            "Epoch: [93/200][0/390]\tTime 0.210 (0.210)\tData 0.136 (0.136)\tLoss 0.3449 (0.3449)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [93/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3080 (0.3509)\tTop 1-err 8.5938 (12.0558)\tTop 5-err 0.7812 (0.3523)\n",
            "Epoch: [93/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3110 (0.3534)\tTop 1-err 12.5000 (12.1674)\tTop 5-err 0.0000 (0.3171)\n",
            "Epoch: [93/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4288 (0.3618)\tTop 1-err 14.0625 (12.4431)\tTop 5-err 0.0000 (0.3363)\n",
            "Epoch: [93/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3262 (0.3600)\tTop 1-err 14.8438 (12.4262)\tTop 5-err 0.0000 (0.3770)\n",
            "Epoch: [93/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2997 (0.3673)\tTop 1-err 7.0312 (12.6805)\tTop 5-err 0.7812 (0.3735)\n",
            "Epoch: [93/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3716 (0.3661)\tTop 1-err 12.5000 (12.5831)\tTop 5-err 0.7812 (0.3815)\n",
            "Epoch: [93/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3104 (0.3685)\tTop 1-err 10.1562 (12.6803)\tTop 5-err 0.0000 (0.3806)\n",
            "Test (on val set): [93/200][0/79]\tTime 0.136 (0.136)\tLoss 0.3986 (0.3986)\tTop 1-err 10.9375 (10.9375)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [93/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4783 (0.5569)\tTop 1-err 18.7500 (18.7653)\tTop 5-err 0.0000 (0.8425)\n",
            "* Epoch: [93/200]\tTop 1-err 18.860\tTop 5-err 0.820\tTest Loss 0.556\n",
            "\n",
            "Epoch 95/200\n",
            "Epoch: [94/200][0/390]\tTime 0.192 (0.192)\tData 0.133 (0.133)\tLoss 0.3637 (0.3637)\tTop 1-err 15.6250 (15.6250)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [94/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4509 (0.3534)\tTop 1-err 16.4062 (12.5766)\tTop 5-err 0.0000 (0.2757)\n",
            "Epoch: [94/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3795 (0.3713)\tTop 1-err 16.4062 (12.9873)\tTop 5-err 0.7812 (0.3558)\n",
            "Epoch: [94/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3093 (0.3690)\tTop 1-err 11.7188 (12.7846)\tTop 5-err 0.0000 (0.3932)\n",
            "Epoch: [94/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3693 (0.3641)\tTop 1-err 11.7188 (12.5583)\tTop 5-err 0.0000 (0.4003)\n",
            "Epoch: [94/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3851 (0.3644)\tTop 1-err 13.2812 (12.5187)\tTop 5-err 0.7812 (0.3953)\n",
            "Epoch: [94/200][300/390]\tTime 0.051 (0.051)\tData 0.000 (0.001)\tLoss 0.3982 (0.3674)\tTop 1-err 17.1875 (12.7362)\tTop 5-err 0.7812 (0.4023)\n",
            "Epoch: [94/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3384 (0.3665)\tTop 1-err 14.0625 (12.6892)\tTop 5-err 2.3438 (0.4140)\n",
            "Test (on val set): [94/200][0/79]\tTime 0.135 (0.135)\tLoss 1.1128 (1.1128)\tTop 1-err 34.3750 (34.3750)\tTop 5-err 3.9062 (3.9062)\n",
            "Test (on val set): [94/200][50/79]\tTime 0.016 (0.019)\tLoss 1.2645 (1.1930)\tTop 1-err 30.4688 (32.7359)\tTop 5-err 5.4688 (3.9369)\n",
            "* Epoch: [94/200]\tTop 1-err 32.390\tTop 5-err 3.950\tTest Loss 1.204\n",
            "\n",
            "Epoch 96/200\n",
            "Epoch: [95/200][0/390]\tTime 0.201 (0.201)\tData 0.130 (0.130)\tLoss 0.3819 (0.3819)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [95/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2730 (0.3525)\tTop 1-err 7.0312 (12.7911)\tTop 5-err 0.0000 (0.2911)\n",
            "Epoch: [95/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.3819 (0.3513)\tTop 1-err 9.3750 (12.2989)\tTop 5-err 2.3438 (0.3868)\n",
            "Epoch: [95/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3353 (0.3536)\tTop 1-err 11.7188 (12.4276)\tTop 5-err 0.0000 (0.3880)\n",
            "Epoch: [95/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2847 (0.3531)\tTop 1-err 9.3750 (12.3095)\tTop 5-err 0.0000 (0.3965)\n",
            "Epoch: [95/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4400 (0.3623)\tTop 1-err 13.2812 (12.5747)\tTop 5-err 0.0000 (0.4046)\n",
            "Epoch: [95/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3552 (0.3639)\tTop 1-err 15.6250 (12.6817)\tTop 5-err 0.0000 (0.4101)\n",
            "Epoch: [95/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4243 (0.3640)\tTop 1-err 14.8438 (12.6380)\tTop 5-err 2.3438 (0.4095)\n",
            "Test (on val set): [95/200][0/79]\tTime 0.130 (0.130)\tLoss 0.5866 (0.5866)\tTop 1-err 17.1875 (17.1875)\tTop 5-err 1.5625 (1.5625)\n",
            "Test (on val set): [95/200][50/79]\tTime 0.016 (0.018)\tLoss 0.8895 (0.7429)\tTop 1-err 20.3125 (21.6299)\tTop 5-err 0.7812 (1.4553)\n",
            "* Epoch: [95/200]\tTop 1-err 21.690\tTop 5-err 1.450\tTest Loss 0.742\n",
            "\n",
            "Epoch 97/200\n",
            "Epoch: [96/200][0/390]\tTime 0.189 (0.189)\tData 0.131 (0.131)\tLoss 0.3475 (0.3475)\tTop 1-err 14.0625 (14.0625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [96/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.4574 (0.3551)\tTop 1-err 21.0938 (11.9485)\tTop 5-err 0.0000 (0.4749)\n",
            "Epoch: [96/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.4850 (0.3606)\tTop 1-err 17.9688 (12.3376)\tTop 5-err 0.0000 (0.4254)\n",
            "Epoch: [96/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3617 (0.3626)\tTop 1-err 10.1562 (12.4276)\tTop 5-err 0.7812 (0.4294)\n",
            "Epoch: [96/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2726 (0.3632)\tTop 1-err 10.1562 (12.5039)\tTop 5-err 0.0000 (0.3926)\n",
            "Epoch: [96/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4735 (0.3676)\tTop 1-err 14.0625 (12.6121)\tTop 5-err 0.0000 (0.4295)\n",
            "Epoch: [96/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2540 (0.3681)\tTop 1-err 10.1562 (12.7154)\tTop 5-err 0.0000 (0.4205)\n",
            "Epoch: [96/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4052 (0.3706)\tTop 1-err 14.8438 (12.7804)\tTop 5-err 0.0000 (0.4363)\n",
            "Test (on val set): [96/200][0/79]\tTime 0.124 (0.124)\tLoss 0.7002 (0.7002)\tTop 1-err 20.3125 (20.3125)\tTop 5-err 3.1250 (3.1250)\n",
            "Test (on val set): [96/200][50/79]\tTime 0.016 (0.018)\tLoss 0.7037 (0.6216)\tTop 1-err 20.3125 (18.9798)\tTop 5-err 2.3438 (1.6544)\n",
            "* Epoch: [96/200]\tTop 1-err 19.510\tTop 5-err 1.490\tTest Loss 0.622\n",
            "\n",
            "Epoch 98/200\n",
            "Epoch: [97/200][0/390]\tTime 0.193 (0.193)\tData 0.132 (0.132)\tLoss 0.2869 (0.2869)\tTop 1-err 7.8125 (7.8125)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [97/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.5226 (0.3595)\tTop 1-err 19.5312 (12.5460)\tTop 5-err 0.7812 (0.3370)\n",
            "Epoch: [97/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3959 (0.3556)\tTop 1-err 14.8438 (12.3376)\tTop 5-err 0.0000 (0.4177)\n",
            "Epoch: [97/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3936 (0.3581)\tTop 1-err 14.8438 (12.3448)\tTop 5-err 0.7812 (0.4243)\n",
            "Epoch: [97/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3439 (0.3563)\tTop 1-err 12.5000 (12.2707)\tTop 5-err 0.0000 (0.4159)\n",
            "Epoch: [97/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3670 (0.3592)\tTop 1-err 14.0625 (12.3942)\tTop 5-err 0.0000 (0.4326)\n",
            "Epoch: [97/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3568 (0.3575)\tTop 1-err 9.3750 (12.3001)\tTop 5-err 0.0000 (0.4283)\n",
            "Epoch: [97/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2757 (0.3609)\tTop 1-err 8.5938 (12.4265)\tTop 5-err 0.7812 (0.4340)\n",
            "Test (on val set): [97/200][0/79]\tTime 0.135 (0.135)\tLoss 0.3738 (0.3738)\tTop 1-err 14.8438 (14.8438)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [97/200][50/79]\tTime 0.016 (0.018)\tLoss 0.5330 (0.4847)\tTop 1-err 21.0938 (16.1305)\tTop 5-err 0.7812 (0.6434)\n",
            "* Epoch: [97/200]\tTop 1-err 16.070\tTop 5-err 0.590\tTest Loss 0.480\n",
            "\n",
            "Epoch 99/200\n",
            "Epoch: [98/200][0/390]\tTime 0.198 (0.198)\tData 0.128 (0.128)\tLoss 0.3509 (0.3509)\tTop 1-err 12.5000 (12.5000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [98/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3781 (0.3410)\tTop 1-err 14.8438 (11.8413)\tTop 5-err 1.5625 (0.2604)\n",
            "Epoch: [98/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.5023 (0.3432)\tTop 1-err 16.4062 (11.6259)\tTop 5-err 1.5625 (0.3790)\n",
            "Epoch: [98/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.5256 (0.3518)\tTop 1-err 14.8438 (11.9516)\tTop 5-err 0.7812 (0.3725)\n",
            "Epoch: [98/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2795 (0.3568)\tTop 1-err 9.3750 (12.1929)\tTop 5-err 0.0000 (0.3848)\n",
            "Epoch: [98/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3973 (0.3581)\tTop 1-err 12.5000 (12.1701)\tTop 5-err 0.0000 (0.4015)\n",
            "Epoch: [98/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3590 (0.3638)\tTop 1-err 11.7188 (12.4221)\tTop 5-err 0.7812 (0.4360)\n",
            "Epoch: [98/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.5976 (0.3633)\tTop 1-err 20.3125 (12.3820)\tTop 5-err 0.7812 (0.4318)\n",
            "Test (on val set): [98/200][0/79]\tTime 0.125 (0.125)\tLoss 0.3812 (0.3812)\tTop 1-err 11.7188 (11.7188)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [98/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4292 (0.4716)\tTop 1-err 15.6250 (15.1042)\tTop 5-err 0.0000 (0.5515)\n",
            "* Epoch: [98/200]\tTop 1-err 15.120\tTop 5-err 0.520\tTest Loss 0.465\n",
            "\n",
            "Epoch 100/200\n",
            "Epoch: [99/200][0/390]\tTime 0.200 (0.200)\tData 0.130 (0.130)\tLoss 0.3693 (0.3693)\tTop 1-err 10.9375 (10.9375)\tTop 5-err 1.5625 (1.5625)\n",
            "Epoch: [99/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.3511 (0.3492)\tTop 1-err 11.7188 (11.6881)\tTop 5-err 0.7812 (0.4442)\n",
            "Epoch: [99/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3355 (0.3498)\tTop 1-err 11.7188 (11.9199)\tTop 5-err 0.0000 (0.4177)\n",
            "Epoch: [99/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3861 (0.3503)\tTop 1-err 9.3750 (11.9154)\tTop 5-err 0.7812 (0.3673)\n",
            "Epoch: [99/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.3867 (0.3475)\tTop 1-err 13.2812 (11.8859)\tTop 5-err 0.7812 (0.3731)\n",
            "Epoch: [99/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.4219 (0.3559)\tTop 1-err 17.9688 (12.1514)\tTop 5-err 0.0000 (0.3891)\n",
            "Epoch: [99/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.3680 (0.3596)\tTop 1-err 15.6250 (12.2508)\tTop 5-err 0.0000 (0.4049)\n",
            "Epoch: [99/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.4500 (0.3598)\tTop 1-err 14.8438 (12.2574)\tTop 5-err 0.7812 (0.3962)\n",
            "Test (on val set): [99/200][0/79]\tTime 0.130 (0.130)\tLoss 0.4021 (0.4021)\tTop 1-err 14.0625 (14.0625)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [99/200][50/79]\tTime 0.016 (0.018)\tLoss 0.4070 (0.4681)\tTop 1-err 11.7188 (15.5484)\tTop 5-err 0.7812 (0.8425)\n",
            "* Epoch: [99/200]\tTop 1-err 15.920\tTop 5-err 0.790\tTest Loss 0.474\n",
            "\n",
            "Epoch 101/200\n",
            "Epoch: [100/200][0/390]\tTime 0.191 (0.191)\tData 0.133 (0.133)\tLoss 0.3914 (0.3914)\tTop 1-err 9.3750 (9.3750)\tTop 5-err 0.7812 (0.7812)\n",
            "Epoch: [100/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.2791 (0.2750)\tTop 1-err 8.5938 (8.9920)\tTop 5-err 0.0000 (0.2451)\n",
            "Epoch: [100/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.3122 (0.2470)\tTop 1-err 10.9375 (8.1915)\tTop 5-err 0.0000 (0.2088)\n",
            "Epoch: [100/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1342 (0.2373)\tTop 1-err 4.6875 (7.8901)\tTop 5-err 0.0000 (0.2018)\n",
            "Epoch: [100/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.2160 (0.2285)\tTop 1-err 5.4688 (7.6726)\tTop 5-err 0.0000 (0.1943)\n",
            "Epoch: [100/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1488 (0.2201)\tTop 1-err 5.4688 (7.3736)\tTop 5-err 0.0000 (0.1836)\n",
            "Epoch: [100/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1857 (0.2124)\tTop 1-err 4.6875 (7.1065)\tTop 5-err 0.0000 (0.1609)\n",
            "Epoch: [100/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.2168 (0.2069)\tTop 1-err 4.6875 (6.9088)\tTop 5-err 0.0000 (0.1536)\n",
            "Test (on val set): [100/200][0/79]\tTime 0.137 (0.137)\tLoss 0.1365 (0.1365)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [100/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1973 (0.2226)\tTop 1-err 6.2500 (7.4142)\tTop 5-err 0.0000 (0.1838)\n",
            "* Epoch: [100/200]\tTop 1-err 7.400\tTop 5-err 0.170\tTest Loss 0.219\n",
            "\n",
            "Epoch 102/200\n",
            "Epoch: [101/200][0/390]\tTime 0.190 (0.190)\tData 0.129 (0.129)\tLoss 0.1211 (0.1211)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [101/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0960 (0.1543)\tTop 1-err 1.5625 (5.2083)\tTop 5-err 0.0000 (0.0613)\n",
            "Epoch: [101/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0932 (0.1489)\tTop 1-err 3.1250 (4.9969)\tTop 5-err 0.0000 (0.0619)\n",
            "Epoch: [101/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1335 (0.1527)\tTop 1-err 3.9062 (5.1273)\tTop 5-err 0.7812 (0.0673)\n",
            "Epoch: [101/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1752 (0.1522)\tTop 1-err 5.4688 (5.0801)\tTop 5-err 0.0000 (0.0777)\n",
            "Epoch: [101/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1956 (0.1515)\tTop 1-err 8.5938 (5.0921)\tTop 5-err 0.0000 (0.0747)\n",
            "Epoch: [101/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1855 (0.1483)\tTop 1-err 5.4688 (4.9522)\tTop 5-err 0.0000 (0.0675)\n",
            "Epoch: [101/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1555 (0.1452)\tTop 1-err 7.0312 (4.8433)\tTop 5-err 0.0000 (0.0623)\n",
            "Test (on val set): [101/200][0/79]\tTime 0.136 (0.136)\tLoss 0.1747 (0.1747)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [101/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1623 (0.2126)\tTop 1-err 7.8125 (7.0772)\tTop 5-err 0.0000 (0.1838)\n",
            "* Epoch: [101/200]\tTop 1-err 7.040\tTop 5-err 0.170\tTest Loss 0.207\n",
            "\n",
            "Epoch 103/200\n",
            "Epoch: [102/200][0/390]\tTime 0.203 (0.203)\tData 0.132 (0.132)\tLoss 0.1078 (0.1078)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [102/200][50/390]\tTime 0.056 (0.053)\tData 0.000 (0.003)\tLoss 0.1194 (0.1327)\tTop 1-err 3.1250 (4.3199)\tTop 5-err 0.0000 (0.0613)\n",
            "Epoch: [102/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.1248 (0.1280)\tTop 1-err 4.6875 (4.2079)\tTop 5-err 0.0000 (0.0619)\n",
            "Epoch: [102/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0979 (0.1305)\tTop 1-err 4.6875 (4.3512)\tTop 5-err 0.0000 (0.0673)\n",
            "Epoch: [102/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0906 (0.1301)\tTop 1-err 3.1250 (4.3183)\tTop 5-err 0.0000 (0.0738)\n",
            "Epoch: [102/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1737 (0.1313)\tTop 1-err 5.4688 (4.3918)\tTop 5-err 0.0000 (0.0747)\n",
            "Epoch: [102/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1129 (0.1306)\tTop 1-err 3.1250 (4.3994)\tTop 5-err 0.0000 (0.0753)\n",
            "Epoch: [102/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0801 (0.1291)\tTop 1-err 3.1250 (4.3959)\tTop 5-err 0.0000 (0.0645)\n",
            "Test (on val set): [102/200][0/79]\tTime 0.129 (0.129)\tLoss 0.2130 (0.2130)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [102/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1781 (0.2069)\tTop 1-err 8.5938 (6.9240)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [102/200]\tTop 1-err 6.880\tTop 5-err 0.140\tTest Loss 0.205\n",
            "\n",
            "Epoch 104/200\n",
            "Epoch: [103/200][0/390]\tTime 0.191 (0.191)\tData 0.133 (0.133)\tLoss 0.1807 (0.1807)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [103/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.1085 (0.1165)\tTop 1-err 3.1250 (3.8909)\tTop 5-err 0.0000 (0.0613)\n",
            "Epoch: [103/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.1530 (0.1136)\tTop 1-err 4.6875 (3.8057)\tTop 5-err 0.7812 (0.0541)\n",
            "Epoch: [103/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0749 (0.1106)\tTop 1-err 1.5625 (3.6683)\tTop 5-err 0.0000 (0.0466)\n",
            "Epoch: [103/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1364 (0.1133)\tTop 1-err 4.6875 (3.8207)\tTop 5-err 0.0000 (0.0622)\n",
            "Epoch: [103/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1371 (0.1146)\tTop 1-err 6.2500 (3.9249)\tTop 5-err 0.0000 (0.0560)\n",
            "Epoch: [103/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0811 (0.1139)\tTop 1-err 1.5625 (3.9037)\tTop 5-err 0.0000 (0.0493)\n",
            "Epoch: [103/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1904 (0.1139)\tTop 1-err 5.4688 (3.9062)\tTop 5-err 0.0000 (0.0490)\n",
            "Test (on val set): [103/200][0/79]\tTime 0.132 (0.132)\tLoss 0.1685 (0.1685)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [103/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1707 (0.2025)\tTop 1-err 9.3750 (7.0466)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [103/200]\tTop 1-err 6.800\tTop 5-err 0.140\tTest Loss 0.200\n",
            "\n",
            "Epoch 105/200\n",
            "Epoch: [104/200][0/390]\tTime 0.191 (0.191)\tData 0.134 (0.134)\tLoss 0.1145 (0.1145)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [104/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0651 (0.0989)\tTop 1-err 2.3438 (3.4007)\tTop 5-err 0.0000 (0.0153)\n",
            "Epoch: [104/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.1367 (0.0997)\tTop 1-err 5.4688 (3.3571)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [104/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1147 (0.0974)\tTop 1-err 5.4688 (3.2130)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [104/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1705 (0.0974)\tTop 1-err 7.8125 (3.2727)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [104/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0912 (0.0989)\tTop 1-err 3.1250 (3.3678)\tTop 5-err 0.0000 (0.0156)\n",
            "Epoch: [104/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1096 (0.0993)\tTop 1-err 3.9062 (3.3923)\tTop 5-err 0.0000 (0.0156)\n",
            "Epoch: [104/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1158 (0.1003)\tTop 1-err 4.6875 (3.4099)\tTop 5-err 0.0000 (0.0156)\n",
            "Test (on val set): [104/200][0/79]\tTime 0.133 (0.133)\tLoss 0.2089 (0.2089)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [104/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1718 (0.2180)\tTop 1-err 7.0312 (7.1078)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [104/200]\tTop 1-err 6.950\tTop 5-err 0.170\tTest Loss 0.214\n",
            "\n",
            "Epoch 106/200\n",
            "Epoch: [105/200][0/390]\tTime 0.200 (0.200)\tData 0.138 (0.138)\tLoss 0.1036 (0.1036)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [105/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.1382 (0.0903)\tTop 1-err 3.1250 (3.0790)\tTop 5-err 0.0000 (0.0306)\n",
            "Epoch: [105/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0964 (0.0859)\tTop 1-err 3.9062 (2.9162)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [105/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1065 (0.0844)\tTop 1-err 3.1250 (2.8870)\tTop 5-err 0.0000 (0.0207)\n",
            "Epoch: [105/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0937 (0.0859)\tTop 1-err 3.1250 (2.9190)\tTop 5-err 0.0000 (0.0233)\n",
            "Epoch: [105/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1112 (0.0886)\tTop 1-err 5.4688 (3.0192)\tTop 5-err 0.0000 (0.0280)\n",
            "Epoch: [105/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0579 (0.0914)\tTop 1-err 1.5625 (3.0705)\tTop 5-err 0.0000 (0.0260)\n",
            "Epoch: [105/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1226 (0.0921)\tTop 1-err 4.6875 (3.1161)\tTop 5-err 0.0000 (0.0245)\n",
            "Test (on val set): [105/200][0/79]\tTime 0.136 (0.136)\tLoss 0.1581 (0.1581)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [105/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1301 (0.2035)\tTop 1-err 6.2500 (6.5870)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [105/200]\tTop 1-err 6.420\tTop 5-err 0.110\tTest Loss 0.200\n",
            "\n",
            "Epoch 107/200\n",
            "Epoch: [106/200][0/390]\tTime 0.195 (0.195)\tData 0.134 (0.134)\tLoss 0.1042 (0.1042)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [106/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0385 (0.0775)\tTop 1-err 0.7812 (2.5276)\tTop 5-err 0.0000 (0.0153)\n",
            "Epoch: [106/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0800 (0.0797)\tTop 1-err 3.1250 (2.7073)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [106/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1180 (0.0843)\tTop 1-err 5.4688 (2.8404)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [106/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0782 (0.0853)\tTop 1-err 2.3438 (2.8685)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [106/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0764 (0.0852)\tTop 1-err 2.3438 (2.9071)\tTop 5-err 0.0000 (0.0187)\n",
            "Epoch: [106/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0394 (0.0847)\tTop 1-err 1.5625 (2.8836)\tTop 5-err 0.0000 (0.0182)\n",
            "Epoch: [106/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0660 (0.0852)\tTop 1-err 2.3438 (2.8824)\tTop 5-err 0.0000 (0.0178)\n",
            "Test (on val set): [106/200][0/79]\tTime 0.125 (0.125)\tLoss 0.1675 (0.1675)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [106/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1664 (0.2021)\tTop 1-err 7.0312 (6.3725)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [106/200]\tTop 1-err 6.530\tTop 5-err 0.090\tTest Loss 0.204\n",
            "\n",
            "Epoch 108/200\n",
            "Epoch: [107/200][0/390]\tTime 0.195 (0.195)\tData 0.135 (0.135)\tLoss 0.0340 (0.0340)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [107/200][50/390]\tTime 0.051 (0.053)\tData 0.000 (0.003)\tLoss 0.0888 (0.0808)\tTop 1-err 3.1250 (2.7727)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [107/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0512 (0.0752)\tTop 1-err 1.5625 (2.6067)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [107/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0232 (0.0786)\tTop 1-err 0.0000 (2.6852)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [107/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0663 (0.0783)\tTop 1-err 3.1250 (2.6508)\tTop 5-err 0.0000 (0.0117)\n",
            "Epoch: [107/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0282 (0.0778)\tTop 1-err 0.0000 (2.5990)\tTop 5-err 0.0000 (0.0125)\n",
            "Epoch: [107/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1046 (0.0785)\tTop 1-err 3.9062 (2.6370)\tTop 5-err 0.0000 (0.0182)\n",
            "Epoch: [107/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1343 (0.0780)\tTop 1-err 4.6875 (2.5975)\tTop 5-err 0.0000 (0.0200)\n",
            "Test (on val set): [107/200][0/79]\tTime 0.127 (0.127)\tLoss 0.1639 (0.1639)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [107/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1325 (0.2154)\tTop 1-err 4.6875 (6.5717)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [107/200]\tTop 1-err 6.450\tTop 5-err 0.100\tTest Loss 0.213\n",
            "\n",
            "Epoch 109/200\n",
            "Epoch: [108/200][0/390]\tTime 0.205 (0.205)\tData 0.133 (0.133)\tLoss 0.0391 (0.0391)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [108/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0778 (0.0774)\tTop 1-err 1.5625 (2.5123)\tTop 5-err 0.0000 (0.0153)\n",
            "Epoch: [108/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0493 (0.0750)\tTop 1-err 0.7812 (2.4211)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [108/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0359 (0.0747)\tTop 1-err 0.0000 (2.4783)\tTop 5-err 0.0000 (0.0207)\n",
            "Epoch: [108/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0496 (0.0734)\tTop 1-err 2.3438 (2.4176)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [108/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0562 (0.0744)\tTop 1-err 1.5625 (2.4371)\tTop 5-err 0.0000 (0.0156)\n",
            "Epoch: [108/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1237 (0.0751)\tTop 1-err 3.9062 (2.4657)\tTop 5-err 0.0000 (0.0182)\n",
            "Epoch: [108/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0762 (0.0752)\tTop 1-err 2.3438 (2.4840)\tTop 5-err 0.0000 (0.0156)\n",
            "Test (on val set): [108/200][0/79]\tTime 0.125 (0.125)\tLoss 0.1552 (0.1552)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [108/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1651 (0.2201)\tTop 1-err 6.2500 (6.4491)\tTop 5-err 0.0000 (0.2298)\n",
            "* Epoch: [108/200]\tTop 1-err 6.470\tTop 5-err 0.220\tTest Loss 0.216\n",
            "\n",
            "Epoch 110/200\n",
            "Epoch: [109/200][0/390]\tTime 0.201 (0.201)\tData 0.129 (0.129)\tLoss 0.0469 (0.0469)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [109/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0203 (0.0632)\tTop 1-err 0.0000 (2.0067)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [109/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0363 (0.0632)\tTop 1-err 1.5625 (2.1658)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [109/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0774 (0.0626)\tTop 1-err 3.1250 (2.1368)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [109/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0145 (0.0632)\tTop 1-err 0.0000 (2.1611)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [109/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0933 (0.0644)\tTop 1-err 2.3438 (2.1943)\tTop 5-err 0.0000 (0.0093)\n",
            "Epoch: [109/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0844 (0.0651)\tTop 1-err 2.3438 (2.2192)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [109/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0622 (0.0669)\tTop 1-err 1.5625 (2.2369)\tTop 5-err 0.0000 (0.0089)\n",
            "Test (on val set): [109/200][0/79]\tTime 0.122 (0.122)\tLoss 0.1948 (0.1948)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [109/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1747 (0.2301)\tTop 1-err 5.4688 (6.6330)\tTop 5-err 0.0000 (0.1685)\n",
            "* Epoch: [109/200]\tTop 1-err 6.610\tTop 5-err 0.160\tTest Loss 0.224\n",
            "\n",
            "Epoch 111/200\n",
            "Epoch: [110/200][0/390]\tTime 0.199 (0.199)\tData 0.138 (0.138)\tLoss 0.0773 (0.0773)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [110/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0695 (0.0624)\tTop 1-err 3.1250 (2.2059)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [110/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0631 (0.0630)\tTop 1-err 2.3438 (2.1658)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [110/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1235 (0.0636)\tTop 1-err 5.4688 (2.1678)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [110/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0533 (0.0641)\tTop 1-err 0.7812 (2.1688)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [110/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0732 (0.0646)\tTop 1-err 3.1250 (2.2006)\tTop 5-err 0.0000 (0.0125)\n",
            "Epoch: [110/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0764 (0.0645)\tTop 1-err 3.1250 (2.1932)\tTop 5-err 0.0000 (0.0130)\n",
            "Epoch: [110/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0613 (0.0655)\tTop 1-err 2.3438 (2.2280)\tTop 5-err 0.0000 (0.0111)\n",
            "Test (on val set): [110/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1826 (0.1826)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [110/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0902 (0.2274)\tTop 1-err 3.9062 (6.6789)\tTop 5-err 0.0000 (0.1685)\n",
            "* Epoch: [110/200]\tTop 1-err 6.400\tTop 5-err 0.120\tTest Loss 0.218\n",
            "\n",
            "Epoch 112/200\n",
            "Epoch: [111/200][0/390]\tTime 0.197 (0.197)\tData 0.135 (0.135)\tLoss 0.1153 (0.1153)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [111/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0747 (0.0585)\tTop 1-err 3.9062 (2.0221)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [111/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0715 (0.0634)\tTop 1-err 1.5625 (2.2664)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [111/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0568 (0.0629)\tTop 1-err 3.1250 (2.1678)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [111/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0649 (0.0622)\tTop 1-err 3.1250 (2.1222)\tTop 5-err 0.0000 (0.0117)\n",
            "Epoch: [111/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0453 (0.0618)\tTop 1-err 1.5625 (2.0947)\tTop 5-err 0.0000 (0.0125)\n",
            "Epoch: [111/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0602 (0.0628)\tTop 1-err 3.1250 (2.1179)\tTop 5-err 0.0000 (0.0104)\n",
            "Epoch: [111/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0373 (0.0626)\tTop 1-err 1.5625 (2.1056)\tTop 5-err 0.0000 (0.0089)\n",
            "Test (on val set): [111/200][0/79]\tTime 0.128 (0.128)\tLoss 0.1373 (0.1373)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [111/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1349 (0.2336)\tTop 1-err 6.2500 (6.9547)\tTop 5-err 0.0000 (0.2145)\n",
            "* Epoch: [111/200]\tTop 1-err 6.760\tTop 5-err 0.170\tTest Loss 0.226\n",
            "\n",
            "Epoch 113/200\n",
            "Epoch: [112/200][0/390]\tTime 0.204 (0.204)\tData 0.131 (0.131)\tLoss 0.0313 (0.0313)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [112/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0283 (0.0515)\tTop 1-err 0.7812 (1.8229)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [112/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0164 (0.0520)\tTop 1-err 0.0000 (1.7868)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [112/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0222 (0.0540)\tTop 1-err 0.7812 (1.8729)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [112/200][200/390]\tTime 0.052 (0.051)\tData 0.000 (0.001)\tLoss 0.0353 (0.0537)\tTop 1-err 0.7812 (1.8268)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [112/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0396 (0.0543)\tTop 1-err 2.3438 (1.8240)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [112/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1109 (0.0549)\tTop 1-err 2.3438 (1.8298)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [112/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0413 (0.0562)\tTop 1-err 1.5625 (1.8897)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [112/200][0/79]\tTime 0.127 (0.127)\tLoss 0.1724 (0.1724)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [112/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1685 (0.2440)\tTop 1-err 5.4688 (7.0772)\tTop 5-err 0.0000 (0.1838)\n",
            "* Epoch: [112/200]\tTop 1-err 6.790\tTop 5-err 0.160\tTest Loss 0.237\n",
            "\n",
            "Epoch 114/200\n",
            "Epoch: [113/200][0/390]\tTime 0.194 (0.194)\tData 0.135 (0.135)\tLoss 0.0211 (0.0211)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [113/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0748 (0.0480)\tTop 1-err 2.3438 (1.3327)\tTop 5-err 0.0000 (0.0153)\n",
            "Epoch: [113/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0640 (0.0482)\tTop 1-err 1.5625 (1.4774)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [113/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0440 (0.0490)\tTop 1-err 0.7812 (1.5625)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [113/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0339 (0.0494)\tTop 1-err 0.7812 (1.5897)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [113/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0387 (0.0496)\tTop 1-err 0.7812 (1.5999)\tTop 5-err 0.0000 (0.0125)\n",
            "Epoch: [113/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0589 (0.0506)\tTop 1-err 1.5625 (1.6456)\tTop 5-err 0.0000 (0.0104)\n",
            "Epoch: [113/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0503 (0.0513)\tTop 1-err 2.3438 (1.6871)\tTop 5-err 0.0000 (0.0089)\n",
            "Test (on val set): [113/200][0/79]\tTime 0.127 (0.127)\tLoss 0.1337 (0.1337)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [113/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1498 (0.2371)\tTop 1-err 6.2500 (6.6176)\tTop 5-err 0.0000 (0.2451)\n",
            "* Epoch: [113/200]\tTop 1-err 6.660\tTop 5-err 0.220\tTest Loss 0.235\n",
            "\n",
            "Epoch 115/200\n",
            "Epoch: [114/200][0/390]\tTime 0.195 (0.195)\tData 0.134 (0.134)\tLoss 0.0303 (0.0303)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [114/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0621 (0.0455)\tTop 1-err 2.3438 (1.3480)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [114/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0940 (0.0467)\tTop 1-err 2.3438 (1.3691)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [114/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0144 (0.0474)\tTop 1-err 0.7812 (1.4228)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [114/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0334 (0.0487)\tTop 1-err 2.3438 (1.5314)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [114/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0632 (0.0490)\tTop 1-err 1.5625 (1.5687)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [114/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0297 (0.0485)\tTop 1-err 0.7812 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [114/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1120 (0.0499)\tTop 1-err 3.1250 (1.6315)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [114/200][0/79]\tTime 0.125 (0.125)\tLoss 0.1736 (0.1736)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [114/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1750 (0.2430)\tTop 1-err 7.0312 (7.1078)\tTop 5-err 0.0000 (0.1991)\n",
            "* Epoch: [114/200]\tTop 1-err 6.990\tTop 5-err 0.180\tTest Loss 0.240\n",
            "\n",
            "Epoch 116/200\n",
            "Epoch: [115/200][0/390]\tTime 0.190 (0.190)\tData 0.131 (0.131)\tLoss 0.0153 (0.0153)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [115/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0475 (0.0519)\tTop 1-err 1.5625 (1.7770)\tTop 5-err 0.0000 (0.0153)\n",
            "Epoch: [115/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0377 (0.0503)\tTop 1-err 1.5625 (1.6631)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [115/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0298 (0.0505)\tTop 1-err 0.7812 (1.6194)\tTop 5-err 0.0000 (0.0103)\n",
            "Epoch: [115/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0521 (0.0501)\tTop 1-err 1.5625 (1.6053)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [115/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1039 (0.0511)\tTop 1-err 4.6875 (1.6683)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [115/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1068 (0.0539)\tTop 1-err 3.9062 (1.8117)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [115/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0358 (0.0541)\tTop 1-err 0.7812 (1.8162)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [115/200][0/79]\tTime 0.139 (0.139)\tLoss 0.1705 (0.1705)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [115/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1927 (0.2426)\tTop 1-err 6.2500 (6.8627)\tTop 5-err 0.0000 (0.2298)\n",
            "* Epoch: [115/200]\tTop 1-err 6.870\tTop 5-err 0.210\tTest Loss 0.238\n",
            "\n",
            "Epoch 117/200\n",
            "Epoch: [116/200][0/390]\tTime 0.205 (0.205)\tData 0.135 (0.135)\tLoss 0.0240 (0.0240)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [116/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.1016 (0.0442)\tTop 1-err 2.3438 (1.4093)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [116/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0949 (0.0453)\tTop 1-err 5.4688 (1.5006)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [116/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1042 (0.0456)\tTop 1-err 4.6875 (1.4797)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [116/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0424 (0.0460)\tTop 1-err 1.5625 (1.4887)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [116/200][250/390]\tTime 0.058 (0.051)\tData 0.000 (0.001)\tLoss 0.0416 (0.0465)\tTop 1-err 1.5625 (1.5096)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [116/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0612 (0.0467)\tTop 1-err 2.3438 (1.5080)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [116/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0764 (0.0476)\tTop 1-err 2.3438 (1.5558)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [116/200][0/79]\tTime 0.123 (0.123)\tLoss 0.1716 (0.1716)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [116/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1843 (0.2427)\tTop 1-err 7.0312 (6.8015)\tTop 5-err 0.0000 (0.1685)\n",
            "* Epoch: [116/200]\tTop 1-err 6.710\tTop 5-err 0.130\tTest Loss 0.238\n",
            "\n",
            "Epoch 118/200\n",
            "Epoch: [117/200][0/390]\tTime 0.191 (0.191)\tData 0.133 (0.133)\tLoss 0.0619 (0.0619)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [117/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0868 (0.0460)\tTop 1-err 3.1250 (1.6238)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [117/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0208 (0.0450)\tTop 1-err 0.0000 (1.5470)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [117/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0519 (0.0439)\tTop 1-err 1.5625 (1.4435)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [117/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0449 (0.0449)\tTop 1-err 2.3438 (1.4964)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [117/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0534 (0.0473)\tTop 1-err 1.5625 (1.5750)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [117/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0766 (0.0476)\tTop 1-err 2.3438 (1.5911)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [117/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0292 (0.0485)\tTop 1-err 0.7812 (1.6293)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [117/200][0/79]\tTime 0.125 (0.125)\tLoss 0.2419 (0.2419)\tTop 1-err 7.0312 (7.0312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [117/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1502 (0.2422)\tTop 1-err 4.6875 (6.6023)\tTop 5-err 0.0000 (0.2145)\n",
            "* Epoch: [117/200]\tTop 1-err 6.770\tTop 5-err 0.170\tTest Loss 0.239\n",
            "\n",
            "Epoch 119/200\n",
            "Epoch: [118/200][0/390]\tTime 0.198 (0.198)\tData 0.138 (0.138)\tLoss 0.0409 (0.0409)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [118/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0215 (0.0416)\tTop 1-err 0.0000 (1.3940)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [118/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0821 (0.0443)\tTop 1-err 2.3438 (1.4387)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [118/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0385 (0.0457)\tTop 1-err 1.5625 (1.5211)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [118/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0669 (0.0470)\tTop 1-err 1.5625 (1.5314)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [118/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0281 (0.0480)\tTop 1-err 0.7812 (1.5905)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [118/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0858 (0.0492)\tTop 1-err 2.3438 (1.6404)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [118/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0268 (0.0502)\tTop 1-err 0.7812 (1.6604)\tTop 5-err 0.0000 (0.0022)\n",
            "Test (on val set): [118/200][0/79]\tTime 0.134 (0.134)\tLoss 0.2234 (0.2234)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [118/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1462 (0.2524)\tTop 1-err 3.9062 (6.7708)\tTop 5-err 0.0000 (0.1838)\n",
            "* Epoch: [118/200]\tTop 1-err 6.930\tTop 5-err 0.150\tTest Loss 0.251\n",
            "\n",
            "Epoch 120/200\n",
            "Epoch: [119/200][0/390]\tTime 0.202 (0.202)\tData 0.131 (0.131)\tLoss 0.0789 (0.0789)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [119/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0647 (0.0440)\tTop 1-err 2.3438 (1.5165)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [119/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0855 (0.0476)\tTop 1-err 3.9062 (1.6012)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [119/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0619 (0.0489)\tTop 1-err 1.5625 (1.6401)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [119/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1013 (0.0499)\tTop 1-err 3.1250 (1.6558)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [119/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0365 (0.0482)\tTop 1-err 1.5625 (1.5812)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [119/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0492 (0.0487)\tTop 1-err 2.3438 (1.6014)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [119/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0703 (0.0490)\tTop 1-err 3.1250 (1.6181)\tTop 5-err 0.0000 (0.0022)\n",
            "Test (on val set): [119/200][0/79]\tTime 0.133 (0.133)\tLoss 0.2469 (0.2469)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [119/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2198 (0.2584)\tTop 1-err 7.0312 (7.1232)\tTop 5-err 0.0000 (0.2604)\n",
            "* Epoch: [119/200]\tTop 1-err 7.190\tTop 5-err 0.270\tTest Loss 0.252\n",
            "\n",
            "Epoch 121/200\n",
            "Epoch: [120/200][0/390]\tTime 0.202 (0.202)\tData 0.144 (0.144)\tLoss 0.0177 (0.0177)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [120/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.0621 (0.0414)\tTop 1-err 3.1250 (1.3480)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [120/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0326 (0.0448)\tTop 1-err 0.7812 (1.4851)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [120/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1118 (0.0473)\tTop 1-err 3.9062 (1.5780)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [120/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0403 (0.0477)\tTop 1-err 0.7812 (1.5897)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [120/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0429 (0.0473)\tTop 1-err 1.5625 (1.5936)\tTop 5-err 0.0000 (0.0093)\n",
            "Epoch: [120/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0576 (0.0480)\tTop 1-err 2.3438 (1.6196)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [120/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0276 (0.0487)\tTop 1-err 0.7812 (1.6360)\tTop 5-err 0.0000 (0.0067)\n",
            "Test (on val set): [120/200][0/79]\tTime 0.125 (0.125)\tLoss 0.2367 (0.2367)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [120/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2252 (0.2724)\tTop 1-err 7.8125 (7.5214)\tTop 5-err 0.0000 (0.1685)\n",
            "* Epoch: [120/200]\tTop 1-err 7.370\tTop 5-err 0.160\tTest Loss 0.268\n",
            "\n",
            "Epoch 122/200\n",
            "Epoch: [121/200][0/390]\tTime 0.195 (0.195)\tData 0.132 (0.132)\tLoss 0.0278 (0.0278)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [121/200][50/390]\tTime 0.052 (0.053)\tData 0.000 (0.003)\tLoss 0.0191 (0.0490)\tTop 1-err 0.0000 (1.6391)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [121/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0437 (0.0463)\tTop 1-err 1.5625 (1.5084)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [121/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0195 (0.0449)\tTop 1-err 0.0000 (1.4487)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [121/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0449 (0.0457)\tTop 1-err 2.3438 (1.4887)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [121/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0552 (0.0478)\tTop 1-err 1.5625 (1.5345)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [121/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0719 (0.0478)\tTop 1-err 1.5625 (1.5547)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [121/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0708 (0.0491)\tTop 1-err 3.9062 (1.6137)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [121/200][0/79]\tTime 0.127 (0.127)\tLoss 0.2839 (0.2839)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [121/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1798 (0.2693)\tTop 1-err 9.3750 (7.6440)\tTop 5-err 0.0000 (0.2298)\n",
            "* Epoch: [121/200]\tTop 1-err 7.540\tTop 5-err 0.190\tTest Loss 0.267\n",
            "\n",
            "Epoch 123/200\n",
            "Epoch: [122/200][0/390]\tTime 0.201 (0.201)\tData 0.130 (0.130)\tLoss 0.0689 (0.0689)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [122/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0411 (0.0450)\tTop 1-err 1.5625 (1.4246)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [122/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0607 (0.0471)\tTop 1-err 1.5625 (1.4929)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [122/200][150/390]\tTime 0.053 (0.051)\tData 0.000 (0.001)\tLoss 0.0464 (0.0482)\tTop 1-err 1.5625 (1.6039)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [122/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0565 (0.0495)\tTop 1-err 2.3438 (1.6325)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [122/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0768 (0.0504)\tTop 1-err 1.5625 (1.6621)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [122/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0840 (0.0509)\tTop 1-err 3.1250 (1.6923)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [122/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0176 (0.0507)\tTop 1-err 0.0000 (1.6738)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [122/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1983 (0.1983)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [122/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1374 (0.2458)\tTop 1-err 5.4688 (6.7096)\tTop 5-err 0.0000 (0.1685)\n",
            "* Epoch: [122/200]\tTop 1-err 6.670\tTop 5-err 0.160\tTest Loss 0.241\n",
            "\n",
            "Epoch 124/200\n",
            "Epoch: [123/200][0/390]\tTime 0.194 (0.194)\tData 0.134 (0.134)\tLoss 0.0337 (0.0337)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [123/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0236 (0.0446)\tTop 1-err 0.0000 (1.5319)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [123/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0462 (0.0446)\tTop 1-err 1.5625 (1.5161)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [123/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0421 (0.0465)\tTop 1-err 1.5625 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [123/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0865 (0.0473)\tTop 1-err 3.9062 (1.5975)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [123/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0533 (0.0467)\tTop 1-err 2.3438 (1.5812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [123/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0513 (0.0484)\tTop 1-err 1.5625 (1.6326)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [123/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0812 (0.0498)\tTop 1-err 0.7812 (1.6782)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [123/200][0/79]\tTime 0.138 (0.138)\tLoss 0.1566 (0.1566)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [123/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1837 (0.2737)\tTop 1-err 5.4688 (7.7819)\tTop 5-err 0.0000 (0.2298)\n",
            "* Epoch: [123/200]\tTop 1-err 7.900\tTop 5-err 0.260\tTest Loss 0.279\n",
            "\n",
            "Epoch 125/200\n",
            "Epoch: [124/200][0/390]\tTime 0.200 (0.200)\tData 0.134 (0.134)\tLoss 0.0469 (0.0469)\tTop 1-err 1.5625 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [124/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0148 (0.0505)\tTop 1-err 0.7812 (1.7463)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [124/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0333 (0.0498)\tTop 1-err 0.0000 (1.6244)\tTop 5-err 0.0000 (0.0155)\n",
            "Epoch: [124/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0667 (0.0481)\tTop 1-err 3.1250 (1.5573)\tTop 5-err 0.0000 (0.0103)\n",
            "Epoch: [124/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0136 (0.0486)\tTop 1-err 0.0000 (1.6208)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [124/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0428 (0.0497)\tTop 1-err 2.3438 (1.6341)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [124/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1168 (0.0501)\tTop 1-err 5.4688 (1.6741)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [124/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0728 (0.0496)\tTop 1-err 3.1250 (1.6404)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [124/200][0/79]\tTime 0.124 (0.124)\tLoss 0.1610 (0.1610)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [124/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2094 (0.2697)\tTop 1-err 7.8125 (7.5214)\tTop 5-err 0.0000 (0.1991)\n",
            "* Epoch: [124/200]\tTop 1-err 7.440\tTop 5-err 0.190\tTest Loss 0.270\n",
            "\n",
            "Epoch 126/200\n",
            "Epoch: [125/200][0/390]\tTime 0.189 (0.189)\tData 0.130 (0.130)\tLoss 0.0853 (0.0853)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [125/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0691 (0.0454)\tTop 1-err 3.1250 (1.6238)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [125/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0276 (0.0431)\tTop 1-err 0.7812 (1.4851)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [125/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0198 (0.0425)\tTop 1-err 0.0000 (1.4073)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [125/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0590 (0.0442)\tTop 1-err 1.5625 (1.4887)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [125/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0518 (0.0447)\tTop 1-err 2.3438 (1.5065)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [125/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0414 (0.0442)\tTop 1-err 1.5625 (1.4820)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [125/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1011 (0.0451)\tTop 1-err 3.9062 (1.5202)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [125/200][0/79]\tTime 0.127 (0.127)\tLoss 0.2036 (0.2036)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [125/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2244 (0.2481)\tTop 1-err 8.5938 (6.9547)\tTop 5-err 0.0000 (0.2604)\n",
            "* Epoch: [125/200]\tTop 1-err 6.730\tTop 5-err 0.230\tTest Loss 0.241\n",
            "\n",
            "Epoch 127/200\n",
            "Epoch: [126/200][0/390]\tTime 0.204 (0.204)\tData 0.135 (0.135)\tLoss 0.0394 (0.0394)\tTop 1-err 1.5625 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [126/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0245 (0.0527)\tTop 1-err 0.0000 (1.8536)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [126/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0304 (0.0471)\tTop 1-err 0.7812 (1.5548)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [126/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0583 (0.0468)\tTop 1-err 1.5625 (1.5470)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [126/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0267 (0.0488)\tTop 1-err 1.5625 (1.6441)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [126/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1026 (0.0501)\tTop 1-err 3.9062 (1.6808)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [126/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0696 (0.0497)\tTop 1-err 2.3438 (1.6741)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [126/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0226 (0.0506)\tTop 1-err 0.7812 (1.7183)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [126/200][0/79]\tTime 0.146 (0.146)\tLoss 0.2398 (0.2398)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [126/200][50/79]\tTime 0.016 (0.019)\tLoss 0.3039 (0.2845)\tTop 1-err 10.1562 (7.8585)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [126/200]\tTop 1-err 7.830\tTop 5-err 0.150\tTest Loss 0.284\n",
            "\n",
            "Epoch 128/200\n",
            "Epoch: [127/200][0/390]\tTime 0.202 (0.202)\tData 0.132 (0.132)\tLoss 0.0992 (0.0992)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [127/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0411 (0.0476)\tTop 1-err 0.7812 (1.5165)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [127/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0304 (0.0516)\tTop 1-err 0.7812 (1.7327)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [127/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0524 (0.0509)\tTop 1-err 3.1250 (1.7177)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [127/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0171 (0.0493)\tTop 1-err 0.0000 (1.6208)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [127/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0341 (0.0493)\tTop 1-err 1.5625 (1.6185)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [127/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0526 (0.0499)\tTop 1-err 1.5625 (1.6248)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [127/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1094 (0.0510)\tTop 1-err 3.9062 (1.6627)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [127/200][0/79]\tTime 0.137 (0.137)\tLoss 0.1924 (0.1924)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [127/200][50/79]\tTime 0.016 (0.019)\tLoss 0.2159 (0.3066)\tTop 1-err 7.0312 (8.4712)\tTop 5-err 0.0000 (0.2145)\n",
            "* Epoch: [127/200]\tTop 1-err 8.630\tTop 5-err 0.200\tTest Loss 0.302\n",
            "\n",
            "Epoch 129/200\n",
            "Epoch: [128/200][0/390]\tTime 0.196 (0.196)\tData 0.134 (0.134)\tLoss 0.0917 (0.0917)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [128/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0663 (0.0501)\tTop 1-err 2.3438 (1.6697)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [128/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0823 (0.0519)\tTop 1-err 3.9062 (1.8023)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [128/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0864 (0.0526)\tTop 1-err 2.3438 (1.8057)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [128/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0462 (0.0520)\tTop 1-err 1.5625 (1.7996)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [128/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0235 (0.0507)\tTop 1-err 0.7812 (1.7181)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [128/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0286 (0.0534)\tTop 1-err 0.7812 (1.7961)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [128/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0652 (0.0542)\tTop 1-err 2.3438 (1.8429)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [128/200][0/79]\tTime 0.132 (0.132)\tLoss 0.2388 (0.2388)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [128/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1803 (0.2811)\tTop 1-err 5.4688 (8.1342)\tTop 5-err 0.0000 (0.1685)\n",
            "* Epoch: [128/200]\tTop 1-err 7.900\tTop 5-err 0.190\tTest Loss 0.279\n",
            "\n",
            "Epoch 130/200\n",
            "Epoch: [129/200][0/390]\tTime 0.193 (0.193)\tData 0.133 (0.133)\tLoss 0.0278 (0.0278)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [129/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0292 (0.0485)\tTop 1-err 0.7812 (1.6697)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [129/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0481 (0.0522)\tTop 1-err 2.3438 (1.7791)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [129/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0768 (0.0503)\tTop 1-err 2.3438 (1.7074)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [129/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0474 (0.0507)\tTop 1-err 1.5625 (1.7102)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [129/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0354 (0.0500)\tTop 1-err 0.7812 (1.6870)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [129/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0442 (0.0499)\tTop 1-err 1.5625 (1.7001)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [129/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0282 (0.0522)\tTop 1-err 0.0000 (1.7873)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [129/200][0/79]\tTime 0.135 (0.135)\tLoss 0.2890 (0.2890)\tTop 1-err 7.0312 (7.0312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [129/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2318 (0.2903)\tTop 1-err 6.2500 (8.0882)\tTop 5-err 0.0000 (0.1838)\n",
            "* Epoch: [129/200]\tTop 1-err 8.140\tTop 5-err 0.180\tTest Loss 0.294\n",
            "\n",
            "Epoch 131/200\n",
            "Epoch: [130/200][0/390]\tTime 0.207 (0.207)\tData 0.136 (0.136)\tLoss 0.0785 (0.0785)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [130/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0605 (0.0500)\tTop 1-err 1.5625 (1.6544)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [130/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0335 (0.0494)\tTop 1-err 0.0000 (1.7172)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [130/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1246 (0.0540)\tTop 1-err 4.6875 (1.8574)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [130/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0515 (0.0550)\tTop 1-err 1.5625 (1.8773)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [130/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0461 (0.0531)\tTop 1-err 2.3438 (1.8177)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [130/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0549 (0.0531)\tTop 1-err 2.3438 (1.7935)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [130/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0547 (0.0537)\tTop 1-err 1.5625 (1.8007)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [130/200][0/79]\tTime 0.136 (0.136)\tLoss 0.2615 (0.2615)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [130/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1692 (0.2818)\tTop 1-err 5.4688 (8.0423)\tTop 5-err 0.0000 (0.3983)\n",
            "* Epoch: [130/200]\tTop 1-err 7.960\tTop 5-err 0.350\tTest Loss 0.281\n",
            "\n",
            "Epoch 132/200\n",
            "Epoch: [131/200][0/390]\tTime 0.199 (0.199)\tData 0.142 (0.142)\tLoss 0.0467 (0.0467)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [131/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0452 (0.0488)\tTop 1-err 2.3438 (1.7463)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [131/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0448 (0.0471)\tTop 1-err 1.5625 (1.5857)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [131/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0971 (0.0488)\tTop 1-err 4.6875 (1.5780)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [131/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0359 (0.0494)\tTop 1-err 0.7812 (1.6286)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [131/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0283 (0.0501)\tTop 1-err 0.7812 (1.6683)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [131/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0640 (0.0515)\tTop 1-err 1.5625 (1.7027)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [131/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0468 (0.0513)\tTop 1-err 1.5625 (1.6938)\tTop 5-err 0.0000 (0.0022)\n",
            "Test (on val set): [131/200][0/79]\tTime 0.135 (0.135)\tLoss 0.2391 (0.2391)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [131/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1545 (0.2706)\tTop 1-err 7.8125 (7.2610)\tTop 5-err 0.0000 (0.3676)\n",
            "* Epoch: [131/200]\tTop 1-err 7.430\tTop 5-err 0.320\tTest Loss 0.271\n",
            "\n",
            "Epoch 133/200\n",
            "Epoch: [132/200][0/390]\tTime 0.191 (0.191)\tData 0.132 (0.132)\tLoss 0.0248 (0.0248)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [132/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.0802 (0.0522)\tTop 1-err 2.3438 (1.7923)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [132/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.1139 (0.0514)\tTop 1-err 2.3438 (1.6631)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [132/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0777 (0.0506)\tTop 1-err 2.3438 (1.6349)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [132/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0704 (0.0530)\tTop 1-err 3.9062 (1.7374)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [132/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0190 (0.0570)\tTop 1-err 0.7812 (1.8831)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [132/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0637 (0.0585)\tTop 1-err 1.5625 (1.9337)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [132/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0961 (0.0587)\tTop 1-err 3.9062 (1.9498)\tTop 5-err 0.0000 (0.0067)\n",
            "Test (on val set): [132/200][0/79]\tTime 0.123 (0.123)\tLoss 0.2787 (0.2787)\tTop 1-err 7.0312 (7.0312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [132/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2382 (0.2948)\tTop 1-err 7.8125 (8.7776)\tTop 5-err 0.0000 (0.2604)\n",
            "* Epoch: [132/200]\tTop 1-err 8.720\tTop 5-err 0.300\tTest Loss 0.308\n",
            "\n",
            "Epoch 134/200\n",
            "Epoch: [133/200][0/390]\tTime 0.191 (0.191)\tData 0.132 (0.132)\tLoss 0.0281 (0.0281)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [133/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0392 (0.0514)\tTop 1-err 0.7812 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [133/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0663 (0.0500)\tTop 1-err 0.7812 (1.6863)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [133/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0473 (0.0515)\tTop 1-err 2.3438 (1.7332)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [133/200][200/390]\tTime 0.050 (0.051)\tData 0.001 (0.001)\tLoss 0.0628 (0.0529)\tTop 1-err 0.7812 (1.7452)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [133/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1493 (0.0538)\tTop 1-err 4.6875 (1.7959)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [133/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0339 (0.0539)\tTop 1-err 0.7812 (1.8065)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [133/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0548 (0.0562)\tTop 1-err 1.5625 (1.8875)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [133/200][0/79]\tTime 0.125 (0.125)\tLoss 0.2064 (0.2064)\tTop 1-err 7.0312 (7.0312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [133/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1504 (0.2591)\tTop 1-err 3.9062 (7.3223)\tTop 5-err 0.0000 (0.1838)\n",
            "* Epoch: [133/200]\tTop 1-err 7.450\tTop 5-err 0.160\tTest Loss 0.261\n",
            "\n",
            "Epoch 135/200\n",
            "Epoch: [134/200][0/390]\tTime 0.201 (0.201)\tData 0.131 (0.131)\tLoss 0.0451 (0.0451)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [134/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0360 (0.0406)\tTop 1-err 1.5625 (1.3174)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [134/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0511 (0.0434)\tTop 1-err 2.3438 (1.4078)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [134/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0912 (0.0466)\tTop 1-err 2.3438 (1.5263)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [134/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0201 (0.0477)\tTop 1-err 0.7812 (1.5392)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [134/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0590 (0.0483)\tTop 1-err 1.5625 (1.5594)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [134/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0395 (0.0493)\tTop 1-err 2.3438 (1.6014)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [134/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0648 (0.0520)\tTop 1-err 2.3438 (1.7139)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [134/200][0/79]\tTime 0.136 (0.136)\tLoss 0.3620 (0.3620)\tTop 1-err 10.1562 (10.1562)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [134/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2458 (0.3155)\tTop 1-err 8.5938 (8.8388)\tTop 5-err 0.7812 (0.2145)\n",
            "* Epoch: [134/200]\tTop 1-err 8.860\tTop 5-err 0.240\tTest Loss 0.318\n",
            "\n",
            "Epoch 136/200\n",
            "Epoch: [135/200][0/390]\tTime 0.237 (0.237)\tData 0.166 (0.166)\tLoss 0.0940 (0.0940)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [135/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.004)\tLoss 0.0936 (0.0568)\tTop 1-err 3.1250 (1.7004)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [135/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0591 (0.0545)\tTop 1-err 1.5625 (1.7249)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [135/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0468 (0.0554)\tTop 1-err 1.5625 (1.8212)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [135/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1201 (0.0571)\tTop 1-err 4.6875 (1.9084)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [135/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0532 (0.0574)\tTop 1-err 2.3438 (1.8769)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [135/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0911 (0.0566)\tTop 1-err 3.1250 (1.8610)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [135/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0537 (0.0569)\tTop 1-err 1.5625 (1.9097)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [135/200][0/79]\tTime 0.127 (0.127)\tLoss 0.2264 (0.2264)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [135/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1827 (0.2982)\tTop 1-err 5.4688 (8.0882)\tTop 5-err 0.0000 (0.2145)\n",
            "* Epoch: [135/200]\tTop 1-err 7.930\tTop 5-err 0.230\tTest Loss 0.292\n",
            "\n",
            "Epoch 137/200\n",
            "Epoch: [136/200][0/390]\tTime 0.195 (0.195)\tData 0.137 (0.137)\tLoss 0.0375 (0.0375)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [136/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0583 (0.0534)\tTop 1-err 2.3438 (1.6085)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [136/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0972 (0.0532)\tTop 1-err 2.3438 (1.6785)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [136/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0661 (0.0552)\tTop 1-err 0.7812 (1.7695)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [136/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0328 (0.0572)\tTop 1-err 0.7812 (1.8812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [136/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0445 (0.0566)\tTop 1-err 0.7812 (1.8675)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [136/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0789 (0.0573)\tTop 1-err 3.1250 (1.8792)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [136/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0862 (0.0581)\tTop 1-err 3.1250 (1.9342)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [136/200][0/79]\tTime 0.126 (0.126)\tLoss 0.2376 (0.2376)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [136/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1671 (0.2923)\tTop 1-err 3.9062 (8.1189)\tTop 5-err 0.0000 (0.2451)\n",
            "* Epoch: [136/200]\tTop 1-err 8.210\tTop 5-err 0.240\tTest Loss 0.297\n",
            "\n",
            "Epoch 138/200\n",
            "Epoch: [137/200][0/390]\tTime 0.198 (0.198)\tData 0.138 (0.138)\tLoss 0.0370 (0.0370)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [137/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0249 (0.0518)\tTop 1-err 0.7812 (1.7310)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [137/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0547 (0.0522)\tTop 1-err 1.5625 (1.7017)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [137/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0254 (0.0547)\tTop 1-err 0.0000 (1.7591)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [137/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0629 (0.0559)\tTop 1-err 3.1250 (1.8734)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [137/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0696 (0.0562)\tTop 1-err 1.5625 (1.8831)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [137/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0776 (0.0577)\tTop 1-err 3.1250 (1.9181)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [137/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0995 (0.0602)\tTop 1-err 3.1250 (1.9921)\tTop 5-err 0.7812 (0.0022)\n",
            "Test (on val set): [137/200][0/79]\tTime 0.135 (0.135)\tLoss 0.2600 (0.2600)\tTop 1-err 7.8125 (7.8125)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [137/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2882 (0.2934)\tTop 1-err 8.5938 (8.5018)\tTop 5-err 0.0000 (0.1532)\n",
            "* Epoch: [137/200]\tTop 1-err 8.550\tTop 5-err 0.200\tTest Loss 0.297\n",
            "\n",
            "Epoch 139/200\n",
            "Epoch: [138/200][0/390]\tTime 0.201 (0.201)\tData 0.132 (0.132)\tLoss 0.0638 (0.0638)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [138/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0364 (0.0521)\tTop 1-err 1.5625 (1.5012)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [138/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0329 (0.0500)\tTop 1-err 1.5625 (1.5470)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [138/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0837 (0.0505)\tTop 1-err 2.3438 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [138/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0965 (0.0522)\tTop 1-err 3.1250 (1.6402)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [138/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0529 (0.0535)\tTop 1-err 1.5625 (1.6683)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [138/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0748 (0.0559)\tTop 1-err 3.1250 (1.7779)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [138/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1881 (0.0583)\tTop 1-err 7.8125 (1.8563)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [138/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1966 (0.1966)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [138/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1750 (0.2654)\tTop 1-err 6.2500 (7.2610)\tTop 5-err 0.0000 (0.2145)\n",
            "* Epoch: [138/200]\tTop 1-err 7.290\tTop 5-err 0.230\tTest Loss 0.270\n",
            "\n",
            "Epoch 140/200\n",
            "Epoch: [139/200][0/390]\tTime 0.213 (0.213)\tData 0.142 (0.142)\tLoss 0.0233 (0.0233)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [139/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.0535 (0.0476)\tTop 1-err 1.5625 (1.7770)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [139/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0773 (0.0526)\tTop 1-err 2.3438 (1.9493)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [139/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0393 (0.0525)\tTop 1-err 2.3438 (1.8522)\tTop 5-err 0.0000 (0.0103)\n",
            "Epoch: [139/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0409 (0.0534)\tTop 1-err 1.5625 (1.8773)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [139/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0995 (0.0538)\tTop 1-err 3.1250 (1.8831)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [139/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0629 (0.0566)\tTop 1-err 3.1250 (1.9700)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [139/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0668 (0.0581)\tTop 1-err 2.3438 (2.0010)\tTop 5-err 0.0000 (0.0089)\n",
            "Test (on val set): [139/200][0/79]\tTime 0.133 (0.133)\tLoss 0.1741 (0.1741)\tTop 1-err 6.2500 (6.2500)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [139/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1922 (0.3145)\tTop 1-err 3.9062 (8.3180)\tTop 5-err 0.0000 (0.3523)\n",
            "* Epoch: [139/200]\tTop 1-err 8.460\tTop 5-err 0.310\tTest Loss 0.320\n",
            "\n",
            "Epoch 141/200\n",
            "Epoch: [140/200][0/390]\tTime 0.201 (0.201)\tData 0.134 (0.134)\tLoss 0.0648 (0.0648)\tTop 1-err 2.3438 (2.3438)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [140/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0889 (0.0653)\tTop 1-err 3.1250 (2.2672)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [140/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0322 (0.0598)\tTop 1-err 0.7812 (2.0653)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [140/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0403 (0.0563)\tTop 1-err 1.5625 (1.9712)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [140/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0453 (0.0554)\tTop 1-err 1.5625 (1.8657)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [140/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0508 (0.0560)\tTop 1-err 1.5625 (1.8955)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [140/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0869 (0.0559)\tTop 1-err 2.3438 (1.8817)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [140/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0974 (0.0565)\tTop 1-err 3.1250 (1.9320)\tTop 5-err 0.0000 (0.0022)\n",
            "Test (on val set): [140/200][0/79]\tTime 0.124 (0.124)\tLoss 0.1948 (0.1948)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [140/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2291 (0.2758)\tTop 1-err 8.5938 (7.5980)\tTop 5-err 0.0000 (0.2145)\n",
            "* Epoch: [140/200]\tTop 1-err 7.600\tTop 5-err 0.220\tTest Loss 0.273\n",
            "\n",
            "Epoch 142/200\n",
            "Epoch: [141/200][0/390]\tTime 0.201 (0.201)\tData 0.131 (0.131)\tLoss 0.0282 (0.0282)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [141/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0490 (0.0459)\tTop 1-err 1.5625 (1.4553)\tTop 5-err 0.0000 (0.0153)\n",
            "Epoch: [141/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0457 (0.0460)\tTop 1-err 2.3438 (1.4697)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [141/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0419 (0.0477)\tTop 1-err 1.5625 (1.5935)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [141/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0298 (0.0509)\tTop 1-err 0.7812 (1.6674)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [141/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0672 (0.0536)\tTop 1-err 2.3438 (1.7648)\tTop 5-err 0.0000 (0.0093)\n",
            "Epoch: [141/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1009 (0.0559)\tTop 1-err 3.9062 (1.8584)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [141/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0297 (0.0570)\tTop 1-err 0.0000 (1.8897)\tTop 5-err 0.0000 (0.0089)\n",
            "Test (on val set): [141/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1385 (0.1385)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [141/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2210 (0.3021)\tTop 1-err 5.4688 (8.5325)\tTop 5-err 0.0000 (0.3370)\n",
            "* Epoch: [141/200]\tTop 1-err 8.330\tTop 5-err 0.350\tTest Loss 0.308\n",
            "\n",
            "Epoch 143/200\n",
            "Epoch: [142/200][0/390]\tTime 0.193 (0.193)\tData 0.134 (0.134)\tLoss 0.1253 (0.1253)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [142/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0410 (0.0524)\tTop 1-err 1.5625 (1.6697)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [142/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0196 (0.0535)\tTop 1-err 0.0000 (1.8564)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [142/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0852 (0.0531)\tTop 1-err 3.1250 (1.8264)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [142/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0333 (0.0556)\tTop 1-err 0.7812 (1.9045)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [142/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0415 (0.0572)\tTop 1-err 0.7812 (1.9267)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [142/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0709 (0.0564)\tTop 1-err 2.3438 (1.8973)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [142/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0405 (0.0554)\tTop 1-err 2.3438 (1.8496)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [142/200][0/79]\tTime 0.137 (0.137)\tLoss 0.3337 (0.3337)\tTop 1-err 7.8125 (7.8125)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [142/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2655 (0.2923)\tTop 1-err 7.0312 (8.1648)\tTop 5-err 0.0000 (0.2604)\n",
            "* Epoch: [142/200]\tTop 1-err 8.070\tTop 5-err 0.220\tTest Loss 0.296\n",
            "\n",
            "Epoch 144/200\n",
            "Epoch: [143/200][0/390]\tTime 0.195 (0.195)\tData 0.136 (0.136)\tLoss 0.0406 (0.0406)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [143/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0585 (0.0500)\tTop 1-err 2.3438 (1.7004)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [143/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0201 (0.0514)\tTop 1-err 0.0000 (1.7791)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [143/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0199 (0.0505)\tTop 1-err 0.7812 (1.7591)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [143/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1055 (0.0536)\tTop 1-err 3.9062 (1.8190)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [143/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0394 (0.0563)\tTop 1-err 0.7812 (1.9142)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [143/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0966 (0.0584)\tTop 1-err 3.1250 (1.9908)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [143/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0510 (0.0592)\tTop 1-err 1.5625 (2.0321)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [143/200][0/79]\tTime 0.126 (0.126)\tLoss 0.2594 (0.2594)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [143/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2553 (0.2683)\tTop 1-err 9.3750 (7.8585)\tTop 5-err 0.0000 (0.2145)\n",
            "* Epoch: [143/200]\tTop 1-err 7.720\tTop 5-err 0.200\tTest Loss 0.266\n",
            "\n",
            "Epoch 145/200\n",
            "Epoch: [144/200][0/390]\tTime 0.193 (0.193)\tData 0.133 (0.133)\tLoss 0.0169 (0.0169)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [144/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0309 (0.0499)\tTop 1-err 0.0000 (1.5012)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [144/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0323 (0.0566)\tTop 1-err 0.0000 (1.7713)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [144/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0637 (0.0613)\tTop 1-err 2.3438 (1.9661)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [144/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1038 (0.0648)\tTop 1-err 4.6875 (2.1611)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [144/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0677 (0.0648)\tTop 1-err 0.7812 (2.1508)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [144/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0795 (0.0633)\tTop 1-err 3.9062 (2.1050)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [144/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1063 (0.0641)\tTop 1-err 3.9062 (2.1278)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [144/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1782 (0.1782)\tTop 1-err 5.4688 (5.4688)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [144/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2401 (0.2710)\tTop 1-err 8.5938 (7.6440)\tTop 5-err 0.0000 (0.2451)\n",
            "* Epoch: [144/200]\tTop 1-err 7.460\tTop 5-err 0.200\tTest Loss 0.270\n",
            "\n",
            "Epoch 146/200\n",
            "Epoch: [145/200][0/390]\tTime 0.208 (0.208)\tData 0.140 (0.140)\tLoss 0.0242 (0.0242)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [145/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0372 (0.0493)\tTop 1-err 0.7812 (1.5778)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [145/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0663 (0.0544)\tTop 1-err 3.1250 (1.7713)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [145/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0800 (0.0552)\tTop 1-err 1.5625 (1.8057)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [145/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0845 (0.0559)\tTop 1-err 3.9062 (1.8618)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [145/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0598 (0.0551)\tTop 1-err 0.7812 (1.8240)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [145/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0483 (0.0562)\tTop 1-err 2.3438 (1.8947)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [145/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0598 (0.0573)\tTop 1-err 1.5625 (1.9298)\tTop 5-err 0.0000 (0.0067)\n",
            "Test (on val set): [145/200][0/79]\tTime 0.128 (0.128)\tLoss 0.1649 (0.1649)\tTop 1-err 7.0312 (7.0312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [145/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1447 (0.2868)\tTop 1-err 6.2500 (7.9044)\tTop 5-err 0.0000 (0.2911)\n",
            "* Epoch: [145/200]\tTop 1-err 7.950\tTop 5-err 0.270\tTest Loss 0.285\n",
            "\n",
            "Epoch 147/200\n",
            "Epoch: [146/200][0/390]\tTime 0.206 (0.206)\tData 0.135 (0.135)\tLoss 0.0485 (0.0485)\tTop 1-err 1.5625 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [146/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0796 (0.0567)\tTop 1-err 3.1250 (1.9608)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [146/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0591 (0.0519)\tTop 1-err 2.3438 (1.7868)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [146/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0418 (0.0506)\tTop 1-err 1.5625 (1.7074)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [146/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0526 (0.0508)\tTop 1-err 2.3438 (1.7296)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [146/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0550 (0.0513)\tTop 1-err 2.3438 (1.7181)\tTop 5-err 0.0000 (0.0031)\n",
            "Epoch: [146/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0313 (0.0532)\tTop 1-err 0.7812 (1.7935)\tTop 5-err 0.0000 (0.0026)\n",
            "Epoch: [146/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0885 (0.0559)\tTop 1-err 3.1250 (1.9030)\tTop 5-err 0.0000 (0.0045)\n",
            "Test (on val set): [146/200][0/79]\tTime 0.134 (0.134)\tLoss 0.1720 (0.1720)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [146/200][50/79]\tTime 0.016 (0.018)\tLoss 0.2097 (0.2988)\tTop 1-err 7.8125 (8.3180)\tTop 5-err 0.0000 (0.1991)\n",
            "* Epoch: [146/200]\tTop 1-err 8.330\tTop 5-err 0.170\tTest Loss 0.293\n",
            "\n",
            "Epoch 148/200\n",
            "Epoch: [147/200][0/390]\tTime 0.193 (0.193)\tData 0.132 (0.132)\tLoss 0.0509 (0.0509)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [147/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.1071 (0.0610)\tTop 1-err 4.6875 (2.0833)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [147/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0344 (0.0601)\tTop 1-err 1.5625 (2.0266)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [147/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0783 (0.0586)\tTop 1-err 1.5625 (2.0281)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [147/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1143 (0.0564)\tTop 1-err 2.3438 (1.9317)\tTop 5-err 0.0000 (0.0039)\n",
            "Epoch: [147/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0446 (0.0556)\tTop 1-err 0.7812 (1.9142)\tTop 5-err 0.0000 (0.0062)\n",
            "Epoch: [147/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0461 (0.0550)\tTop 1-err 2.3438 (1.8636)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [147/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0255 (0.0544)\tTop 1-err 0.7812 (1.8140)\tTop 5-err 0.0000 (0.0067)\n",
            "Test (on val set): [147/200][0/79]\tTime 0.127 (0.127)\tLoss 0.2516 (0.2516)\tTop 1-err 9.3750 (9.3750)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [147/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1830 (0.3309)\tTop 1-err 7.0312 (9.2525)\tTop 5-err 0.0000 (0.1532)\n",
            "* Epoch: [147/200]\tTop 1-err 9.110\tTop 5-err 0.160\tTest Loss 0.323\n",
            "\n",
            "Epoch 149/200\n",
            "Epoch: [148/200][0/390]\tTime 0.189 (0.189)\tData 0.131 (0.131)\tLoss 0.0382 (0.0382)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [148/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0348 (0.0561)\tTop 1-err 1.5625 (1.8689)\tTop 5-err 0.0000 (0.0153)\n",
            "Epoch: [148/200][100/390]\tTime 0.057 (0.051)\tData 0.000 (0.002)\tLoss 0.0277 (0.0544)\tTop 1-err 0.7812 (1.8874)\tTop 5-err 0.0000 (0.0077)\n",
            "Epoch: [148/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0370 (0.0556)\tTop 1-err 1.5625 (1.9505)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [148/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0237 (0.0577)\tTop 1-err 0.7812 (2.0717)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [148/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.1654 (0.0604)\tTop 1-err 6.2500 (2.0761)\tTop 5-err 0.0000 (0.0093)\n",
            "Epoch: [148/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0401 (0.0631)\tTop 1-err 0.7812 (2.1724)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [148/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0618 (0.0630)\tTop 1-err 1.5625 (2.1879)\tTop 5-err 0.0000 (0.0089)\n",
            "Test (on val set): [148/200][0/79]\tTime 0.125 (0.125)\tLoss 0.1864 (0.1864)\tTop 1-err 7.0312 (7.0312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [148/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1662 (0.2897)\tTop 1-err 4.6875 (7.8585)\tTop 5-err 0.0000 (0.2757)\n",
            "* Epoch: [148/200]\tTop 1-err 7.930\tTop 5-err 0.240\tTest Loss 0.291\n",
            "\n",
            "Epoch 150/200\n",
            "Epoch: [149/200][0/390]\tTime 0.206 (0.206)\tData 0.134 (0.134)\tLoss 0.1100 (0.1100)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [149/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0183 (0.0492)\tTop 1-err 0.7812 (1.6544)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [149/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0155 (0.0483)\tTop 1-err 0.0000 (1.7481)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [149/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0915 (0.0520)\tTop 1-err 2.3438 (1.8160)\tTop 5-err 0.0000 (0.0052)\n",
            "Epoch: [149/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0266 (0.0545)\tTop 1-err 0.7812 (1.9279)\tTop 5-err 0.0000 (0.0078)\n",
            "Epoch: [149/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0426 (0.0563)\tTop 1-err 1.5625 (1.9983)\tTop 5-err 0.0000 (0.0093)\n",
            "Epoch: [149/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0649 (0.0595)\tTop 1-err 3.1250 (2.0894)\tTop 5-err 0.0000 (0.0104)\n",
            "Epoch: [149/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.1330 (0.0617)\tTop 1-err 4.6875 (2.1523)\tTop 5-err 0.0000 (0.0089)\n",
            "Test (on val set): [149/200][0/79]\tTime 0.135 (0.135)\tLoss 0.1969 (0.1969)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [149/200][50/79]\tTime 0.016 (0.019)\tLoss 0.2734 (0.3346)\tTop 1-err 7.8125 (9.2678)\tTop 5-err 0.0000 (0.3983)\n",
            "* Epoch: [149/200]\tTop 1-err 9.180\tTop 5-err 0.320\tTest Loss 0.331\n",
            "\n",
            "Epoch 151/200\n",
            "Epoch: [150/200][0/390]\tTime 0.208 (0.208)\tData 0.132 (0.132)\tLoss 0.0449 (0.0449)\tTop 1-err 1.5625 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [150/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0701 (0.0486)\tTop 1-err 1.5625 (1.5625)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [150/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0369 (0.0428)\tTop 1-err 0.7812 (1.3691)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [150/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0359 (0.0371)\tTop 1-err 1.5625 (1.1434)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [150/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0143 (0.0349)\tTop 1-err 0.0000 (1.0456)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [150/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0285 (0.0327)\tTop 1-err 0.7812 (0.9680)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [150/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0111 (0.0311)\tTop 1-err 0.7812 (0.9084)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [150/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0114 (0.0299)\tTop 1-err 0.7812 (0.8703)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [150/200][0/79]\tTime 0.129 (0.129)\tLoss 0.1225 (0.1225)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [150/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1491 (0.2085)\tTop 1-err 7.0312 (6.1121)\tTop 5-err 0.0000 (0.1532)\n",
            "* Epoch: [150/200]\tTop 1-err 6.060\tTop 5-err 0.130\tTest Loss 0.211\n",
            "\n",
            "Epoch 152/200\n",
            "Epoch: [151/200][0/390]\tTime 0.192 (0.192)\tData 0.132 (0.132)\tLoss 0.0040 (0.0040)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [151/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0144 (0.0164)\tTop 1-err 0.0000 (0.3676)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [151/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0092 (0.0167)\tTop 1-err 0.0000 (0.3481)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [151/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0079 (0.0172)\tTop 1-err 0.0000 (0.3466)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [151/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0045 (0.0171)\tTop 1-err 0.0000 (0.3459)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [151/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0139 (0.0171)\tTop 1-err 0.7812 (0.3735)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [151/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0079 (0.0168)\tTop 1-err 0.0000 (0.3738)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [151/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0214 (0.0165)\tTop 1-err 0.0000 (0.3695)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [151/200][0/79]\tTime 0.133 (0.133)\tLoss 0.1066 (0.1066)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [151/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1376 (0.2028)\tTop 1-err 7.0312 (5.7445)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [151/200]\tTop 1-err 5.840\tTop 5-err 0.100\tTest Loss 0.209\n",
            "\n",
            "Epoch 153/200\n",
            "Epoch: [152/200][0/390]\tTime 0.203 (0.203)\tData 0.132 (0.132)\tLoss 0.0228 (0.0228)\tTop 1-err 0.7812 (0.7812)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [152/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0132 (0.0146)\tTop 1-err 0.0000 (0.3523)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [152/200][100/390]\tTime 0.052 (0.052)\tData 0.000 (0.002)\tLoss 0.0051 (0.0140)\tTop 1-err 0.0000 (0.3326)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [152/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0209 (0.0138)\tTop 1-err 1.5625 (0.3311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [152/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0206 (0.0138)\tTop 1-err 0.0000 (0.3187)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [152/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0287 (0.0136)\tTop 1-err 0.7812 (0.3050)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [152/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0246 (0.0132)\tTop 1-err 0.7812 (0.2881)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [152/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0089 (0.0129)\tTop 1-err 0.0000 (0.2782)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [152/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1310 (0.1310)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [152/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1213 (0.2039)\tTop 1-err 5.4688 (5.7751)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [152/200]\tTop 1-err 5.860\tTop 5-err 0.130\tTest Loss 0.209\n",
            "\n",
            "Epoch 154/200\n",
            "Epoch: [153/200][0/390]\tTime 0.203 (0.203)\tData 0.137 (0.137)\tLoss 0.0147 (0.0147)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [153/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0044 (0.0132)\tTop 1-err 0.0000 (0.3217)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [153/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0089 (0.0135)\tTop 1-err 0.0000 (0.3481)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [153/200][150/390]\tTime 0.053 (0.051)\tData 0.000 (0.001)\tLoss 0.0071 (0.0127)\tTop 1-err 0.0000 (0.3053)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [153/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0117 (0.0121)\tTop 1-err 0.0000 (0.2799)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [153/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0062 (0.0114)\tTop 1-err 0.0000 (0.2459)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [153/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0104 (0.0112)\tTop 1-err 0.0000 (0.2362)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [153/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0062 (0.0112)\tTop 1-err 0.0000 (0.2337)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [153/200][0/79]\tTime 0.143 (0.143)\tLoss 0.1073 (0.1073)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [153/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1014 (0.2009)\tTop 1-err 4.6875 (5.6373)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [153/200]\tTop 1-err 5.780\tTop 5-err 0.110\tTest Loss 0.206\n",
            "\n",
            "Epoch 155/200\n",
            "Epoch: [154/200][0/390]\tTime 0.204 (0.204)\tData 0.140 (0.140)\tLoss 0.0054 (0.0054)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [154/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0288 (0.0095)\tTop 1-err 0.7812 (0.2145)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [154/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0122 (0.0096)\tTop 1-err 0.0000 (0.2011)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [154/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0044 (0.0091)\tTop 1-err 0.0000 (0.1811)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [154/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0058 (0.0092)\tTop 1-err 0.0000 (0.1749)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [154/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0126 (0.0090)\tTop 1-err 0.0000 (0.1681)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [154/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0049 (0.0089)\tTop 1-err 0.0000 (0.1557)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [154/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0039 (0.0089)\tTop 1-err 0.0000 (0.1580)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [154/200][0/79]\tTime 0.123 (0.123)\tLoss 0.1086 (0.1086)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [154/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0945 (0.2019)\tTop 1-err 4.6875 (5.6066)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [154/200]\tTop 1-err 5.720\tTop 5-err 0.120\tTest Loss 0.208\n",
            "\n",
            "Epoch 156/200\n",
            "Epoch: [155/200][0/390]\tTime 0.203 (0.203)\tData 0.140 (0.140)\tLoss 0.0039 (0.0039)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [155/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0016 (0.0084)\tTop 1-err 0.0000 (0.1991)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [155/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0143 (0.0083)\tTop 1-err 0.0000 (0.1392)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [155/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0076 (0.0087)\tTop 1-err 0.0000 (0.1449)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [155/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0110 (0.0083)\tTop 1-err 0.0000 (0.1322)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [155/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0086 (0.0083)\tTop 1-err 0.0000 (0.1307)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [155/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0081 (0.0083)\tTop 1-err 0.0000 (0.1350)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [155/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0036 (0.0082)\tTop 1-err 0.0000 (0.1380)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [155/200][0/79]\tTime 0.122 (0.122)\tLoss 0.1031 (0.1031)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [155/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0905 (0.2023)\tTop 1-err 3.1250 (5.4381)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [155/200]\tTop 1-err 5.580\tTop 5-err 0.110\tTest Loss 0.207\n",
            "\n",
            "Epoch 157/200\n",
            "Epoch: [156/200][0/390]\tTime 0.191 (0.191)\tData 0.132 (0.132)\tLoss 0.0049 (0.0049)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [156/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0106 (0.0070)\tTop 1-err 0.7812 (0.1225)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [156/200][100/390]\tTime 0.056 (0.052)\tData 0.000 (0.002)\tLoss 0.0029 (0.0064)\tTop 1-err 0.0000 (0.0928)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [156/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0048 (0.0065)\tTop 1-err 0.0000 (0.0983)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [156/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0066 (0.0065)\tTop 1-err 0.0000 (0.0894)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [156/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0079 (0.0067)\tTop 1-err 0.7812 (0.0965)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [156/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0020 (0.0067)\tTop 1-err 0.0000 (0.0986)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [156/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0057 (0.0069)\tTop 1-err 0.0000 (0.1135)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [156/200][0/79]\tTime 0.132 (0.132)\tLoss 0.1050 (0.1050)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [156/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0984 (0.2016)\tTop 1-err 3.9062 (5.5453)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [156/200]\tTop 1-err 5.570\tTop 5-err 0.120\tTest Loss 0.207\n",
            "\n",
            "Epoch 158/200\n",
            "Epoch: [157/200][0/390]\tTime 0.199 (0.199)\tData 0.140 (0.140)\tLoss 0.0039 (0.0039)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [157/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0138 (0.0076)\tTop 1-err 0.7812 (0.1532)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [157/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0044 (0.0075)\tTop 1-err 0.0000 (0.1392)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [157/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0031 (0.0081)\tTop 1-err 0.0000 (0.1604)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [157/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0046 (0.0081)\tTop 1-err 0.0000 (0.1516)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [157/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0229 (0.0080)\tTop 1-err 0.7812 (0.1556)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [157/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0031 (0.0078)\tTop 1-err 0.0000 (0.1505)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [157/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0050 (0.0080)\tTop 1-err 0.0000 (0.1514)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [157/200][0/79]\tTime 0.142 (0.142)\tLoss 0.1100 (0.1100)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [157/200][50/79]\tTime 0.016 (0.019)\tLoss 0.0893 (0.2061)\tTop 1-err 4.6875 (5.6219)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [157/200]\tTop 1-err 5.640\tTop 5-err 0.120\tTest Loss 0.210\n",
            "\n",
            "Epoch 159/200\n",
            "Epoch: [158/200][0/390]\tTime 0.209 (0.209)\tData 0.139 (0.139)\tLoss 0.0062 (0.0062)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [158/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.0029 (0.0065)\tTop 1-err 0.0000 (0.0919)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [158/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0021 (0.0060)\tTop 1-err 0.0000 (0.0851)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [158/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0023 (0.0058)\tTop 1-err 0.0000 (0.0776)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [158/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0036 (0.0060)\tTop 1-err 0.0000 (0.0816)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [158/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0185 (0.0061)\tTop 1-err 0.7812 (0.0903)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [158/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0040 (0.0061)\tTop 1-err 0.0000 (0.1064)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [158/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0047 (0.0063)\tTop 1-err 0.0000 (0.1091)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [158/200][0/79]\tTime 0.123 (0.123)\tLoss 0.1227 (0.1227)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [158/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0921 (0.2037)\tTop 1-err 3.9062 (5.4534)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [158/200]\tTop 1-err 5.470\tTop 5-err 0.130\tTest Loss 0.207\n",
            "\n",
            "Epoch 160/200\n",
            "Epoch: [159/200][0/390]\tTime 0.205 (0.205)\tData 0.132 (0.132)\tLoss 0.0101 (0.0101)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [159/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0049 (0.0067)\tTop 1-err 0.0000 (0.1379)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [159/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0067 (0.0067)\tTop 1-err 0.0000 (0.1315)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [159/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0041 (0.0064)\tTop 1-err 0.0000 (0.1138)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [159/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0016 (0.0062)\tTop 1-err 0.0000 (0.1049)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [159/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0045 (0.0063)\tTop 1-err 0.0000 (0.1152)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [159/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0037 (0.0062)\tTop 1-err 0.0000 (0.1064)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [159/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0021 (0.0063)\tTop 1-err 0.0000 (0.1157)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [159/200][0/79]\tTime 0.123 (0.123)\tLoss 0.1368 (0.1368)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [159/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0909 (0.2038)\tTop 1-err 3.9062 (5.5147)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [159/200]\tTop 1-err 5.530\tTop 5-err 0.140\tTest Loss 0.209\n",
            "\n",
            "Epoch 161/200\n",
            "Epoch: [160/200][0/390]\tTime 0.205 (0.205)\tData 0.135 (0.135)\tLoss 0.0034 (0.0034)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [160/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0025 (0.0063)\tTop 1-err 0.0000 (0.1225)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [160/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0058 (0.0068)\tTop 1-err 0.0000 (0.1315)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [160/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0041 (0.0065)\tTop 1-err 0.0000 (0.1138)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [160/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0028 (0.0062)\tTop 1-err 0.0000 (0.1011)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [160/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0018 (0.0061)\tTop 1-err 0.0000 (0.0965)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [160/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0105 (0.0061)\tTop 1-err 0.0000 (0.0960)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [160/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0021 (0.0060)\tTop 1-err 0.0000 (0.0957)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [160/200][0/79]\tTime 0.121 (0.121)\tLoss 0.1243 (0.1243)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [160/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0995 (0.2072)\tTop 1-err 4.6875 (5.4228)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [160/200]\tTop 1-err 5.460\tTop 5-err 0.130\tTest Loss 0.211\n",
            "\n",
            "Epoch 162/200\n",
            "Epoch: [161/200][0/390]\tTime 0.194 (0.194)\tData 0.134 (0.134)\tLoss 0.0011 (0.0011)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [161/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0159 (0.0051)\tTop 1-err 0.7812 (0.0766)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [161/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0115 (0.0054)\tTop 1-err 0.7812 (0.1006)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [161/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0108 (0.0054)\tTop 1-err 0.7812 (0.0880)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [161/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0041 (0.0053)\tTop 1-err 0.0000 (0.0816)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [161/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0026 (0.0052)\tTop 1-err 0.0000 (0.0747)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [161/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0060 (0.0052)\tTop 1-err 0.0000 (0.0805)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [161/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0030 (0.0052)\tTop 1-err 0.0000 (0.0735)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [161/200][0/79]\tTime 0.134 (0.134)\tLoss 0.1291 (0.1291)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [161/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0918 (0.2047)\tTop 1-err 4.6875 (5.5607)\tTop 5-err 0.0000 (0.1532)\n",
            "* Epoch: [161/200]\tTop 1-err 5.560\tTop 5-err 0.160\tTest Loss 0.207\n",
            "\n",
            "Epoch 163/200\n",
            "Epoch: [162/200][0/390]\tTime 0.200 (0.200)\tData 0.142 (0.142)\tLoss 0.0040 (0.0040)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [162/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0045 (0.0042)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [162/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0027 (0.0045)\tTop 1-err 0.0000 (0.0387)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [162/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0235 (0.0046)\tTop 1-err 0.7812 (0.0466)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [162/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0129 (0.0048)\tTop 1-err 0.7812 (0.0622)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [162/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0043 (0.0050)\tTop 1-err 0.0000 (0.0778)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [162/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0028 (0.0050)\tTop 1-err 0.0000 (0.0779)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [162/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0077 (0.0050)\tTop 1-err 0.0000 (0.0779)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [162/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1210 (0.1210)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [162/200][50/79]\tTime 0.017 (0.018)\tLoss 0.0999 (0.2074)\tTop 1-err 4.6875 (5.5607)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [162/200]\tTop 1-err 5.540\tTop 5-err 0.120\tTest Loss 0.211\n",
            "\n",
            "Epoch 164/200\n",
            "Epoch: [163/200][0/390]\tTime 0.192 (0.192)\tData 0.131 (0.131)\tLoss 0.0034 (0.0034)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [163/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0028 (0.0056)\tTop 1-err 0.0000 (0.0613)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [163/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0016 (0.0052)\tTop 1-err 0.0000 (0.0619)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [163/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0015 (0.0050)\tTop 1-err 0.0000 (0.0673)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [163/200][200/390]\tTime 0.056 (0.051)\tData 0.000 (0.001)\tLoss 0.0041 (0.0051)\tTop 1-err 0.0000 (0.0700)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [163/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0051)\tTop 1-err 0.0000 (0.0809)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [163/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0087 (0.0051)\tTop 1-err 0.0000 (0.0753)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [163/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0028 (0.0051)\tTop 1-err 0.0000 (0.0757)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [163/200][0/79]\tTime 0.125 (0.125)\tLoss 0.1311 (0.1311)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [163/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0913 (0.2081)\tTop 1-err 3.9062 (5.3768)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [163/200]\tTop 1-err 5.510\tTop 5-err 0.140\tTest Loss 0.212\n",
            "\n",
            "Epoch 165/200\n",
            "Epoch: [164/200][0/390]\tTime 0.210 (0.210)\tData 0.136 (0.136)\tLoss 0.0019 (0.0019)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [164/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0035 (0.0042)\tTop 1-err 0.0000 (0.0460)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [164/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0047 (0.0041)\tTop 1-err 0.0000 (0.0309)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [164/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0021 (0.0041)\tTop 1-err 0.0000 (0.0310)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [164/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0069 (0.0043)\tTop 1-err 0.0000 (0.0428)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [164/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0041 (0.0042)\tTop 1-err 0.0000 (0.0374)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [164/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0035 (0.0043)\tTop 1-err 0.0000 (0.0441)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [164/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0079 (0.0044)\tTop 1-err 0.0000 (0.0445)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [164/200][0/79]\tTime 0.138 (0.138)\tLoss 0.1286 (0.1286)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [164/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1040 (0.2069)\tTop 1-err 4.6875 (5.3002)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [164/200]\tTop 1-err 5.380\tTop 5-err 0.130\tTest Loss 0.211\n",
            "\n",
            "Epoch 166/200\n",
            "Epoch: [165/200][0/390]\tTime 0.214 (0.214)\tData 0.145 (0.145)\tLoss 0.0024 (0.0024)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [165/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0038 (0.0044)\tTop 1-err 0.0000 (0.0766)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [165/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0062 (0.0045)\tTop 1-err 0.0000 (0.0851)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [165/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0017 (0.0045)\tTop 1-err 0.0000 (0.0673)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [165/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0021 (0.0045)\tTop 1-err 0.0000 (0.0661)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [165/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0030 (0.0044)\tTop 1-err 0.0000 (0.0623)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [165/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0034 (0.0045)\tTop 1-err 0.0000 (0.0571)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [165/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0024 (0.0046)\tTop 1-err 0.0000 (0.0601)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [165/200][0/79]\tTime 0.122 (0.122)\tLoss 0.1407 (0.1407)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [165/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0935 (0.2063)\tTop 1-err 3.9062 (5.2237)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [165/200]\tTop 1-err 5.370\tTop 5-err 0.140\tTest Loss 0.211\n",
            "\n",
            "Epoch 167/200\n",
            "Epoch: [166/200][0/390]\tTime 0.198 (0.198)\tData 0.139 (0.139)\tLoss 0.0101 (0.0101)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [166/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0067 (0.0049)\tTop 1-err 0.0000 (0.0613)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [166/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0132 (0.0045)\tTop 1-err 0.7812 (0.0619)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [166/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0035 (0.0045)\tTop 1-err 0.0000 (0.0880)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [166/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0012 (0.0045)\tTop 1-err 0.0000 (0.0777)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [166/200][250/390]\tTime 0.052 (0.051)\tData 0.000 (0.001)\tLoss 0.0015 (0.0049)\tTop 1-err 0.0000 (0.0934)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [166/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0012 (0.0047)\tTop 1-err 0.0000 (0.0857)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [166/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0139 (0.0048)\tTop 1-err 0.0000 (0.0868)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [166/200][0/79]\tTime 0.136 (0.136)\tLoss 0.1414 (0.1414)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [166/200][50/79]\tTime 0.016 (0.019)\tLoss 0.1022 (0.2073)\tTop 1-err 3.9062 (5.3309)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [166/200]\tTop 1-err 5.460\tTop 5-err 0.130\tTest Loss 0.211\n",
            "\n",
            "Epoch 168/200\n",
            "Epoch: [167/200][0/390]\tTime 0.199 (0.199)\tData 0.136 (0.136)\tLoss 0.0021 (0.0021)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [167/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0428 (0.0054)\tTop 1-err 1.5625 (0.0766)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [167/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0040 (0.0049)\tTop 1-err 0.0000 (0.0774)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [167/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0019 (0.0050)\tTop 1-err 0.0000 (0.0828)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [167/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0046 (0.0048)\tTop 1-err 0.0000 (0.0777)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [167/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0021 (0.0047)\tTop 1-err 0.0000 (0.0778)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [167/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0028 (0.0047)\tTop 1-err 0.0000 (0.0779)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [167/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0041 (0.0046)\tTop 1-err 0.0000 (0.0779)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [167/200][0/79]\tTime 0.129 (0.129)\tLoss 0.1181 (0.1181)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [167/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0928 (0.2044)\tTop 1-err 4.6875 (5.2696)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [167/200]\tTop 1-err 5.390\tTop 5-err 0.130\tTest Loss 0.209\n",
            "\n",
            "Epoch 169/200\n",
            "Epoch: [168/200][0/390]\tTime 0.208 (0.208)\tData 0.136 (0.136)\tLoss 0.0021 (0.0021)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [168/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0104 (0.0048)\tTop 1-err 0.7812 (0.0460)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [168/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0031 (0.0046)\tTop 1-err 0.0000 (0.0464)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [168/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0023 (0.0044)\tTop 1-err 0.0000 (0.0517)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [168/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0038 (0.0046)\tTop 1-err 0.0000 (0.0700)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [168/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0044)\tTop 1-err 0.0000 (0.0623)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [168/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0045 (0.0045)\tTop 1-err 0.0000 (0.0649)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [168/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0012 (0.0045)\tTop 1-err 0.0000 (0.0712)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [168/200][0/79]\tTime 0.154 (0.154)\tLoss 0.1247 (0.1247)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [168/200][50/79]\tTime 0.017 (0.019)\tLoss 0.0993 (0.2058)\tTop 1-err 3.9062 (5.1930)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [168/200]\tTop 1-err 5.320\tTop 5-err 0.150\tTest Loss 0.209\n",
            "\n",
            "Epoch 170/200\n",
            "Epoch: [169/200][0/390]\tTime 0.202 (0.202)\tData 0.143 (0.143)\tLoss 0.0056 (0.0056)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [169/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0028 (0.0035)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [169/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0020 (0.0042)\tTop 1-err 0.0000 (0.0696)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [169/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0028 (0.0040)\tTop 1-err 0.0000 (0.0621)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [169/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0019 (0.0042)\tTop 1-err 0.0000 (0.0661)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [169/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0010 (0.0041)\tTop 1-err 0.0000 (0.0560)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [169/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0010 (0.0040)\tTop 1-err 0.0000 (0.0545)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [169/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0021 (0.0040)\tTop 1-err 0.0000 (0.0556)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [169/200][0/79]\tTime 0.129 (0.129)\tLoss 0.1465 (0.1465)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [169/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1061 (0.2086)\tTop 1-err 4.6875 (5.3922)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [169/200]\tTop 1-err 5.360\tTop 5-err 0.140\tTest Loss 0.213\n",
            "\n",
            "Epoch 171/200\n",
            "Epoch: [170/200][0/390]\tTime 0.200 (0.200)\tData 0.140 (0.140)\tLoss 0.0037 (0.0037)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [170/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0020 (0.0042)\tTop 1-err 0.0000 (0.0460)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [170/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0013 (0.0039)\tTop 1-err 0.0000 (0.0464)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [170/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0009 (0.0038)\tTop 1-err 0.0000 (0.0414)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [170/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0022 (0.0039)\tTop 1-err 0.0000 (0.0544)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [170/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0047 (0.0039)\tTop 1-err 0.0000 (0.0529)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [170/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0021 (0.0037)\tTop 1-err 0.0000 (0.0441)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [170/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0057 (0.0037)\tTop 1-err 0.0000 (0.0445)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [170/200][0/79]\tTime 0.132 (0.132)\tLoss 0.1320 (0.1320)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [170/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1146 (0.2064)\tTop 1-err 3.9062 (5.1164)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [170/200]\tTop 1-err 5.290\tTop 5-err 0.150\tTest Loss 0.211\n",
            "\n",
            "Epoch 172/200\n",
            "Epoch: [171/200][0/390]\tTime 0.207 (0.207)\tData 0.134 (0.134)\tLoss 0.0012 (0.0012)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [171/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0069 (0.0035)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [171/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0041 (0.0037)\tTop 1-err 0.0000 (0.0541)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [171/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0024 (0.0038)\tTop 1-err 0.0000 (0.0569)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [171/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0017 (0.0036)\tTop 1-err 0.0000 (0.0466)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [171/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0036)\tTop 1-err 0.0000 (0.0529)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [171/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0058 (0.0036)\tTop 1-err 0.0000 (0.0571)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [171/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0029 (0.0038)\tTop 1-err 0.0000 (0.0579)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [171/200][0/79]\tTime 0.127 (0.127)\tLoss 0.1343 (0.1343)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [171/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0994 (0.2074)\tTop 1-err 4.6875 (5.1777)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [171/200]\tTop 1-err 5.290\tTop 5-err 0.140\tTest Loss 0.212\n",
            "\n",
            "Epoch 173/200\n",
            "Epoch: [172/200][0/390]\tTime 0.214 (0.214)\tData 0.144 (0.144)\tLoss 0.0022 (0.0022)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [172/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0019 (0.0038)\tTop 1-err 0.0000 (0.0613)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [172/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0094 (0.0038)\tTop 1-err 0.0000 (0.0464)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [172/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0023 (0.0040)\tTop 1-err 0.0000 (0.0466)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [172/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0022 (0.0039)\tTop 1-err 0.0000 (0.0389)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [172/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0034 (0.0039)\tTop 1-err 0.0000 (0.0498)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [172/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0010 (0.0039)\tTop 1-err 0.0000 (0.0493)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [172/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0008 (0.0039)\tTop 1-err 0.0000 (0.0512)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [172/200][0/79]\tTime 0.140 (0.140)\tLoss 0.1323 (0.1323)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [172/200][50/79]\tTime 0.016 (0.019)\tLoss 0.0900 (0.2063)\tTop 1-err 5.4688 (5.1777)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [172/200]\tTop 1-err 5.340\tTop 5-err 0.160\tTest Loss 0.211\n",
            "\n",
            "Epoch 174/200\n",
            "Epoch: [173/200][0/390]\tTime 0.206 (0.206)\tData 0.133 (0.133)\tLoss 0.0014 (0.0014)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [173/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0016 (0.0034)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [173/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0016 (0.0032)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [173/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0024 (0.0034)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [173/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0047 (0.0036)\tTop 1-err 0.0000 (0.0311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [173/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0040 (0.0036)\tTop 1-err 0.0000 (0.0374)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [173/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0009 (0.0036)\tTop 1-err 0.0000 (0.0415)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [173/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0013 (0.0036)\tTop 1-err 0.0000 (0.0445)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [173/200][0/79]\tTime 0.129 (0.129)\tLoss 0.1328 (0.1328)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [173/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0854 (0.2060)\tTop 1-err 4.6875 (5.1471)\tTop 5-err 0.0000 (0.0766)\n",
            "* Epoch: [173/200]\tTop 1-err 5.330\tTop 5-err 0.120\tTest Loss 0.212\n",
            "\n",
            "Epoch 175/200\n",
            "Epoch: [174/200][0/390]\tTime 0.193 (0.193)\tData 0.133 (0.133)\tLoss 0.0054 (0.0054)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [174/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0008 (0.0038)\tTop 1-err 0.0000 (0.0766)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [174/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0037 (0.0039)\tTop 1-err 0.0000 (0.0851)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [174/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0016 (0.0037)\tTop 1-err 0.0000 (0.0673)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [174/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0012 (0.0037)\tTop 1-err 0.0000 (0.0622)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [174/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0033 (0.0039)\tTop 1-err 0.0000 (0.0716)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [174/200][300/390]\tTime 0.053 (0.051)\tData 0.000 (0.001)\tLoss 0.0253 (0.0038)\tTop 1-err 0.7812 (0.0701)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [174/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0039)\tTop 1-err 0.0000 (0.0712)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [174/200][0/79]\tTime 0.127 (0.127)\tLoss 0.1216 (0.1216)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [174/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0841 (0.2090)\tTop 1-err 4.6875 (5.1930)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [174/200]\tTop 1-err 5.310\tTop 5-err 0.120\tTest Loss 0.214\n",
            "\n",
            "Epoch 176/200\n",
            "Epoch: [175/200][0/390]\tTime 0.196 (0.196)\tData 0.136 (0.136)\tLoss 0.0013 (0.0013)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [175/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0033 (0.0037)\tTop 1-err 0.0000 (0.0613)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [175/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0008 (0.0032)\tTop 1-err 0.0000 (0.0387)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [175/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0057 (0.0032)\tTop 1-err 0.0000 (0.0310)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [175/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0031 (0.0038)\tTop 1-err 0.0000 (0.0466)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [175/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0104 (0.0038)\tTop 1-err 0.0000 (0.0436)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [175/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0031 (0.0038)\tTop 1-err 0.0000 (0.0467)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [175/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0041 (0.0038)\tTop 1-err 0.0000 (0.0467)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [175/200][0/79]\tTime 0.125 (0.125)\tLoss 0.1358 (0.1358)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [175/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0806 (0.2078)\tTop 1-err 3.1250 (5.0705)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [175/200]\tTop 1-err 5.300\tTop 5-err 0.110\tTest Loss 0.211\n",
            "\n",
            "Epoch 177/200\n",
            "Epoch: [176/200][0/390]\tTime 0.202 (0.202)\tData 0.132 (0.132)\tLoss 0.0016 (0.0016)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [176/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0014 (0.0028)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [176/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0052 (0.0031)\tTop 1-err 0.0000 (0.0309)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [176/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0031)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [176/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0061 (0.0032)\tTop 1-err 0.0000 (0.0272)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [176/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0017 (0.0034)\tTop 1-err 0.0000 (0.0311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [176/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0239 (0.0034)\tTop 1-err 0.7812 (0.0363)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [176/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0015 (0.0035)\tTop 1-err 0.0000 (0.0423)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [176/200][0/79]\tTime 0.129 (0.129)\tLoss 0.1370 (0.1370)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [176/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0944 (0.2110)\tTop 1-err 3.9062 (5.3156)\tTop 5-err 0.0000 (0.1532)\n",
            "* Epoch: [176/200]\tTop 1-err 5.430\tTop 5-err 0.150\tTest Loss 0.215\n",
            "\n",
            "Epoch 178/200\n",
            "Epoch: [177/200][0/390]\tTime 0.207 (0.207)\tData 0.135 (0.135)\tLoss 0.0012 (0.0012)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [177/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.0140 (0.0031)\tTop 1-err 0.7812 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [177/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0022 (0.0034)\tTop 1-err 0.0000 (0.0309)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [177/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0030 (0.0035)\tTop 1-err 0.0000 (0.0414)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [177/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0043 (0.0034)\tTop 1-err 0.0000 (0.0389)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [177/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0034)\tTop 1-err 0.0000 (0.0405)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [177/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0034 (0.0035)\tTop 1-err 0.0000 (0.0441)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [177/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0029 (0.0034)\tTop 1-err 0.0000 (0.0401)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [177/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1127 (0.1127)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [177/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0911 (0.2082)\tTop 1-err 4.6875 (5.2543)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [177/200]\tTop 1-err 5.380\tTop 5-err 0.120\tTest Loss 0.213\n",
            "\n",
            "Epoch 179/200\n",
            "Epoch: [178/200][0/390]\tTime 0.199 (0.199)\tData 0.139 (0.139)\tLoss 0.0042 (0.0042)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [178/200][50/390]\tTime 0.050 (0.055)\tData 0.000 (0.003)\tLoss 0.0021 (0.0036)\tTop 1-err 0.0000 (0.0613)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [178/200][100/390]\tTime 0.050 (0.053)\tData 0.000 (0.002)\tLoss 0.0057 (0.0035)\tTop 1-err 0.0000 (0.0541)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [178/200][150/390]\tTime 0.050 (0.052)\tData 0.000 (0.001)\tLoss 0.0012 (0.0030)\tTop 1-err 0.0000 (0.0362)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [178/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0030)\tTop 1-err 0.0000 (0.0272)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [178/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0061 (0.0029)\tTop 1-err 0.0000 (0.0249)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [178/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0034 (0.0029)\tTop 1-err 0.0000 (0.0208)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [178/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0053 (0.0030)\tTop 1-err 0.0000 (0.0267)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [178/200][0/79]\tTime 0.136 (0.136)\tLoss 0.1236 (0.1236)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [178/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0883 (0.2076)\tTop 1-err 3.1250 (5.1317)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [178/200]\tTop 1-err 5.220\tTop 5-err 0.130\tTest Loss 0.212\n",
            "\n",
            "Epoch 180/200\n",
            "Epoch: [179/200][0/390]\tTime 0.204 (0.204)\tData 0.132 (0.132)\tLoss 0.0054 (0.0054)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [179/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0019 (0.0033)\tTop 1-err 0.0000 (0.0613)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [179/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0007 (0.0032)\tTop 1-err 0.0000 (0.0619)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [179/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0055 (0.0029)\tTop 1-err 0.0000 (0.0414)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [179/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0009 (0.0029)\tTop 1-err 0.0000 (0.0350)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [179/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0014 (0.0030)\tTop 1-err 0.0000 (0.0405)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [179/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0017 (0.0030)\tTop 1-err 0.0000 (0.0415)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [179/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0022 (0.0031)\tTop 1-err 0.0000 (0.0445)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [179/200][0/79]\tTime 0.132 (0.132)\tLoss 0.1235 (0.1235)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [179/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0832 (0.2031)\tTop 1-err 3.1250 (5.0705)\tTop 5-err 0.0000 (0.1532)\n",
            "* Epoch: [179/200]\tTop 1-err 5.180\tTop 5-err 0.150\tTest Loss 0.208\n",
            "\n",
            "Epoch 181/200\n",
            "Epoch: [180/200][0/390]\tTime 0.194 (0.194)\tData 0.133 (0.133)\tLoss 0.0032 (0.0032)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [180/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0035 (0.0026)\tTop 1-err 0.0000 (0.0460)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [180/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0019 (0.0029)\tTop 1-err 0.0000 (0.0464)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [180/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0028 (0.0030)\tTop 1-err 0.0000 (0.0310)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [180/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0029)\tTop 1-err 0.0000 (0.0272)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [180/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0030)\tTop 1-err 0.0000 (0.0249)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [180/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0009 (0.0030)\tTop 1-err 0.0000 (0.0260)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [180/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0010 (0.0029)\tTop 1-err 0.0000 (0.0267)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [180/200][0/79]\tTime 0.132 (0.132)\tLoss 0.1418 (0.1418)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [180/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0885 (0.2059)\tTop 1-err 3.9062 (5.1164)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [180/200]\tTop 1-err 5.200\tTop 5-err 0.150\tTest Loss 0.211\n",
            "\n",
            "Epoch 182/200\n",
            "Epoch: [181/200][0/390]\tTime 0.193 (0.193)\tData 0.132 (0.132)\tLoss 0.0026 (0.0026)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [181/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0023 (0.0039)\tTop 1-err 0.0000 (0.0613)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [181/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0032 (0.0034)\tTop 1-err 0.0000 (0.0541)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [181/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0028 (0.0032)\tTop 1-err 0.0000 (0.0466)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [181/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0069 (0.0031)\tTop 1-err 0.0000 (0.0428)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [181/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0022 (0.0031)\tTop 1-err 0.0000 (0.0374)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [181/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0009 (0.0030)\tTop 1-err 0.0000 (0.0363)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [181/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0007 (0.0030)\tTop 1-err 0.0000 (0.0356)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [181/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1134 (0.1134)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [181/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0878 (0.2046)\tTop 1-err 3.9062 (5.1624)\tTop 5-err 0.0000 (0.1532)\n",
            "* Epoch: [181/200]\tTop 1-err 5.250\tTop 5-err 0.140\tTest Loss 0.211\n",
            "\n",
            "Epoch 183/200\n",
            "Epoch: [182/200][0/390]\tTime 0.197 (0.197)\tData 0.137 (0.137)\tLoss 0.0027 (0.0027)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [182/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0175 (0.0034)\tTop 1-err 0.7812 (0.0460)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [182/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0011 (0.0033)\tTop 1-err 0.0000 (0.0309)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [182/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0017 (0.0032)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [182/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0021 (0.0030)\tTop 1-err 0.0000 (0.0194)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [182/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0028 (0.0030)\tTop 1-err 0.0000 (0.0218)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [182/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0008 (0.0030)\tTop 1-err 0.0000 (0.0234)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [182/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0040 (0.0030)\tTop 1-err 0.0000 (0.0200)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [182/200][0/79]\tTime 0.133 (0.133)\tLoss 0.1300 (0.1300)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [182/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0919 (0.2042)\tTop 1-err 4.6875 (5.2849)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [182/200]\tTop 1-err 5.350\tTop 5-err 0.100\tTest Loss 0.211\n",
            "\n",
            "Epoch 184/200\n",
            "Epoch: [183/200][0/390]\tTime 0.191 (0.191)\tData 0.132 (0.132)\tLoss 0.0023 (0.0023)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [183/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0021 (0.0026)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [183/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0019 (0.0027)\tTop 1-err 0.0000 (0.0232)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [183/200][150/390]\tTime 0.056 (0.051)\tData 0.000 (0.001)\tLoss 0.0070 (0.0026)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [183/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0033 (0.0026)\tTop 1-err 0.0000 (0.0233)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [183/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0012 (0.0026)\tTop 1-err 0.0000 (0.0249)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [183/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0027)\tTop 1-err 0.0000 (0.0286)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [183/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0017 (0.0028)\tTop 1-err 0.0000 (0.0312)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [183/200][0/79]\tTime 0.131 (0.131)\tLoss 0.1308 (0.1308)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [183/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0944 (0.2080)\tTop 1-err 4.6875 (5.2390)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [183/200]\tTop 1-err 5.350\tTop 5-err 0.110\tTest Loss 0.214\n",
            "\n",
            "Epoch 185/200\n",
            "Epoch: [184/200][0/390]\tTime 0.201 (0.201)\tData 0.141 (0.141)\tLoss 0.0007 (0.0007)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [184/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0039 (0.0032)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [184/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0041 (0.0028)\tTop 1-err 0.0000 (0.0077)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [184/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0012 (0.0029)\tTop 1-err 0.0000 (0.0207)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [184/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0049 (0.0030)\tTop 1-err 0.0000 (0.0194)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [184/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0017 (0.0031)\tTop 1-err 0.0000 (0.0249)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [184/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0007 (0.0030)\tTop 1-err 0.0000 (0.0260)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [184/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0021 (0.0030)\tTop 1-err 0.0000 (0.0223)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [184/200][0/79]\tTime 0.122 (0.122)\tLoss 0.1210 (0.1210)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [184/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0961 (0.2037)\tTop 1-err 4.6875 (5.2237)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [184/200]\tTop 1-err 5.290\tTop 5-err 0.130\tTest Loss 0.210\n",
            "\n",
            "Epoch 186/200\n",
            "Epoch: [185/200][0/390]\tTime 0.194 (0.194)\tData 0.133 (0.133)\tLoss 0.0008 (0.0008)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [185/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0015 (0.0030)\tTop 1-err 0.0000 (0.0460)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [185/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0019 (0.0026)\tTop 1-err 0.0000 (0.0309)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [185/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0012 (0.0028)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [185/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0112 (0.0029)\tTop 1-err 0.7812 (0.0389)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [185/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0014 (0.0029)\tTop 1-err 0.0000 (0.0374)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [185/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0017 (0.0029)\tTop 1-err 0.0000 (0.0337)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [185/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0033 (0.0029)\tTop 1-err 0.0000 (0.0356)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [185/200][0/79]\tTime 0.151 (0.151)\tLoss 0.1291 (0.1291)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [185/200][50/79]\tTime 0.017 (0.019)\tLoss 0.0887 (0.2070)\tTop 1-err 3.1250 (5.0245)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [185/200]\tTop 1-err 5.150\tTop 5-err 0.140\tTest Loss 0.211\n",
            "\n",
            "Epoch 187/200\n",
            "Epoch: [186/200][0/390]\tTime 0.205 (0.205)\tData 0.135 (0.135)\tLoss 0.0017 (0.0017)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [186/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0048 (0.0030)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [186/200][100/390]\tTime 0.056 (0.052)\tData 0.000 (0.002)\tLoss 0.0016 (0.0030)\tTop 1-err 0.0000 (0.0309)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [186/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0010 (0.0028)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [186/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0016 (0.0027)\tTop 1-err 0.0000 (0.0233)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [186/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0018 (0.0028)\tTop 1-err 0.0000 (0.0311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [186/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0007 (0.0028)\tTop 1-err 0.0000 (0.0311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [186/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0011 (0.0028)\tTop 1-err 0.0000 (0.0289)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [186/200][0/79]\tTime 0.135 (0.135)\tLoss 0.1058 (0.1058)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [186/200][50/79]\tTime 0.016 (0.019)\tLoss 0.0845 (0.2064)\tTop 1-err 3.9062 (5.0092)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [186/200]\tTop 1-err 5.190\tTop 5-err 0.130\tTest Loss 0.212\n",
            "\n",
            "Epoch 188/200\n",
            "Epoch: [187/200][0/390]\tTime 0.197 (0.197)\tData 0.138 (0.138)\tLoss 0.0013 (0.0013)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [187/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0017 (0.0022)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [187/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0021 (0.0022)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [187/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0014 (0.0025)\tTop 1-err 0.0000 (0.0310)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [187/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0041 (0.0025)\tTop 1-err 0.0000 (0.0233)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [187/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0036 (0.0025)\tTop 1-err 0.0000 (0.0249)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [187/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0026)\tTop 1-err 0.0000 (0.0311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [187/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0006 (0.0027)\tTop 1-err 0.0000 (0.0356)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [187/200][0/79]\tTime 0.140 (0.140)\tLoss 0.1173 (0.1173)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [187/200][50/79]\tTime 0.016 (0.019)\tLoss 0.0870 (0.2072)\tTop 1-err 3.9062 (5.0245)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [187/200]\tTop 1-err 5.160\tTop 5-err 0.130\tTest Loss 0.210\n",
            "\n",
            "Epoch 189/200\n",
            "Epoch: [188/200][0/390]\tTime 0.197 (0.197)\tData 0.131 (0.131)\tLoss 0.0007 (0.0007)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [188/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0035 (0.0029)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [188/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0022 (0.0025)\tTop 1-err 0.0000 (0.0077)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [188/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0011 (0.0025)\tTop 1-err 0.0000 (0.0207)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [188/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0026 (0.0027)\tTop 1-err 0.0000 (0.0389)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [188/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0017 (0.0027)\tTop 1-err 0.0000 (0.0436)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [188/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0044 (0.0027)\tTop 1-err 0.0000 (0.0415)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [188/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0051 (0.0028)\tTop 1-err 0.0000 (0.0401)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [188/200][0/79]\tTime 0.134 (0.134)\tLoss 0.1162 (0.1162)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [188/200][50/79]\tTime 0.016 (0.018)\tLoss 0.1017 (0.2084)\tTop 1-err 3.9062 (5.1930)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [188/200]\tTop 1-err 5.260\tTop 5-err 0.160\tTest Loss 0.211\n",
            "\n",
            "Epoch 190/200\n",
            "Epoch: [189/200][0/390]\tTime 0.199 (0.199)\tData 0.135 (0.135)\tLoss 0.0029 (0.0029)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [189/200][50/390]\tTime 0.050 (0.055)\tData 0.000 (0.003)\tLoss 0.0028 (0.0026)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [189/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0009 (0.0025)\tTop 1-err 0.0000 (0.0232)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [189/200][150/390]\tTime 0.050 (0.052)\tData 0.000 (0.001)\tLoss 0.0024 (0.0027)\tTop 1-err 0.0000 (0.0310)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [189/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0059 (0.0028)\tTop 1-err 0.0000 (0.0311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [189/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0025 (0.0027)\tTop 1-err 0.0000 (0.0342)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [189/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0015 (0.0027)\tTop 1-err 0.0000 (0.0311)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [189/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0012 (0.0029)\tTop 1-err 0.0000 (0.0378)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [189/200][0/79]\tTime 0.127 (0.127)\tLoss 0.1096 (0.1096)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [189/200][50/79]\tTime 0.017 (0.018)\tLoss 0.0868 (0.2050)\tTop 1-err 3.1250 (4.9786)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [189/200]\tTop 1-err 5.110\tTop 5-err 0.140\tTest Loss 0.210\n",
            "\n",
            "Epoch 191/200\n",
            "Epoch: [190/200][0/390]\tTime 0.192 (0.192)\tData 0.132 (0.132)\tLoss 0.0016 (0.0016)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [190/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0015 (0.0027)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [190/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0014 (0.0027)\tTop 1-err 0.0000 (0.0232)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [190/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0037 (0.0024)\tTop 1-err 0.0000 (0.0207)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [190/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0006 (0.0025)\tTop 1-err 0.0000 (0.0194)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [190/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0025)\tTop 1-err 0.0000 (0.0218)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [190/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0149 (0.0025)\tTop 1-err 0.7812 (0.0234)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [190/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0014 (0.0026)\tTop 1-err 0.0000 (0.0245)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [190/200][0/79]\tTime 0.133 (0.133)\tLoss 0.1158 (0.1158)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [190/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0992 (0.2092)\tTop 1-err 4.6875 (5.0858)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [190/200]\tTop 1-err 5.220\tTop 5-err 0.160\tTest Loss 0.213\n",
            "\n",
            "Epoch 192/200\n",
            "Epoch: [191/200][0/390]\tTime 0.210 (0.210)\tData 0.137 (0.137)\tLoss 0.0017 (0.0017)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [191/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0016 (0.0032)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [191/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0017 (0.0026)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [191/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0010 (0.0026)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [191/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0016 (0.0026)\tTop 1-err 0.0000 (0.0194)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [191/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0017 (0.0025)\tTop 1-err 0.0000 (0.0187)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [191/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0021 (0.0026)\tTop 1-err 0.0000 (0.0208)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [191/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0010 (0.0025)\tTop 1-err 0.0000 (0.0200)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [191/200][0/79]\tTime 0.146 (0.146)\tLoss 0.1116 (0.1116)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [191/200][50/79]\tTime 0.016 (0.019)\tLoss 0.0945 (0.2089)\tTop 1-err 3.1250 (5.1164)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [191/200]\tTop 1-err 5.130\tTop 5-err 0.150\tTest Loss 0.211\n",
            "\n",
            "Epoch 193/200\n",
            "Epoch: [192/200][0/390]\tTime 0.212 (0.212)\tData 0.142 (0.142)\tLoss 0.0015 (0.0015)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [192/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0009 (0.0025)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [192/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0013 (0.0024)\tTop 1-err 0.0000 (0.0232)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [192/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0018 (0.0023)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [192/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0023 (0.0025)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [192/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0010 (0.0026)\tTop 1-err 0.0000 (0.0187)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [192/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0009 (0.0026)\tTop 1-err 0.0000 (0.0182)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [192/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0085 (0.0025)\tTop 1-err 0.7812 (0.0200)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [192/200][0/79]\tTime 0.124 (0.124)\tLoss 0.1055 (0.1055)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [192/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0947 (0.2070)\tTop 1-err 3.9062 (4.9939)\tTop 5-err 0.0000 (0.0919)\n",
            "* Epoch: [192/200]\tTop 1-err 5.080\tTop 5-err 0.100\tTest Loss 0.210\n",
            "\n",
            "Epoch 194/200\n",
            "Epoch: [193/200][0/390]\tTime 0.193 (0.193)\tData 0.135 (0.135)\tLoss 0.0012 (0.0012)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [193/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0046 (0.0027)\tTop 1-err 0.0000 (0.0306)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [193/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0015 (0.0024)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [193/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0020 (0.0024)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [193/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0025)\tTop 1-err 0.0000 (0.0194)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [193/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0014 (0.0025)\tTop 1-err 0.0000 (0.0187)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [193/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0027 (0.0025)\tTop 1-err 0.0000 (0.0234)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [193/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0058 (0.0025)\tTop 1-err 0.0000 (0.0223)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [193/200][0/79]\tTime 0.126 (0.126)\tLoss 0.1046 (0.1046)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [193/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0964 (0.2061)\tTop 1-err 3.9062 (4.9786)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [193/200]\tTop 1-err 5.100\tTop 5-err 0.130\tTest Loss 0.210\n",
            "\n",
            "Epoch 195/200\n",
            "Epoch: [194/200][0/390]\tTime 0.198 (0.198)\tData 0.137 (0.137)\tLoss 0.0011 (0.0011)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [194/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0026 (0.0020)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [194/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0015 (0.0026)\tTop 1-err 0.0000 (0.0309)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [194/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0027 (0.0025)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [194/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0008 (0.0025)\tTop 1-err 0.0000 (0.0272)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [194/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0029 (0.0026)\tTop 1-err 0.0000 (0.0280)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [194/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0022 (0.0025)\tTop 1-err 0.0000 (0.0260)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [194/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0016 (0.0025)\tTop 1-err 0.0000 (0.0223)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [194/200][0/79]\tTime 0.126 (0.126)\tLoss 0.0949 (0.0949)\tTop 1-err 3.1250 (3.1250)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [194/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0860 (0.2053)\tTop 1-err 4.6875 (4.9786)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [194/200]\tTop 1-err 5.110\tTop 5-err 0.160\tTest Loss 0.210\n",
            "\n",
            "Epoch 196/200\n",
            "Epoch: [195/200][0/390]\tTime 0.208 (0.208)\tData 0.137 (0.137)\tLoss 0.0033 (0.0033)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [195/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0019 (0.0024)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [195/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0011 (0.0023)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [195/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0014 (0.0022)\tTop 1-err 0.0000 (0.0207)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [195/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0030 (0.0021)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [195/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0014 (0.0022)\tTop 1-err 0.0000 (0.0218)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [195/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0190 (0.0022)\tTop 1-err 0.7812 (0.0208)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [195/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0012 (0.0023)\tTop 1-err 0.0000 (0.0245)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [195/200][0/79]\tTime 0.134 (0.134)\tLoss 0.1087 (0.1087)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [195/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0805 (0.2058)\tTop 1-err 3.9062 (4.9479)\tTop 5-err 0.0000 (0.1072)\n",
            "* Epoch: [195/200]\tTop 1-err 5.020\tTop 5-err 0.130\tTest Loss 0.209\n",
            "\n",
            "Epoch 197/200\n",
            "Epoch: [196/200][0/390]\tTime 0.208 (0.208)\tData 0.137 (0.137)\tLoss 0.0015 (0.0015)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [196/200][50/390]\tTime 0.050 (0.054)\tData 0.000 (0.003)\tLoss 0.0117 (0.0022)\tTop 1-err 0.7812 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [196/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0012 (0.0026)\tTop 1-err 0.0000 (0.0232)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [196/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0019 (0.0025)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [196/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0015 (0.0023)\tTop 1-err 0.0000 (0.0194)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [196/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0023)\tTop 1-err 0.0000 (0.0156)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [196/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0024)\tTop 1-err 0.0000 (0.0208)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [196/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0015 (0.0024)\tTop 1-err 0.0000 (0.0223)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [196/200][0/79]\tTime 0.144 (0.144)\tLoss 0.1023 (0.1023)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [196/200][50/79]\tTime 0.016 (0.019)\tLoss 0.0850 (0.2058)\tTop 1-err 4.6875 (5.0245)\tTop 5-err 0.0000 (0.0919)\n",
            "* Epoch: [196/200]\tTop 1-err 5.090\tTop 5-err 0.100\tTest Loss 0.210\n",
            "\n",
            "Epoch 198/200\n",
            "Epoch: [197/200][0/390]\tTime 0.195 (0.195)\tData 0.132 (0.132)\tLoss 0.0014 (0.0014)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [197/200][50/390]\tTime 0.051 (0.053)\tData 0.000 (0.003)\tLoss 0.0034 (0.0021)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [197/200][100/390]\tTime 0.050 (0.052)\tData 0.000 (0.002)\tLoss 0.0016 (0.0022)\tTop 1-err 0.0000 (0.0077)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [197/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0035 (0.0024)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [197/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0010 (0.0024)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [197/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0019 (0.0025)\tTop 1-err 0.0000 (0.0156)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [197/200][300/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0017 (0.0025)\tTop 1-err 0.0000 (0.0156)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [197/200][350/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0038 (0.0024)\tTop 1-err 0.0000 (0.0134)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [197/200][0/79]\tTime 0.127 (0.127)\tLoss 0.1036 (0.1036)\tTop 1-err 3.9062 (3.9062)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [197/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0884 (0.2045)\tTop 1-err 4.6875 (5.0092)\tTop 5-err 0.0000 (0.1225)\n",
            "* Epoch: [197/200]\tTop 1-err 5.140\tTop 5-err 0.140\tTest Loss 0.210\n",
            "\n",
            "Epoch 199/200\n",
            "Epoch: [198/200][0/390]\tTime 0.195 (0.195)\tData 0.136 (0.136)\tLoss 0.0078 (0.0078)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [198/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0014 (0.0021)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [198/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0026 (0.0023)\tTop 1-err 0.0000 (0.0077)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [198/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0018 (0.0025)\tTop 1-err 0.0000 (0.0207)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [198/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0044 (0.0025)\tTop 1-err 0.0000 (0.0194)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [198/200][250/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0163 (0.0025)\tTop 1-err 0.7812 (0.0249)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [198/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0011 (0.0024)\tTop 1-err 0.0000 (0.0208)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [198/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0015 (0.0024)\tTop 1-err 0.0000 (0.0223)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [198/200][0/79]\tTime 0.130 (0.130)\tLoss 0.1004 (0.1004)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [198/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0866 (0.2040)\tTop 1-err 3.9062 (5.1011)\tTop 5-err 0.0000 (0.1379)\n",
            "* Epoch: [198/200]\tTop 1-err 5.220\tTop 5-err 0.150\tTest Loss 0.209\n",
            "\n",
            "Epoch 200/200\n",
            "Epoch: [199/200][0/390]\tTime 0.193 (0.193)\tData 0.133 (0.133)\tLoss 0.0057 (0.0057)\tTop 1-err 0.0000 (0.0000)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [199/200][50/390]\tTime 0.050 (0.053)\tData 0.000 (0.003)\tLoss 0.0022 (0.0019)\tTop 1-err 0.0000 (0.0153)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [199/200][100/390]\tTime 0.050 (0.051)\tData 0.000 (0.002)\tLoss 0.0011 (0.0020)\tTop 1-err 0.0000 (0.0155)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [199/200][150/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0013 (0.0022)\tTop 1-err 0.0000 (0.0259)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [199/200][200/390]\tTime 0.050 (0.051)\tData 0.000 (0.001)\tLoss 0.0014 (0.0025)\tTop 1-err 0.0000 (0.0272)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [199/200][250/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0024 (0.0024)\tTop 1-err 0.0000 (0.0249)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [199/200][300/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0018 (0.0025)\tTop 1-err 0.0000 (0.0234)\tTop 5-err 0.0000 (0.0000)\n",
            "Epoch: [199/200][350/390]\tTime 0.050 (0.050)\tData 0.000 (0.001)\tLoss 0.0019 (0.0025)\tTop 1-err 0.0000 (0.0245)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [199/200][0/79]\tTime 0.131 (0.131)\tLoss 0.1203 (0.1203)\tTop 1-err 4.6875 (4.6875)\tTop 5-err 0.0000 (0.0000)\n",
            "Test (on val set): [199/200][50/79]\tTime 0.016 (0.018)\tLoss 0.0981 (0.2100)\tTop 1-err 3.1250 (5.0705)\tTop 5-err 0.0000 (0.0919)\n",
            "* Epoch: [199/200]\tTop 1-err 5.070\tTop 5-err 0.110\tTest Loss 0.213\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished training with SGD.\n",
            "Best val top1 error: 5.02\n",
            "Final val top1 error: 5.07\n",
            "Best val top5 error: 0.13\n",
            "Final val top5 error: 0.11\n"
          ]
        }
      ],
      "source": [
        "Trainer(model2, 'SGD', dataset_name='cifar10', max_epochs=200, random_seed=2025)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}