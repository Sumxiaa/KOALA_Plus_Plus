# KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products

> Official implementation of **KOALA++**, a scalable Kalman-based optimization algorithm for neural network training.  
> ğŸ“¢ The code will be released **after the acceptance of our paper**.

---
## ğŸ“° Latest News
- **[2025-09]** ğŸ‰ Our paper has been **accepted at NeurIPS 2025**!  
- ğŸ”œ Code release is coming soon â€” stay tuned!

## ğŸ“Œ Overview
KOALA++ introduces a **Kalman-based optimization framework** that explicitly models structured gradient uncertainty.

âœ¨ Key highlights:
- âš¡ **No Hessian needed** â€“ avoids expensive second-order computations.  
- ğŸ” **Compact covariance updates** â€“ recursively updates gradientâ€“covariance products.  
- ğŸ§© **Beyond diagonals** â€“ captures richer uncertainty without storing full covariance matrices.  
- ğŸ† **Strong performance** â€“ matches or surpasses state-of-the-art first- and second-order optimizers.  
- âš™ï¸ **Efficient** â€“ maintains the scalability of first-order methods.  

---

## ğŸ“– Citation
If you find this work useful, please cite our paper:

```bibtex
@misc{xia2025koalaefficientkalmanbasedoptimization,
      title={KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products}, 
      author={Zixuan Xia and Aram Davtyan and Paolo Favaro},
      year={2025},
      eprint={2506.04432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.04432}, 
}

```

---

## âœ‰ï¸ Contact
For questions or collaboration inquiries, please reach out:  
**Zixuan Xia** â€” xxiazixuan824@gmail.com Â· zixuan.xia@students.unibe.ch 

---

â­ï¸ *We appreciate your interest in KOALA++ and look forward to sharing the code and results with the community.*

