# KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products

Abstract, _We propose KOALA++, a scalable Kalman-based optimization algorithm that explicitly models structured gradient uncertainty in neural network training. Unlike second-order methods, which rely on expensive second order gradient calculation, our method directly estimates the parameter covariance matrix by recursively updating compact gradient covariance products. This design improves upon the original KOALA framework that assumed diagonal covariance by implicitly capturing richer uncertainty structure without storing the full covariance matrix and avoiding large matrix inversions. Across diverse tasks, including image classification and language modeling, KOALA++ achieves accuracy on par or better than state-of-the-art first- and second-order optimizers while maintaining the efficiency of first-order methods._ 

<center>
<a href="https://openreview.net/group?id=NeurIPS.cc/2025/Conference/Authors&referrer=%5BHomepage%5D(%2F)" target="_blank">
    <img alt="OpenReview" src="https://img.shields.io/badge/OpenReview-KOALA++-blue?logo=openreview" height="30" />
</a>
<a href="https://arxiv.org/abs/2506.04432" target="_blank">
    <img alt="arXiv" src="https://img.shields.io/badge/arXiv-KOALA++-red?logo=arxiv" height="30" />
<div>
    <a href="https://sumxiaa.github.io/" target="_blank">Zixuan Xia</a><sup>1</sup>,</span>
    <a href="https://araachie.github.io/" target="_blank">Aram Davtyan</a><sup>2</sup>, </span>
    <a href="https://www.cvg.unibe.ch/people/favaro" target="_blank">Paolo Favaro</a><sup>2</sup>,</span>
</div>
<div>
    <sup>1</sup>Work done during Master studies at the University of Bern&emsp;
    <sup>2</sup>Computer Vision Group, University of Bern&emsp;
</div>
</center>
<center>
    <img src="img/KOALA++.png" alt="Overview of Project" width="100%" height="280"/>
</center>

⚠️ **Note**  
This image was generated by ChatGPT.  
It is included here only for aesthetic purposes in the project structure  
and does not have any functional relation to the project itself.


> Official implementation of **KOALA++**, a scalable Kalman-based optimization algorithm for neural network training.  
> 📢 The code will be released **after the acceptance of our paper**.

---
## 📰 Latest News
- **[2025-09]** 🎉 Our paper has been **accepted at NeurIPS 2025**!  
- 🔜 Code release is coming soon — stay tuned!

---
## 📁 Repository Contents

- [Methodology and Key Insights](#💡-methodology-and-key-insights)
- [Getting Started](#🌟-getting-started)
- [Image Classification](#️🖼️-image-classification)
- [Language Modeling](#📖-language-modeling)
    - [Dataset Download](#dataset-download)
    - [Usage](#🎯-usage-1)
- [Distributed Training](#⚡️-distributed-training)
- [Getting Started](#🌟-getting-started)
    - [Prerequisites](#prerequisites)
    - [Using pip](#using-pip)
    - [Using Conda](#using-conda)
- [License](#📜-license)
- [Citation](#📖-citation)

---
## 💡 Methodology and Key Insights

**KOALA++** extends the Kalman filtering view of optimization by explicitly propagating structured gradient uncertainty. Its core innovation lies in tracking a **directional covariance surrogate**:  


$v_k := H_k P_{k-1},$

instead of the full covariance $P_{k-1} \in \mathbb{R}^{n \times n}$.  
This surrogate captures anisotropic uncertainty while keeping memory and computational cost comparable to first-order optimizers.

---

### Why $\alpha_k$ and $w_k$ Appear
Expanding the recursion for $v_k$ gives:


$v_k = H_k P_{k-1} = H_k (I - K_{k-1} H_{k-1})(P_{k-2} + Q)$

which introduces a dependency on **$H_k P_{k-2}$**.  
Since $P_{k-2}$ is not explicitly stored, KOALA++ approximates it by solving a **least-squares problem** subject to the relation:


$v_{k-1} = H_{k-1} P_{k-2}$

---

### Least-Squares Objective
We seek a surrogate for $P_{k-2}$:


$$
\begin{aligned}
\min_{P_{k-2}} \quad & \| P_{k-2} \|_F^2 \\
\text{s.t.} \quad & H_{k-1} P_{k-2} = v_{k-1}
\end{aligned}
$$


This system admits multiple solutions. KOALA++ considers **two variants**:

---

#### (i) Vanilla (Asymmetric) Solution
Unconstrained least-squares yields:


$P_{k-2} = \frac{H_{k-1}^\top v_{k-1}}{\|H_{k-1}\|^2}.$

Substituting back gives:


$H_k P_{k-2} = \alpha_k v_{k-1}, 
\quad \alpha_k = \frac{H_k H_{k-1}^\top}{\|H_{k-1}\|^2}.$

---

#### (ii) Symmetric Solution
If we enforce $P_{k-2} = P_{k-2}^\top$, the least-squares solution is:


$P_{k-2} =
\frac{H_{k-1}^\top v_{k-1} + v_{k-1}^\top H_{k-1}}{\|H_{k-1}\|^2}- \frac{H_{k-1} v_{k-1}^\top H_{k-1}^\top}{\|H_{k-1}\|^4}.$


---

### Unified Update Rule
Combining both cases, KOALA++ adopts the recursion:


$v_k = (\alpha_k - \lambda_k) v_{k-1} 
      + (H_k - \lambda_k H_{k-1}) Q 
      + w_k H_{k-1},$

where:
- $\alpha_k$: alignment coefficient (always present),  
- $w_k$: correction term (only in symmetric variant, $w_k=0$ for vanilla),  
- $\lambda_k$: Kalman attenuation factor.  

Finally, parameters are updated as:


$\theta_k = \theta_{k-1} - 
\eta_k \, \frac{L_k(\theta_{k-1})}{H_k v_k^\top + H_k Q H_k^\top + R} \cdot (v_k^\top + QH_k^\top).$
---

### Algorithm Summary
<p align="center">
  <img src="imgs/algorithm1.png" alt="Algorithm 1 KOALA++" width="70%"/>
</p>

---

## 📖 Citation
If you find this work useful, please cite our paper:

```bibtex
@misc{xia2025koalaefficientkalmanbasedoptimization,
      title={KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products}, 
      author={Zixuan Xia and Aram Davtyan and Paolo Favaro},
      year={2025},
      eprint={2506.04432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.04432}, 
}

```

---

## ✉️ Contact
For questions or collaboration inquiries, please reach out:  
**Zixuan Xia** — xxiazixuan824@gmail.com · zixuan.xia@students.unibe.ch 

---

⭐️ *We appreciate your interest in KOALA++ and look forward to sharing the code and results with the community.*

